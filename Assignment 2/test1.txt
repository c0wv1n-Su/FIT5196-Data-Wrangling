Budgeted Optimization with Concurrent
Stochastic-Duration Experiments

Authored by:

Alan Fern
Javad Azimi
Xiaoli Z. Fern

Abstract

Budgeted optimization involves optimizing an unknown function that
is costly to evaluate by requesting a limited number of function evaluations
at intelligently selected inputs. Typical problem formulations assume that
experiments are selected one at a time with a limited total number of ex-
periments, which fail to capture important aspects of many real-world
problems. This paper deﬁnes a novel problem formulation with the fol-
lowing important extensions: 1) allowing for concurrent experiments; 2)
allowing for stochastic experiment durations; and 3) placing constraints
on both the total number of experiments and the total experimental time.
We develop both oﬄine and online algorithms for selecting concurrent ex-
periments in this new setting and provide experimental results on a num-
ber of optimization benchmarks. The results show that our algorithms
produce highly eﬀective schedules compared to natural baselines.

1 Paper Body

We study the optimization of an unknown function f by requesting n experi-
ments, each specifying an input x and producing a noisy observation of f (x). In
practice, the function f might be the performance of a device parameterized by
x. We consider the setting where running experiments is costly (e.g. in terms
of time), which renders methods that rely on many function evaluations, such
as stochastic search or empirical gradient methods, impractical. Bayesian op-
timization (BO) [8, 4] addresses this issue by leveraging Bayesian modeling to
maintain a posterior over the unknown function based on previous experiments.
The posterior is then used to intelligently select new experiments to trade-oﬀ
exploring new parts of the experimental space and exploiting promising parts.
Traditional BO follows a sequential approach where only one experiment is se-
lected and run at a time. However, it is often desirable to select more than one
experiment at a time so that multiple experiments can be run simultaneously

1

to leverage parallel facilities. Recently, Azimi et al. (2010) proposed a batch
BO algorithm that selects a batch of k ? 1 experiments at a time. While this
broadens the applicability of BO, it is still limited to selecting a ﬁxed number of
experiments at each step. As such, prior work on BO, both batch and sequen-
tial, completely ignores the problem of how to schedule experiments under ﬁxed
experimental budget and time constraints. Furthermore, existing work assumes
that the durations of experiments are identical and deterministic, whereas in
practice they are often stochastic. Consider one of our motivating applications
of optimizing the power output of nano-enhanced Microbial Fuel Cells (MFCs).
MFCs [3] use micro-organisms to generate electricity. Their performance de-
pends 1
strongly on the surface properties of the anode [10]. Our problem involves
optimizing nano-enhanced anodes, where various types of nano-structures, e.g.
carbon nano-wire, are grown directly on the anode surface. Because there is
little understanding of how diﬀerent nano-enhancements impact power output,
optimizing anode design is largely guess work. Our original goal was to de-
velop BO algorithms for aiding this process. However, many aspects of this
domain complicate the application of BO. First, there is a ﬁxed budget on the
number of experiments that can be run due to limited funds and a ﬁxed time
period for the pro ject. Second, we can run multiple concurrent experiments,
limited by the number of experimental apparatus. Third, the time required to
run each experiment is variable because each experiment requires the construc-
tion of a nano-structure with speciﬁc properties. Nano-fabrication is highly
unpredictable and the amount of time to successfully produce a structure is
quite variable. Clearly prior BO models fail to capture critical aspects of the
experimental process in this domain. In this paper, we consider the following
extensions. First, we have l available labs (which may correspond to experi-
mental stations at one location or to physically distinct laboratories), allowing
up to l concurrent experiments. Second, experiments have stochastic durations,
independently and identically distributed according to a known density function
pd . Finally, we are constrained by a budget of n total experiments and a time
horizon h by which point we must ﬁnish. The goal is to maximize the unknown
function f by selecting experiments and when to start them while satisfying the
constraints. We propose oﬄine (Section 4) and online (Section 5) scheduling
approaches for this problem, which aim to balance two competing factors. First,
a scheduler should ensure that all n experiments complete within the horizon
h, which encourages high concurrency. Second, we wish to select new exper-
iments given as many previously completed experiments as possible to make
more intelligent experiment selections, which encourages low concurrency. We
introduce a novel measure of the second factor, cumulative prior experiments
(CPE) (Section 3), which our approaches aim to optimize. Our experimental
results indicate that these approaches signiﬁcantly outperform a set of baselines
across a range of benchmark optimization problems.
2
Problem Setup
Let X ? ¡d be a d-dimensional compact input space, where each dimension i

2

is bounded in [ai , bi ]. An element of X is called an experiment. An unknown
real-valued function f : X ? ¡ represents the expected value of the dependent
variable after running an experiment. For example, f (x) might be the result of a
wetlab experiment described by x. Conducting an experiment x produces a noisy
outcome y = f (x) + , where is a random noise term. Bayesian Optimization
(BO) aims to ﬁnd an experiment x ? X that approximately maximizes f by
requesting a limited number of experiments and observing their outcomes. We
extend traditional BO algorithms and study the experiment scheduling problem.
Assuming a known density function pd for the experiment durations, the inputs
to our problem include the total number of available labs l, the total number of
experiments n, and the time horizon h by which we must ﬁnish. The goal is to
design a policy ? for selecting when to start experiments and which ones to start
to optimize f . Speciﬁcally, the inputs to ? are the set of completed experiments
and their outcomes, the set of currently running experiments with their elapsed
running time, the number of free labs, and the remaining time till the horizon.
Given this information, ? must select a set of experiments (possibly empty)
to start that is no larger than the number of free labs. Any run of the policy
ends when either n experiments are completed or the time horizon is reached,
resulting in a set X of n or fewer completed experiments. The ob jective is to
obtain a policy with small regret, which is the expected diﬀerence between the
optimal value of f and the value of f for the predicted best experiment in X. In
theory, the optimal policy can be found by solving a POMDP with hidden state
corresponding to the unknown function f . However, this POMDP is beyond
the reach of any existing solvers. Thus, we focus on deﬁning and comparing
several principled policies that work well in practice, but without optimality
guarantees. Note that this problem has not been studied in the literature to the
best of our knowledge. 2
3
Overview of General Approach
A policy for our problem must make two types of decisions: 1) scheduling
when to start new experiments, and 2) selecting the speciﬁc experiments to
start. In this work, we factor the problem based on these decisions and focus
on approaches for scheduling experiments. We assume a black box function
SelectBatch for intelligently selecting the k ? 1 experiments based on both com-
pleted and currently running experiments. The implementation of SelectBatch
is described in Section 6. Optimal scheduling to minimize regret appears to
be computationally hard for non-trivial instances of SelectBatch. Further, we
desire scheduling approaches that do not depend on the details of SelectBatch,
but work well for any reasonable implementation. Thus, rather than directly
optimizing regret for a speciﬁc SelectBatch, we consider the following surrogate
criteria. First, we want to ﬁnish all n experiments within the horizon h with
high probability. Second, we would like to select each experiment based on as
much information as possible, measured by the number of previously completed
experiments. These two goals are at odds, since maximizing the completion
probability requires maximizing concurrency of the experiments, which mini-
mizes the second criterion. Our oﬄine and online scheduling approaches provide

3

diﬀerent ways for managing this trade-oﬀ. Cosines
0.32
0.28
0.08
Regret
0.09
0.07
0.26
0.06
0.24
0.05
0.22
0.04
0.2
0.18
Hydrogen
0.1
0.3
Regret
To quantify the second criterion, consider a complete execution E of a sched-
uler. For any experiment e in E, let priorE (e) denote the number of experi-
ments in E that completed before starting e. P We deﬁne the cumulative prior
experiments (CPE) of E as: e?E priorE (e). Intuitively, a scheduler with a high
expected CPE is desirable, since CPE measures the total amount of information
SelectBatch uses to make its decisions.
0
20
40
60
CPE
80
100
120
0.03
0
20
40
60
80
100
120
CPE
Figure 1: The correlation between CPE and CPE agrees with intuition when
considering extreme policies. regret for 30 diﬀerent schedulers on two BO A
poor scheduler that starts all n experiments at the same time benchmarks.
(assuming enough labs) will have a minimum CPE of zero. Further, CPE is

4

maximized by a scheduler that sequentially executes all experiments (assuming
enough time). However, in between these extremes, CPE fails to capture certain
intuitive properties. For example, CPE increases linearly in the number of
prior experiments, while one might expect diminishing returns as the number
of prior experiments becomes large. Similarly, as the number of experiments
started together (the batch size) increases, we might also expect diminishing
returns since SelectBatch must choose the experiments based on the same prior
experiments. Unfortunately, quantifying these intuitions in a general way is still
an open problem. Despite its potential shortcomings, we have found CPE to
be a robust measure in practice. To empirically examine the utility of CPE, we
conducted experiments on a number of BO benchmarks. For each domain, we
used 30 manually designed diverse schedulers, some started more experiments
early on than later, and vice-versa, while others included random and uniform
schedules. We measured the average regret achieved for each scheduler given
the same inputs and the expected CPE of the executions. Figure 1 shows the
results for two of the domains (other results are highly similar), where each
point corresponds to the average regret and CPE of a particular scheduler.
We observe a clear and non-trivial correlation between regret and CPE, which
provides empirical evidence that CPE is a useful measure to optimize. Further,
as we will see in our experiments, the performance of our methods is also highly
correlated with CPE.
4
Oﬄine Scheduling
We now consider oﬄine schedules, which assign start times to all n experi-
ments before the experimental process begins. Note that while the schedules are
oﬄine, the overall BO policy has online characteristics, since the exact exper-
iments to run are only speciﬁed when they need to be started by SelectBatch,
based 3
on the most recent information. This oﬄine scheduling approach is often
convenient in real experimental domains where it is useful to plan out a static
equipment/personnel schedule for the duration of a pro ject. Below we ﬁrst
consider a restricted class of schedules, called staged schedules, for which we
present a solution that optimizes CPE. Next, we describe an approach for a
more general class of schedules. 4.1
Staged Schedules
A staged schedule deﬁnes a consecutivePsequence of NPexperimental stages,
denoted by a sequence of tuples h(ni , di )iN i=1 , where 0 ¡ ni ? l, i di ? h, and i
ni ? n. Stage i begins by starting up ni new experiments selected by SelectBatch
using the most recent information, and ends after a duration of di , upon which
stage i + 1 starts. In some applications, staged schedules are preferable as they
allow pro ject planning to focus on a relatively small number of time points (the
beginning of each stage). While our approach tries to ensure that experiments
ﬁnish within their stage, experiments are never terminated and hence might
run longer than their speciﬁed duration. If, because of this, at the beginning
of stage i there are not ni free labs, the experiments will wait till labs free up.
We say that an execution E of a staged schedule S is safe if each experiment is

5

completed within its speciﬁed duration in S. We say that a staged schedule S
is p-safe if with probability at least p an execution of S is safe which provides
a probabilistic guarantee that all n experiments complete within the horizon h.
Further, it ensures with probability p that the maximum number of concurrent
experiments when executing S is maxi ni (since experiments from two stages will
not overlap with probability p). As such, we are interested in ﬁnding staged
schedules that are p-safe for a user speciﬁed p, e.g. 95%. Meanwhile, we want to
maximize CPE. PN Pi?1 The CPE of any safe execution of S (slightly abusing
notation) is: CPE(S) = i=2 ni j=1 nj . Typical applications will use relative
high values of p, since otherwise experimental resources would be wasted, and
thus with high probability we expect the CPE of an execution of S to equal
CPE(S). Our goal is thus to maximize CPE(S) while ensuring p-safeness.
It
turns out that for any ﬁxed number of stages N , the schedules that maximize
CPE(S) must be uniform. A staged schedule is deﬁned to be uniform if ?i, j,
—ni ? nj — ? 1, i.e., the batch sizes across stages may diﬀer by at most a single
experiment. Proposition 1. For any number of experiments n and labs l, let SN
be the set of corresponding N stage schedules, where N ? dn/le. For any S ? SN
, CPE(S) = maxS 0 ?SN CPE(S 0 ) if and only if S is uniform. It is easy to
verify that for a given n and l, an N stage uniform schedule achieves a strictly
higher CPE than any N ? 1 stage schedule. This implies that we should prefer
uniform schedules with maximum number of stages allowed by the p-safeness
restriction. This motivates us to solve the following problem: Find a p-safe
uniform schedule with maximum number of stages.
Algorithm 1 Algorithm for computing a p-safe uniform schedule with max-
imum number of stages. Input:number of experiments (n), number of labs (l),
horizon (h), safety probability (p) Output:A p-safe uniform schedule with max-
imum number of stages N = dn/le, S ? null loop S 0 ? MaxProbUniform(N, n,
l, h) if S 0 is not p-safe then return S end if S ? S0, N ? N + 1 end loop
Our approach, outlined in Algorithm 1, considers N stage schedules in order
of increasing N , starting at the minimum possible number of stages N = dn/le
for running all experiments. For each value of N , the call to MaxProbUniform
computes a uniform schedule S with the highest probability of a safe execution,
among all N stage uniform schedules. If the resulting schedule is p-safe then we
consider N + 1 stages. Otherwise, there is no uniform N stage schedule that is
p-safe and we return a uniform N ? 1 stage schedule, which was computed in
the previous iteration. 4
It remains to describe the MaxProbUniform function, which computes a
uniform N stage schedule S = h(ni , di )iN i=1 that maximizes the probability
of a safe execution. First, any N stage uniform schedule must have N 0 = (n
mod N ) stages with n0 = bn/N c+1 experiments and N ?N 0 stages with n0
?1 experiments. Furthermore, the probability of a safe execution is invariant to
the ordering of the stages, since we assume i.i.d. distribution on the experiment
durations. The MaxProbUniform problem is now reduced to computing the
durations di of S that maximize the probability of safeness for each given ni
. For this we will assume that the distribution of the experiment duration pd
is log-concave, which allows us to characterize the solution using the following

6

lemma. Lemma 1. For any duration distribution pd that is log-concave, if an N
stage schedule S = h(ni , di )iN i=1 0 0 is p-safe, then there is a p-safe N stage
schedule S 0 = h(ni , d0i )iN i=1 such that if ni = nj then di = dj . This lemma
suggests that any stages with equal ni ?s should have equal di ?s to maximize
the probability of safe execution. For a uniform schedule, ni is either n0 or n0
? 1. Thus we only need to consider schedules with two durations, d0 for stages
with ni = n0 and d00 for stages with ni = n0 ? 1. Since all durations must 0 ?N
0 0 sum to h, d0 and d00 are deterministically related by: d00 = h?d N ?N 0 .
Based on this, for any value of d the probability of the uniform schedule using
durations d0 and d00 is as follows, where Pd is the CDF of pd .
Pd (d0 )
N 0 ?n0
Pd
h ? d0 ? N 0 N ? N0
(N ?N 0 )?(n0 ?1) (1)
We compute MaxProbUniform by maximizing Equation 1 with respect to
d0 and using the corresponding duration for d00 . Putting everything together
we get the following result. Theorem 1. For any log-concave pd , comput-
ing MaxProbUniform by maximizing Equation 1 over d0 , if a p-safe uniform
schedule exists, Algorithm 1 returns a maximum-stage p-safe uniform schedule.
4.2 Independent Lab Schedules We now consider a more general class of oﬄine
schedules and a heuristic algorithm for computing them. This class allows the
start times of diﬀerent labs to be decoupled, desirable in settings where labs are
run by independent experimenters. Further, our online scheduling approach is
based on repeatedly calling an oﬄine scheduler, which requires the ﬂexibility
to make schedules for labs in diﬀerent stages of execution. An independent lab
(IL) P schedule S speciﬁes a number of labs k ¡ l and for each lab i, a number
of i experiments mi such that i mi = n. Further, for each lab i a sequence of
mi durations Di = hd1i , .
.
.
, dm i i is given. The execution of S runs
each lab independently, by having each lab start up experiments whenever they
move to the next stage. Stage j of lab i ends after a duration of dji , or after
the experiment ﬁnishes when it runs longer than dji (i.e. we do not terminate
experiments). Each experiment is selected according to SelectBatch, given in-
formation about all completed and running experiments across all labs. We
say that an execution of an IL schedule is safe if all experiments ﬁnish within
their speciﬁed durations, which also yields a notion of p-safeness. We are again
interested in computing p-safe schedules that maximizes the CPE. Intuitively,
CPE will be maximized if the amount of concurrency during an execution is
minimized, suggesting the use of as few labs as possible. This motivates the
problem of ﬁnding a p-safe IL schedule that use the minimum number of labs.
Below we describe our heuristic approach to this problem. Algorithm Descrip-
tion. Starting with k = 1, we compute a k labs IL schedule with the goal of
maximizing the probability of safe execution.
If this probability is less than
p, we increment k, and otherwise output the schedule for k labs. To compute
a schedule for each value of k, we ﬁrst allocate the number of experiments mi
across k labs as uniformly as possible. In particular, (n mod k) labs will have

7

bn/kc + 1 experiments and k ? (n mod k) labs will have bn/kc experiments.
This choice is motivated by the intuition that the best way to maximize the
probability of a safe execution is to distribute the work across labs as uniformly
as possible. Given mi for each lab, we assign all durations of lab i to be h/mi
, which can be shown to be optimal for log-concave pd . In this way, for each
value of k the schedule we compute has just two possible values of mi and labs
with the same mi have the same stage durations. 5
5
Online Scheduling Approaches
We now consider online scheduling, which selects the start time of exper-
iments online. The ﬂexibility of the online approaches oﬀers the potential to
outperform oﬄine schedules by adapting to speciﬁc stochastic outcomes ob-
served during experimental runs. Below we ﬁrst describe two baseline online
approaches, followed by our main approach, policy switching, which aims to
directly optimize CPE. Online Fastest Completion Policy (OnFCP). This base-
line policy simply tries to ﬁnish all of the n experiments as quickly as possible.
As such, it keeps all l labs busy as long as there are experiments left to run.
Speciﬁcally whenever a lab (or labs) becomes free the policy immediately uses
SelectBatch with the latest information to select new experiments to start right
away. This policy will achieve a low value of expected CPE since it maximizes
concurrency. Online Minimum Eager Lab Policy (OnMEL). One problem with
OnFCP is that it does not attempt to use the full time horizon. The OnMEL
policy simply restricts OnFCP to use only k labs, where k is the minimum
number of labs required to guarantee with probability at least p that all n
experiments complete within the horizon. Monte-Carlo simulation is used to
estimate p for each k. Policy Switching (PS). Our policy switching approach
decides the number of new experiments to start at each decision epoch. Decision
epochs are assumed to occur every ? units of time, where ? is a small constant
relative to the expected experiment durations. The motivation behind policy
switching is to exploit the availability of a policy generator that can produce
multiple policies at any decision epoch, where at least one of them is expected
to be good. Given such a generator, the goal is to deﬁne a new (switching)
policy that performs as well or better than the best of the generated policies in
any state. In our case, the ob jective is to improve CPE, though other ob jectives
can also be used. This is motivated by prior work on policy switching [6] over a
ﬁxed policy library, and generalize that work to handle arbitrary policy gener-
ators instead of static policy libraries. Below we describe the general approach
and then the speciﬁc policy generator that we use. Let t denote the number of
remaining decision epochs (stages-to-go), which is originally equal to bh/?c and
decremented by one each epoch. We use s to denote the experimental state of
the scheduling problem, which encodes the number of completed experiments
and ongoing experiments with their elapsed running time. We assume access to
a policy generator ?(s, t) which returns a set of base scheduling policies (pos-
sibly nonstationary) given inputs s and t. Prior work on policy switching [6]
corresponds to the case where ?(s, t) returns a ﬁxed set of policies regardless of s
and t. Given ?(s, t), ? ? (s, t, ?) denotes the resulting switching policy based on

8

s, t, and the base policy ? selected in the previous epoch. The decision returned
by ? ? is computed by ﬁrst conducting N simulations of each policy returned by
?(s, t) along with ? to estimate their CPEs. The base policy with the highest
estimated CPE is then selected and its decision is returned by ? ? . The need
to compare to the previous policy ? is due to the use of a dynamic policy gen-
erator, rather than a ﬁxed library. The base policy passed into policy switching
for the ﬁrst decision epoch can be arbitrary. Despite its simplicity, we can make
guarantees about the quality of ? ? assuming a bound on the CPE estimation
error. In particular, the CPE of the switching policy will not be much worse
than the best of the policies produced by our generator given accurate simula-
tions. We say that a CPE estimator is -accurate if it can estimate the CPE Ct?
(s) of any base policy ? for any s and t within an accuracy bound of . Below we
denote the expected CPE of ? ? for s, t, and ? to be Ct?? (s, ?). Theorem 2.
Let ?(s, t) be a policy generator and ? ? be the switching policy computed with
(s, ?) ? max?0 ??(s,t)?{?} Ct? (s) ? 2t. We use a simple policy generator ?(s,
-accurate 0 estimates. For any state s, stages-to-go t, and base policy ?, Ct??
t) that makes multiple calls to the oﬄine IL scheduler described earlier. The
intuition is to notice that the produced p-safe schedules are fairly pessimistic in
terms of the experiment runtimes. In reality many experiments will ﬁnish early
and we can adaptively exploit such situations. Speciﬁcally, rather than follow
the ﬁxed oﬄine schedule we may choose to use fewer labs and hence improve
CPE. Similarly if experiments run too long, we will increase the number of labs.
6

Table 1: Benchmark Functions
1 ? (u2 + v 2 ? 0.3cos(3?u) ? 0.3cos(3?v)) Rosenbrock(2)[1] 10 ? 100(y ?
x2 )2 ? (1 ? x)2 u = 1.6x ? 0.5,d v = 1.6y ? 0.5 2 2 20 P ?i=1 4?i exp ??j=1 Aij
(xj ? Pij ) i Hartman(3,6)[7] Michalewicz(5)[9]? 5i=1 sin(xi ). sin i.x ? ?1?4 ,
A4?d , P4?d are constants 1 Shekel(4)[7] ?10 ?1?10 , A4?10 are constants i=1 ?
+? 4(x ?A )2 Cosines(2)[1]
i
j=1
j
ji
We deﬁne ?(s, t) to return k + 1 policies, {?(s,t,0) , . . . , ?(s,t,k) }, where
k is the number of experiments running in s. Policy ?(s,t,i) is deﬁned so that it
waits for i current experiments to ﬁnish, and then uses the oﬄine IL scheduler
to return a schedule. This amounts to adding a small lookahead to the oﬄine IL
scheduler where diﬀerent amounts of waiting time are considered 1 . Note that
the deﬁnition of these policies depends on s and t and hence can not be viewed
as a ﬁxed set of static policies as used by traditional policy switching. In the
initial state s0 , ?(s0 ,h,0) corresponds to the oﬄine IL schedule and hence the
above theorem guarantees that we will not perform much worse than the oﬄine
IL, with the expectation of performing much better. Whenever policy switching
selects a ?i with i ¿ 0 then no new experiments will be started and we wait
for the next decision epoch. For i = 0, it will apply the oﬄine IL scheduler to
return a p-safe schedule to start immediately, which may require starting new

9

labs to ensure high probability of completing n experiments.
6
Experiments
Implementation of SelectBatch. Given the set of completed experiments
O and on-going experiments A, SelectBatch selects k new experiments. We
implement SelectBatch based on a recent batch BO algorithm [2], which greedily
selects k experiments considering only O. We modify this greedy algorithm to
also consider A by forcing the selected batch to include the ongoing experiments
plus k additional experiments. SelectBatch makes selections based on a posterior
over the unknown function f . We use Gaussian Process Pd with the RBF
kernel and the kernel width = 0.01 i=1 li , where li is the input space length in
dimension i. Benchmark Functions. We evaluate our scheduling policies using
6 well-known synthetic benchmark functions (shown in Tab. 1 with dimension
inside the parenthesis) and two real-world benchmark functions Hydrogen and
FuelCell over [0, 1]2 [2]. The Hydrogen data is produced by a study on biosolar
hydrogen production [5], where the goal was to maximize hydrogen production
of a particular bacteria by optimizing PH and Nitrogen levels. The FuelCell
data was collected in our motivating application mentioned in Sect. 1. In both
cases, the benchmark function was created by ﬁtting regression models to the
available data. Evaluation. We consider a p-safeness guarantee of p = 0.95 and
the number of available labs l is 10. For pd (x), we use one sided truncated
normal distribution such that x ? (0, inf ) with ? = 1, ? 2 = 0.1, and we set
the total number of experiments n = 20. We consider three time horizons h
of 6, 5, and 4. Given l, n and h, to evaluate policy ? using function f (with
a set of initial observed experiments), we execute ? and get a set X of n or
fewer completed experiments. We measure the regret of ? as the diﬀerence
between the optimal value of f (known for all eight functions) and the f value of
the predicted best experiment in X. Results. Table 2 shows the results of our
proposed oﬄine and online schedulers. We also include, as a reference point,
the result of the un-constrained sequential policy (i.e., selecting one experiment
at a time) using SelectBatch, which can be viewed as an eﬀective upper bound
on the optimal performance of any constrained scheduler because it ignores the
time horizon (h = ?). The values in the table correspond to the regrets (smaller
values are better) achieved by each policy, averaged across 100 independent runs
with the same initial experiments (5 for 2-d and 3-d functions and 20 for the
rest) for all policies in each run. 1 For simplicity our previous discussion of the
IL scheduler did not consider states with ongoing experiments, which will occur
here. To handle this the scheduler ﬁrst considers using already executing labs
taking into account how long they have been running. If more labs are required
to ensure p-safeness new ones are added.
7
Table 2: The proposed policies results for diﬀerent horizons. h=4 Functionh
= ? OnFCP OfStaged OfIL OnMEL Cosines .142 .339 .181 .195 .275 .182 .191
.258 FuelCell .160 .240 Hydro .025 .115 .069 .070 .123 .008 .013 .010 .009 .013
Rosen Hart(3) .037 .095 .070 .069 .096 .509 .508 .525 Michal .465 .545 Shekel
.427 .660 .630 .648 .688 Hart(6) .265 .348 .338 .340 .354 CPE 190 55 100 100 66

10

h=5 PS OfStaged OfIL OnMEL .205 .181 .194 .274 .206 .167 .190 .239 .059
.071 .069 .086 .008 .009 .008 .011 .067 .055 .064 .081 .502 .500 .510 .521 .623
.635 .645 .682 .347 .334 .330 .333 100 100 100 91
h=6 PS OfStaged OfIL OnMEL .150 .167 .147 .270 .185 .154 .163 .230 .042
.036 .035 .064 .008 .007 .009 .010 .045 .045 .050 .070 .494 .477 .460 .502 .540
.530 .564 .576 .297 .304 .266 .301 118 133 137 120
PS .156 .153 .025 .009 .038 .480 .510 .262 138
We ﬁrst note that the two oﬄine algorithms (OfStages and OfIL) perform
similarly across all three horizon settings. This suggests that there is limited
beneﬁt in these scenarios to using the more ﬂexible IL schedules, which were
primarily introduced for use in the online scheduling context. Comparing with
the two online baselines (OnFCP and OnMEL), the oﬄine algorithms perform
signiﬁcantly better. This may seem surprising at ﬁrst because online policies
should oﬀer more ﬂexibility than ﬁxed oﬄine schedules. However, the oﬄine
schedules purposefully wait for experiments to complete before starting up new
experiments, which tends to improve the CPE values. To see this, the last
row of Table 2 gives the average CPEs of each policy. Both OnFCP and On-
MEL yield signiﬁcantly lower CPEs compared to the oﬄine algorithms, which
correlates with their signiﬁcantly larger regrets. Finally, policy switching con-
sistently outperforms other policies (excluding h = ?) on the medium horizon
setting and performs similarly in the other settings. This makes sense since the
added ﬂexibility of PS is not as critical for long and short horizons. For short
horizons, there is less opportunity for scheduling choices and for longer horizons
the scheduling problem is easier and hence the oﬄine approaches are more com-
petitive. In addition, looking at Table 2, we see that PS achieves a signiﬁcantly
higher CPE than oﬄine approaches in the medium horizon, and is similar to
them in the other horizons, again correlating with the regret. Further examina-
tion of the schedules produced by PS indicates that although it begins with the
same number of labs as OfIL, PS often selects fewer labs in later steps if early
experiments are completed sooner than expected, which leads to higher CPE
and consequently better performance. Note that the variances of the proposed
policies are very small which are shown in the supplementary materials.
7
Summary and Future Work
Motivated by real-world applications we introduced a novel setting for Bayesian
optimization that incorporates a budget on the total time and number of ex-
periments and allows for concurrent, stochastic-duration experiments. We con-
sidered oﬄine and online approaches for scheduling experiments in this setting,
relying on a black box function to intelligently select speciﬁc experiments at
their scheduled start times. These approaches aimed to optimize a novel ob-
jective function, Cumulative Prior Experiments (CPE), which we empirically
demonstrate to strongly correlate with performance on the original optimiza-
tion problem. Our oﬄine scheduling approaches signiﬁcantly outperformed some
natural baselines and our online approach of policy switching was the best over-
all performer. For further work we plan to consider alternatives to CPE, which,
for example, incorporate factors such as diminishing returns. We also plan to

11

study further extensions to the experimental model for BO and also for active
learning. For example, taking into account varying costs and duration distri-
butions across labs and experiments. In general, we believe that there is much
opportunity for more tightly integrating scheduling and planning algorithms
into BO and active learning to more accurately model real-world conditions.
Acknowledgments The authors acknowledge the support of the NSF under
grants IIS-0905678. 8

2 References

[1] B. S. Anderson, A. Moore, and D. Cohn. A nonparametric approach to
noisy and costly optimization.
In ICML, 2000.
[2] J. Azimi, A. Fern, and
X. Fern. Batch bayesian optimization via simulation matching.
In NIPS,
2010. [3] D. Bond and D. Lovley. Electricity production by geobacter sulfurre-
ducens attached to electrodes. Applications of Environmental Microbiology,
69:1548?1555, 2003.
[4] E. Brochu, M. Cora, and N. de Freitas. A tutorial
on Bayesian optimization of expensive cost functions, with application to ac-
tive user modeling and hierarchical reinforcement learning. Technical Report
TR-2009-23, Department of Computer Science, University of British Columbia,
2009. [5] E. H. Burrows, W.-K. Wong, X. Fern, F. W. Chaplen, and R. L. Ely.
Optimization of ph and nitrogen for enhanced hydrogen production by syne-
chocystis sp. pcc 6803 via statistical and machine learning methods. Biotech-
nology Progress, 25:1009?1017, 2009.
[6] H. Chang, R. Givan, and E. Chong.
Parallel rollout for online solution of partially observable markov decision pro-
cesses. Discrete Event Dynamic Systems, 14:309?341, 2004.
[7] L. Dixon and
G. Szeg. The Global Optimization Problem: An Introduction Toward Global
Optimization. NorthHolland, Amsterdam, 1978.
[8] D. Jones. A taxonomy
of global optimization methods based on response surfaces. Journal of Global
Optimization, pages 345?383, 2001.
[9] Z. Michalewicz. Genetic algorithms
+ data structures = evolution programs (2nd, extended ed.). Springer-Verlag
New York, Inc., New York, NY, USA, 1994.
[10] D. Park and J. Zeikus. Im-
proved fuel cell and electrode designs for producing electricity from microbial
degradation. Biotechnol.Bioeng., 81(3):348?355, 2003.
9

12

