{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5196 Assessment 2\n",
    "<a id=\"FIT5196\"></a>\n",
    "\n",
    "#### Student Name: Jiawei Su\n",
    "#### Student ID: 29590183\n",
    "\n",
    "\n",
    "#### Student Name: Weiwei Jin\n",
    "#### Student ID: 28106946\n",
    "\n",
    "Date: 08/31/2019\n",
    "\n",
    "Version: 1.5\n",
    "\n",
    "Environment: Python 3.7.1 and Jupyter notebook\n",
    "\n",
    "Libraries used:\n",
    "* pandas (for dataframe, included in Anaconda Python 3.7.1) \n",
    "* re (for regular expression, included in Anaconda Python 3.7.1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "\n",
    "* [Student Information](#FIT5196)\n",
    "* [1. Introduction](#Introduction)\n",
    "* [2. Import libraries](#Lib)\n",
    "* [3. Regular Expressions Design](#Reg)\n",
    "   * [3.1. Approach](#app)\n",
    "   * [3.2. Patterns](#pat)\n",
    "   * [3.2. Explanation of Patterns](#Expat)\n",
    "* [4. Functions](#Functions)\n",
    "* [5. Processing the file](#Processing)\n",
    "   * [5.1. Read file](#read)\n",
    "   * [5.2. Process data and generate CSV file](#csv)\n",
    "   * [5.3. Process data and generate JSON file](#json)\n",
    "* [6. Summary](#Summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import importlib\n",
    "import sys\n",
    "importlib.reload(sys)\n",
    "\n",
    "from pdfminer.pdfparser import PDFParser,PDFDocument\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import PDFPageAggregator\n",
    "from pdfminer.layout import LTTextBoxHorizontal,LAParams\n",
    "from pdfminer.pdfinterp import PDFTextExtractionNotAllowed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2. Parse XXXX File\n",
    "\n",
    "In this section, you can write your python scripts to parse the correspondiing file.\n",
    "You should \n",
    "* write proper notes for all code block in this notebook using the Markdown cells\n",
    "* provide proper comment in your scripts\n",
    "* run all cells to make sure scripts are runable. If the scripts cannot be run by the assessors, they will not be assessed and zero mark will be given to the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_SOURCE_PATH = r'Group067.pdf'\n",
    "TEXT_ORIGINAL_PATH = 'links_orig.txt'\n",
    "\n",
    "# Function to parse a PDF file, preprocess it, add the raw content of each line to a text file.\n",
    "def parse_to_txt(path, outpath):\n",
    "    # Open file in binary read mode\n",
    "    fp = open(path, 'rb')\n",
    "    \n",
    "    # Use file to breate a PDF parser \n",
    "    praser = PDFParser(fp)\n",
    "    \n",
    "    # Generate a PDF document\n",
    "    doc = PDFDocument()\n",
    "    \n",
    "    # pass document to parser\n",
    "    praser.set_document(doc)\n",
    "    doc.set_parser(praser)\n",
    "\n",
    "    doc.initialize()\n",
    "    \n",
    "    if not doc.is_extractable:\n",
    "        raise PDFTextExtractionNotAllowed\n",
    "    else:\n",
    "        # Create PDF resource manager\n",
    "        rsrcmgr = PDFResourceManager()\n",
    "        \n",
    "        # Create a PDF device object\n",
    "        laparams = LAParams()\n",
    "        device = PDFPageAggregator(rsrcmgr, laparams=laparams)\n",
    "        \n",
    "        # Create a PDF interpreter object\n",
    "        interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "\n",
    "        # traverse page list obtained from get_pages()\n",
    "        for page in doc.get_pages(): \n",
    "            interpreter.process_page(page)\n",
    "            \n",
    "            layout = device.get_result()\n",
    "            for x in layout:\n",
    "                if (isinstance(x, LTTextBoxHorizontal)):\n",
    "                    with open(outpath, 'a') as f:\n",
    "                        results = x.get_text()\n",
    "                        f.write(results + '\\n')\n",
    "\n",
    "\n",
    "\n",
    "# Function to parse a PDF file, preprocess it, prepare it for processing.\n",
    "def parse_to_proc(path):\n",
    "    # Open file in binary read mode\n",
    "    fp = open(path, 'rb')\n",
    "    \n",
    "    # Use file to breate a PDF parser \n",
    "    praser = PDFParser(fp)\n",
    "    \n",
    "    # Generate a PDF document\n",
    "    doc = PDFDocument()\n",
    "    \n",
    "    # pass document to parser\n",
    "    praser.set_document(doc)\n",
    "    doc.set_parser(praser)\n",
    "\n",
    "    doc.initialize()\n",
    "    \n",
    "    if not doc.is_extractable:\n",
    "        raise PDFTextExtractionNotAllowed\n",
    "    else:\n",
    "        # Create PDF resource manager\n",
    "        rsrcmgr = PDFResourceManager()\n",
    "        \n",
    "        # Create a PDF device object\n",
    "        laparams = LAParams()\n",
    "        device = PDFPageAggregator(rsrcmgr, laparams=laparams)\n",
    "        \n",
    "        # Create a PDF interpreter object\n",
    "        interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "        \n",
    "        # Define a dictionary to return abstract, titles and body of every PDF file.\n",
    "        essay_dict = {}\n",
    "        essay_dict[\"abstract\"] = \"\"\n",
    "        essay_dict[\"titles\"] = \"\"\n",
    "        essay_dict[\"body\"] = \"\"\n",
    "        \n",
    "        return_string = \"\"\n",
    "\n",
    "        # Traverse page list obtained from get_pages()\n",
    "        for page in doc.get_pages(): \n",
    "            interpreter.process_page(page)\n",
    "            \n",
    "            layout = device.get_result()\n",
    "            for x in layout:\n",
    "                if (isinstance(x, LTTextBoxHorizontal)):\n",
    "                    results = x.get_text()\n",
    "                    if (len(results) <= 3):\n",
    "                        pass\n",
    "                    else:\n",
    "                        if \"Authored by:\" in results:\n",
    "                            essay_dict[\"titles\"] = return_string.strip(\"\\n\")\n",
    "                            return_string = \"\"\n",
    "                        elif \"Abstract\" in results:\n",
    "                            return_string = \"\"\n",
    "                            continue\n",
    "                        elif \"1 Paper Body\" in results:\n",
    "                            essay_dict[\"abstract\"] = return_string.strip(\"\\n\")\n",
    "                            return_string = \"\"\n",
    "                            continue\n",
    "                        elif \"2 References\" in results:\n",
    "                            temp_str = return_string.replace(\"\\n\", \" \")\n",
    "                            final_str = temp_str.replace(\"- \", \"\")\n",
    "                            essay_dict[\"body\"] = final_str\n",
    "                        return_string += results.strip(\"\\n\")                    \n",
    "        return essay_dict\n",
    "\n",
    "                        \n",
    "# Function to read the a text file, preprocess it, add the content of the text file to a list and return the list.\n",
    "def read_file(path):        \n",
    "    content = []\n",
    "    try:\n",
    "        with open(path, 'r', encoding = 'utf-8') as fp: \n",
    "            for line in fp:\n",
    "                if 'url' in line:\n",
    "                    pass\n",
    "                elif 'filename' in line:\n",
    "                    pass\n",
    "                else:\n",
    "                    if line:\n",
    "                        content.append(line.strip('\\n'))\n",
    "    finally:\n",
    "        fp.close()\n",
    "    return content\n",
    "\n",
    "def catch_pdf(file_list):\n",
    "    COUNT = 1\n",
    "    filenames = []\n",
    "    for entry in file_list:\n",
    "        url = entry.split()[1]\n",
    "        print('downloading file No. ' + str(COUNT) + ' with urllib...')\n",
    "        COUNT += 1\n",
    "        filenames.append(entry.split()[0])\n",
    "        OUTPUT_PATH = \"pdf/\" + entry.split()[0]\n",
    "        urllib.request.urlretrieve(url, OUTPUT_PATH)\n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_to_txt(PDF_SOURCE_PATH, TEXT_ORIGINAL_PATH)\n",
    "\n",
    "str_list = list(filter(None, read_file(TEXT_ORIGINAL_PATH))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading file No. 1 with urllib...\n",
      "downloading file No. 2 with urllib...\n",
      "downloading file No. 3 with urllib...\n",
      "downloading file No. 4 with urllib...\n",
      "downloading file No. 5 with urllib...\n",
      "downloading file No. 6 with urllib...\n",
      "downloading file No. 7 with urllib...\n",
      "downloading file No. 8 with urllib...\n",
      "downloading file No. 9 with urllib...\n",
      "downloading file No. 10 with urllib...\n",
      "downloading file No. 11 with urllib...\n",
      "downloading file No. 12 with urllib...\n",
      "downloading file No. 13 with urllib...\n",
      "downloading file No. 14 with urllib...\n",
      "downloading file No. 15 with urllib...\n",
      "downloading file No. 16 with urllib...\n",
      "downloading file No. 17 with urllib...\n",
      "downloading file No. 18 with urllib...\n",
      "downloading file No. 19 with urllib...\n",
      "downloading file No. 20 with urllib...\n",
      "downloading file No. 21 with urllib...\n",
      "downloading file No. 22 with urllib...\n",
      "downloading file No. 23 with urllib...\n",
      "downloading file No. 24 with urllib...\n",
      "downloading file No. 25 with urllib...\n",
      "downloading file No. 26 with urllib...\n",
      "downloading file No. 27 with urllib...\n",
      "downloading file No. 28 with urllib...\n",
      "downloading file No. 29 with urllib...\n",
      "downloading file No. 30 with urllib...\n",
      "downloading file No. 31 with urllib...\n",
      "downloading file No. 32 with urllib...\n",
      "downloading file No. 33 with urllib...\n",
      "downloading file No. 34 with urllib...\n",
      "downloading file No. 35 with urllib...\n",
      "downloading file No. 36 with urllib...\n",
      "downloading file No. 37 with urllib...\n",
      "downloading file No. 38 with urllib...\n",
      "downloading file No. 39 with urllib...\n",
      "downloading file No. 40 with urllib...\n",
      "downloading file No. 41 with urllib...\n",
      "downloading file No. 42 with urllib...\n",
      "downloading file No. 43 with urllib...\n",
      "downloading file No. 44 with urllib...\n",
      "downloading file No. 45 with urllib...\n",
      "downloading file No. 46 with urllib...\n",
      "downloading file No. 47 with urllib...\n",
      "downloading file No. 48 with urllib...\n",
      "downloading file No. 49 with urllib...\n",
      "downloading file No. 50 with urllib...\n",
      "downloading file No. 51 with urllib...\n",
      "downloading file No. 52 with urllib...\n",
      "downloading file No. 53 with urllib...\n",
      "downloading file No. 54 with urllib...\n",
      "downloading file No. 55 with urllib...\n",
      "downloading file No. 56 with urllib...\n",
      "downloading file No. 57 with urllib...\n",
      "downloading file No. 58 with urllib...\n",
      "downloading file No. 59 with urllib...\n",
      "downloading file No. 60 with urllib...\n",
      "downloading file No. 61 with urllib...\n",
      "downloading file No. 62 with urllib...\n",
      "downloading file No. 63 with urllib...\n",
      "downloading file No. 64 with urllib...\n",
      "downloading file No. 65 with urllib...\n",
      "downloading file No. 66 with urllib...\n",
      "downloading file No. 67 with urllib...\n",
      "downloading file No. 68 with urllib...\n",
      "downloading file No. 69 with urllib...\n",
      "downloading file No. 70 with urllib...\n",
      "downloading file No. 71 with urllib...\n",
      "downloading file No. 72 with urllib...\n",
      "downloading file No. 73 with urllib...\n",
      "downloading file No. 74 with urllib...\n",
      "downloading file No. 75 with urllib...\n",
      "downloading file No. 76 with urllib...\n",
      "downloading file No. 77 with urllib...\n",
      "downloading file No. 78 with urllib...\n",
      "downloading file No. 79 with urllib...\n",
      "downloading file No. 80 with urllib...\n",
      "downloading file No. 81 with urllib...\n",
      "downloading file No. 82 with urllib...\n",
      "downloading file No. 83 with urllib...\n",
      "downloading file No. 84 with urllib...\n",
      "downloading file No. 85 with urllib...\n",
      "downloading file No. 86 with urllib...\n",
      "downloading file No. 87 with urllib...\n",
      "downloading file No. 88 with urllib...\n",
      "downloading file No. 89 with urllib...\n",
      "downloading file No. 90 with urllib...\n",
      "downloading file No. 91 with urllib...\n",
      "downloading file No. 92 with urllib...\n",
      "downloading file No. 93 with urllib...\n",
      "downloading file No. 94 with urllib...\n",
      "downloading file No. 95 with urllib...\n",
      "downloading file No. 96 with urllib...\n",
      "downloading file No. 97 with urllib...\n",
      "downloading file No. 98 with urllib...\n",
      "downloading file No. 99 with urllib...\n",
      "downloading file No. 100 with urllib...\n",
      "downloading file No. 101 with urllib...\n",
      "downloading file No. 102 with urllib...\n",
      "downloading file No. 103 with urllib...\n",
      "downloading file No. 104 with urllib...\n",
      "downloading file No. 105 with urllib...\n",
      "downloading file No. 106 with urllib...\n",
      "downloading file No. 107 with urllib...\n",
      "downloading file No. 108 with urllib...\n",
      "downloading file No. 109 with urllib...\n",
      "downloading file No. 110 with urllib...\n",
      "downloading file No. 111 with urllib...\n",
      "downloading file No. 112 with urllib...\n",
      "downloading file No. 113 with urllib...\n",
      "downloading file No. 114 with urllib...\n",
      "downloading file No. 115 with urllib...\n",
      "downloading file No. 116 with urllib...\n",
      "downloading file No. 117 with urllib...\n",
      "downloading file No. 118 with urllib...\n",
      "downloading file No. 119 with urllib...\n",
      "downloading file No. 120 with urllib...\n",
      "downloading file No. 121 with urllib...\n",
      "downloading file No. 122 with urllib...\n",
      "downloading file No. 123 with urllib...\n",
      "downloading file No. 124 with urllib...\n",
      "downloading file No. 125 with urllib...\n",
      "downloading file No. 126 with urllib...\n",
      "downloading file No. 127 with urllib...\n",
      "downloading file No. 128 with urllib...\n",
      "downloading file No. 129 with urllib...\n",
      "downloading file No. 130 with urllib...\n",
      "downloading file No. 131 with urllib...\n",
      "downloading file No. 132 with urllib...\n",
      "downloading file No. 133 with urllib...\n",
      "downloading file No. 134 with urllib...\n",
      "downloading file No. 135 with urllib...\n",
      "downloading file No. 136 with urllib...\n",
      "downloading file No. 137 with urllib...\n",
      "downloading file No. 138 with urllib...\n",
      "downloading file No. 139 with urllib...\n",
      "downloading file No. 140 with urllib...\n",
      "downloading file No. 141 with urllib...\n",
      "downloading file No. 142 with urllib...\n",
      "downloading file No. 143 with urllib...\n",
      "downloading file No. 144 with urllib...\n",
      "downloading file No. 145 with urllib...\n",
      "downloading file No. 146 with urllib...\n",
      "downloading file No. 147 with urllib...\n",
      "downloading file No. 148 with urllib...\n",
      "downloading file No. 149 with urllib...\n",
      "downloading file No. 150 with urllib...\n",
      "downloading file No. 151 with urllib...\n",
      "downloading file No. 152 with urllib...\n",
      "downloading file No. 153 with urllib...\n",
      "downloading file No. 154 with urllib...\n",
      "downloading file No. 155 with urllib...\n",
      "downloading file No. 156 with urllib...\n",
      "downloading file No. 157 with urllib...\n",
      "downloading file No. 158 with urllib...\n",
      "downloading file No. 159 with urllib...\n",
      "downloading file No. 160 with urllib...\n",
      "downloading file No. 161 with urllib...\n",
      "downloading file No. 162 with urllib...\n",
      "downloading file No. 163 with urllib...\n",
      "downloading file No. 164 with urllib...\n",
      "downloading file No. 165 with urllib...\n",
      "downloading file No. 166 with urllib...\n",
      "downloading file No. 167 with urllib...\n",
      "downloading file No. 168 with urllib...\n",
      "downloading file No. 169 with urllib...\n",
      "downloading file No. 170 with urllib...\n",
      "downloading file No. 171 with urllib...\n",
      "downloading file No. 172 with urllib...\n",
      "downloading file No. 173 with urllib...\n",
      "downloading file No. 174 with urllib...\n",
      "downloading file No. 175 with urllib...\n",
      "downloading file No. 176 with urllib...\n",
      "downloading file No. 177 with urllib...\n",
      "downloading file No. 178 with urllib...\n",
      "downloading file No. 179 with urllib...\n",
      "downloading file No. 180 with urllib...\n",
      "downloading file No. 181 with urllib...\n",
      "downloading file No. 182 with urllib...\n",
      "downloading file No. 183 with urllib...\n",
      "downloading file No. 184 with urllib...\n",
      "downloading file No. 185 with urllib...\n",
      "downloading file No. 186 with urllib...\n",
      "downloading file No. 187 with urllib...\n",
      "downloading file No. 188 with urllib...\n",
      "downloading file No. 189 with urllib...\n",
      "downloading file No. 190 with urllib...\n",
      "downloading file No. 191 with urllib...\n",
      "downloading file No. 192 with urllib...\n",
      "downloading file No. 193 with urllib...\n",
      "downloading file No. 194 with urllib...\n",
      "downloading file No. 195 with urllib...\n",
      "downloading file No. 196 with urllib...\n",
      "downloading file No. 197 with urllib...\n",
      "downloading file No. 198 with urllib...\n",
      "downloading file No. 199 with urllib...\n",
      "downloading file No. 200 with urllib...\n"
     ]
    }
   ],
   "source": [
    "filenames = catch_pdf(str_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_TEST_PATH = r'pdf/PP4300.pdf'\n",
    "TEXT_TEST_PATH = 'test1.txt'\n",
    "\n",
    "parse_to_txt(PDF_TEST_PATH, TEXT_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = parse_to_proc(PDF_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Budgeted Optimization with Concurrent\\nStochastic-Duration Experiments'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[\"titles\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Budgeted optimization involves optimizing an unknown function that\\nis costly to evaluate by requesting a limited number of function evaluations\\nat intelligently selected inputs. Typical problem formulations assume that\\nexperiments are selected one at a time with a limited total number of ex-\\nperiments, which fail to capture important aspects of many real-world\\nproblems. This paper deﬁnes a novel problem formulation with the fol-\\nlowing important extensions: 1) allowing for concurrent experiments; 2)\\nallowing for stochastic experiment durations; and 3) placing constraints\\non both the total number of experiments and the total experimental time.\\nWe develop both oﬄine and online algorithms for selecting concurrent ex-\\nperiments in this new setting and provide experimental results on a num-\\nber of optimization benchmarks. The results show that our algorithms\\nproduce highly eﬀective schedules compared to natural baselines.'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[\"abstract\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We study the optimization of an unknown function f by requesting n experiments, each specifying an input x and producing a noisy observation of f (x). In practice, the function f might be the performance of a device parameterized by x. We consider the setting where running experiments is costly (e.g. in terms of time), which renders methods that rely on many function evaluations, such as stochastic search or empirical gradient methods, impractical. Bayesian optimization (BO) [8, 4] addresses this issue by leveraging Bayesian modeling to maintain a posterior over the unknown function based on previous experiments. The posterior is then used to intelligently select new experiments to trade-oﬀ exploring new parts of the experimental space and exploiting promising parts. Traditional BO follows a sequential approach where only one experiment is selected and run at a time. However, it is often desirable to select more than one experiment at a time so that multiple experiments can be run simultaneouslyto leverage parallel facilities. Recently, Azimi et al. (2010) proposed a batch BO algorithm that selects a batch of k ? 1 experiments at a time. While this broadens the applicability of BO, it is still limited to selecting a ﬁxed number of experiments at each step. As such, prior work on BO, both batch and sequential, completely ignores the problem of how to schedule experiments under ﬁxed experimental budget and time constraints. Furthermore, existing work assumes that the durations of experiments are identical and deterministic, whereas in practice they are often stochastic. Consider one of our motivating applications of optimizing the power output of nano-enhanced Microbial Fuel Cells (MFCs). MFCs [3] use micro-organisms to generate electricity. Their performance depends 1 strongly on the surface properties of the anode [10]. Our problem involves optimizing nano-enhanced anodes, where various types of nano-structures, e.g. carbon nano-wire, are grown directly on the anode surface. Because there is little understanding of how diﬀerent nano-enhancements impact power output, optimizing anode design is largely guess work. Our original goal was to develop BO algorithms for aiding this process. However, many aspects of this domain complicate the application of BO. First, there is a ﬁxed budget on the number of experiments that can be run due to limited funds and a ﬁxed time period for the pro ject. Second, we can run multiple concurrent experiments, limited by the number of experimental apparatus. Third, the time required to run each experiment is variable because each experiment requires the construction of a nano-structure with speciﬁc properties. Nano-fabrication is highly unpredictable and the amount of time to successfully produce a structure is quite variable. Clearly prior BO models fail to capture critical aspects of the experimental process in this domain. In this paper, we consider the following extensions. First, we have l available labs (which may correspond to experimental stations at one location or to physically distinct laboratories), allowing up to l concurrent experiments. Second, experiments have stochastic durations, independently and identically distributed according to a known density function pd . Finally, we are constrained by a budget of n total experiments and a time horizon h by which point we must ﬁnish. The goal is to maximize the unknown function f by selecting experiments and when to start them while satisfying the constraints. We propose oﬄine (Section 4) and online (Section 5) scheduling approaches for this problem, which aim to balance two competing factors. First, a scheduler should ensure that all n experiments complete within the horizon h, which encourages high concurrency. Second, we wish to select new experiments given as many previously completed experiments as possible to make more intelligent experiment selections, which encourages low concurrency. We introduce a novel measure of the second factor, cumulative prior experiments (CPE) (Section 3), which our approaches aim to optimize. Our experimental results indicate that these approaches signiﬁcantly outperform a set of baselines across a range of benchmark optimization problems. 2 Problem Setup Let X ? ¡d be a d-dimensional compact input space, where each dimension iis bounded in [ai , bi ]. An element of X is called an experiment. An unknown real-valued function f : X ? ¡ represents the expected value of the dependent variable after running an experiment. For example, f (x) might be the result of a wetlab experiment described by x. Conducting an experiment x produces a noisy outcome y = f (x) + , where is a random noise term. Bayesian Optimization (BO) aims to ﬁnd an experiment x ? X that approximately maximizes f by requesting a limited number of experiments and observing their outcomes. We extend traditional BO algorithms and study the experiment scheduling problem. Assuming a known density function pd for the experiment durations, the inputs to our problem include the total number of available labs l, the total number of experiments n, and the time horizon h by which we must ﬁnish. The goal is to design a policy ? for selecting when to start experiments and which ones to start to optimize f . Speciﬁcally, the inputs to ? are the set of completed experiments and their outcomes, the set of currently running experiments with their elapsed running time, the number of free labs, and the remaining time till the horizon. Given this information, ? must select a set of experiments (possibly empty) to start that is no larger than the number of free labs. Any run of the policy ends when either n experiments are completed or the time horizon is reached, resulting in a set X of n or fewer completed experiments. The ob jective is to obtain a policy with small regret, which is the expected diﬀerence between the optimal value of f and the value of f for the predicted best experiment in X. In theory, the optimal policy can be found by solving a POMDP with hidden state corresponding to the unknown function f . However, this POMDP is beyond the reach of any existing solvers. Thus, we focus on deﬁning and comparing several principled policies that work well in practice, but without optimality guarantees. Note that this problem has not been studied in the literature to the best of our knowledge. 2 3 Overview of General Approach A policy for our problem must make two types of decisions: 1) scheduling when to start new experiments, and 2) selecting the speciﬁc experiments to start. In this work, we factor the problem based on these decisions and focus on approaches for scheduling experiments. We assume a black box function SelectBatch for intelligently selecting the k ? 1 experiments based on both completed and currently running experiments. The implementation of SelectBatch is described in Section 6. Optimal scheduling to minimize regret appears to be computationally hard for non-trivial instances of SelectBatch. Further, we desire scheduling approaches that do not depend on the details of SelectBatch, but work well for any reasonable implementation. Thus, rather than directly optimizing regret for a speciﬁc SelectBatch, we consider the following surrogate criteria. First, we want to ﬁnish all n experiments within the horizon h with high probability. Second, we would like to select each experiment based on as much information as possible, measured by the number of previously completed experiments. These two goals are at odds, since maximizing the completion probability requires maximizing concurrency of the experiments, which minimizes the second criterion. Our oﬄine and online scheduling approaches providediﬀerent ways for managing this trade-oﬀ. Cosines 0.32 0.28 0.08 Regret 0.09 0.07 0.26 0.06 0.24 0.05 0.22 0.04 0.2 0.18 Hydrogen 0.1 0.3 Regret To quantify the second criterion, consider a complete execution E of a scheduler. For any experiment e in E, let priorE (e) denote the number of experiments in E that completed before starting e. P We deﬁne the cumulative prior experiments (CPE) of E as: e?E priorE (e). Intuitively, a scheduler with a high expected CPE is desirable, since CPE measures the total amount of information SelectBatch uses to make its decisions. 0 20 40 60 CPE 80 100 120 0.03 0 20 40 60 80 100 120 CPE Figure 1: The correlation between CPE and CPE agrees with intuition when considering extreme policies. regret for 30 diﬀerent schedulers on two BO A poor scheduler that starts all n experiments at the same time benchmarks. (assuming enough labs) will have a minimum CPE of zero. Further, CPE ismaximized by a scheduler that sequentially executes all experiments (assuming enough time). However, in between these extremes, CPE fails to capture certain intuitive properties. For example, CPE increases linearly in the number of prior experiments, while one might expect diminishing returns as the number of prior experiments becomes large. Similarly, as the number of experiments started together (the batch size) increases, we might also expect diminishing returns since SelectBatch must choose the experiments based on the same prior experiments. Unfortunately, quantifying these intuitions in a general way is still an open problem. Despite its potential shortcomings, we have found CPE to be a robust measure in practice. To empirically examine the utility of CPE, we conducted experiments on a number of BO benchmarks. For each domain, we used 30 manually designed diverse schedulers, some started more experiments early on than later, and vice-versa, while others included random and uniform schedules. We measured the average regret achieved for each scheduler given the same inputs and the expected CPE of the executions. Figure 1 shows the results for two of the domains (other results are highly similar), where each point corresponds to the average regret and CPE of a particular scheduler. We observe a clear and non-trivial correlation between regret and CPE, which provides empirical evidence that CPE is a useful measure to optimize. Further, as we will see in our experiments, the performance of our methods is also highly correlated with CPE. 4 Oﬄine Scheduling We now consider oﬄine schedules, which assign start times to all n experiments before the experimental process begins. Note that while the schedules are oﬄine, the overall BO policy has online characteristics, since the exact experiments to run are only speciﬁed when they need to be started by SelectBatch, based 3 on the most recent information. This oﬄine scheduling approach is often convenient in real experimental domains where it is useful to plan out a static equipment/personnel schedule for the duration of a pro ject. Below we ﬁrst consider a restricted class of schedules, called staged schedules, for which we present a solution that optimizes CPE. Next, we describe an approach for a more general class of schedules. 4.1 Staged Schedules A staged schedule deﬁnes a consecutivePsequence of NPexperimental stages, denoted by a sequence of tuples h(ni , di )iN i=1 , where 0 ¡ ni ? l, i di ? h, and i ni ? n. Stage i begins by starting up ni new experiments selected by SelectBatch using the most recent information, and ends after a duration of di , upon which stage i + 1 starts. In some applications, staged schedules are preferable as they allow pro ject planning to focus on a relatively small number of time points (the beginning of each stage). While our approach tries to ensure that experiments ﬁnish within their stage, experiments are never terminated and hence might run longer than their speciﬁed duration. If, because of this, at the beginning of stage i there are not ni free labs, the experiments will wait till labs free up. We say that an execution E of a staged schedule S is safe if each experiment iscompleted within its speciﬁed duration in S. We say that a staged schedule S is p-safe if with probability at least p an execution of S is safe which provides a probabilistic guarantee that all n experiments complete within the horizon h. Further, it ensures with probability p that the maximum number of concurrent experiments when executing S is maxi ni (since experiments from two stages will not overlap with probability p). As such, we are interested in ﬁnding staged schedules that are p-safe for a user speciﬁed p, e.g. 95%. Meanwhile, we want to maximize CPE. PN Pi?1 The CPE of any safe execution of S (slightly abusing notation) is: CPE(S) = i=2 ni j=1 nj . Typical applications will use relative high values of p, since otherwise experimental resources would be wasted, and thus with high probability we expect the CPE of an execution of S to equal CPE(S). Our goal is thus to maximize CPE(S) while ensuring p-safeness. It turns out that for any ﬁxed number of stages N , the schedules that maximize CPE(S) must be uniform. A staged schedule is deﬁned to be uniform if ?i, j, —ni ? nj — ? 1, i.e., the batch sizes across stages may diﬀer by at most a single experiment. Proposition 1. For any number of experiments n and labs l, let SN be the set of corresponding N stage schedules, where N ? dn/le. For any S ? SN , CPE(S) = maxS 0 ?SN CPE(S 0 ) if and only if S is uniform. It is easy to verify that for a given n and l, an N stage uniform schedule achieves a strictly higher CPE than any N ? 1 stage schedule. This implies that we should prefer uniform schedules with maximum number of stages allowed by the p-safeness restriction. This motivates us to solve the following problem: Find a p-safe uniform schedule with maximum number of stages. Algorithm 1 Algorithm for computing a p-safe uniform schedule with maximum number of stages. Input:number of experiments (n), number of labs (l), horizon (h), safety probability (p) Output:A p-safe uniform schedule with maximum number of stages N = dn/le, S ? null loop S 0 ? MaxProbUniform(N, n, l, h) if S 0 is not p-safe then return S end if S ? S0, N ? N + 1 end loop Our approach, outlined in Algorithm 1, considers N stage schedules in order of increasing N , starting at the minimum possible number of stages N = dn/le for running all experiments. For each value of N , the call to MaxProbUniform computes a uniform schedule S with the highest probability of a safe execution, among all N stage uniform schedules. If the resulting schedule is p-safe then we consider N + 1 stages. Otherwise, there is no uniform N stage schedule that is p-safe and we return a uniform N ? 1 stage schedule, which was computed in the previous iteration. 4 It remains to describe the MaxProbUniform function, which computes a uniform N stage schedule S = h(ni , di )iN i=1 that maximizes the probability of a safe execution. First, any N stage uniform schedule must have N 0 = (n mod N ) stages with n0 = bn/N c+1 experiments and N ?N 0 stages with n0 ?1 experiments. Furthermore, the probability of a safe execution is invariant to the ordering of the stages, since we assume i.i.d. distribution on the experiment durations. The MaxProbUniform problem is now reduced to computing the durations di of S that maximize the probability of safeness for each given ni . For this we will assume that the distribution of the experiment duration pd is log-concave, which allows us to characterize the solution using the followinglemma. Lemma 1. For any duration distribution pd that is log-concave, if an N stage schedule S = h(ni , di )iN i=1 0 0 is p-safe, then there is a p-safe N stage schedule S 0 = h(ni , d0i )iN i=1 such that if ni = nj then di = dj . This lemma suggests that any stages with equal ni ?s should have equal di ?s to maximize the probability of safe execution. For a uniform schedule, ni is either n0 or n0 ? 1. Thus we only need to consider schedules with two durations, d0 for stages with ni = n0 and d00 for stages with ni = n0 ? 1. Since all durations must 0 ?N 0 0 sum to h, d0 and d00 are deterministically related by: d00 = h?d N ?N 0 . Based on this, for any value of d the probability of the uniform schedule using durations d0 and d00 is as follows, where Pd is the CDF of pd . Pd (d0 ) N 0 ?n0 Pd h ? d0 ? N 0 N ? N0 (N ?N 0 )?(n0 ?1) (1) We compute MaxProbUniform by maximizing Equation 1 with respect to d0 and using the corresponding duration for d00 . Putting everything together we get the following result. Theorem 1. For any log-concave pd , computing MaxProbUniform by maximizing Equation 1 over d0 , if a p-safe uniform schedule exists, Algorithm 1 returns a maximum-stage p-safe uniform schedule. 4.2 Independent Lab Schedules We now consider a more general class of oﬄine schedules and a heuristic algorithm for computing them. This class allows the start times of diﬀerent labs to be decoupled, desirable in settings where labs are run by independent experimenters. Further, our online scheduling approach is based on repeatedly calling an oﬄine scheduler, which requires the ﬂexibility to make schedules for labs in diﬀerent stages of execution. An independent lab (IL) P schedule S speciﬁes a number of labs k ¡ l and for each lab i, a number of i experiments mi such that i mi = n. Further, for each lab i a sequence of mi durations Di = hd1i , . . . , dm i i is given. The execution of S runs each lab independently, by having each lab start up experiments whenever they move to the next stage. Stage j of lab i ends after a duration of dji , or after the experiment ﬁnishes when it runs longer than dji (i.e. we do not terminate experiments). Each experiment is selected according to SelectBatch, given information about all completed and running experiments across all labs. We say that an execution of an IL schedule is safe if all experiments ﬁnish within their speciﬁed durations, which also yields a notion of p-safeness. We are again interested in computing p-safe schedules that maximizes the CPE. Intuitively, CPE will be maximized if the amount of concurrency during an execution is minimized, suggesting the use of as few labs as possible. This motivates the problem of ﬁnding a p-safe IL schedule that use the minimum number of labs. Below we describe our heuristic approach to this problem. Algorithm Description. Starting with k = 1, we compute a k labs IL schedule with the goal of maximizing the probability of safe execution. If this probability is less than p, we increment k, and otherwise output the schedule for k labs. To compute a schedule for each value of k, we ﬁrst allocate the number of experiments mi across k labs as uniformly as possible. In particular, (n mod k) labs will havebn/kc + 1 experiments and k ? (n mod k) labs will have bn/kc experiments. This choice is motivated by the intuition that the best way to maximize the probability of a safe execution is to distribute the work across labs as uniformly as possible. Given mi for each lab, we assign all durations of lab i to be h/mi , which can be shown to be optimal for log-concave pd . In this way, for each value of k the schedule we compute has just two possible values of mi and labs with the same mi have the same stage durations. 5 5 Online Scheduling Approaches We now consider online scheduling, which selects the start time of experiments online. The ﬂexibility of the online approaches oﬀers the potential to outperform oﬄine schedules by adapting to speciﬁc stochastic outcomes observed during experimental runs. Below we ﬁrst describe two baseline online approaches, followed by our main approach, policy switching, which aims to directly optimize CPE. Online Fastest Completion Policy (OnFCP). This baseline policy simply tries to ﬁnish all of the n experiments as quickly as possible. As such, it keeps all l labs busy as long as there are experiments left to run. Speciﬁcally whenever a lab (or labs) becomes free the policy immediately uses SelectBatch with the latest information to select new experiments to start right away. This policy will achieve a low value of expected CPE since it maximizes concurrency. Online Minimum Eager Lab Policy (OnMEL). One problem with OnFCP is that it does not attempt to use the full time horizon. The OnMEL policy simply restricts OnFCP to use only k labs, where k is the minimum number of labs required to guarantee with probability at least p that all n experiments complete within the horizon. Monte-Carlo simulation is used to estimate p for each k. Policy Switching (PS). Our policy switching approach decides the number of new experiments to start at each decision epoch. Decision epochs are assumed to occur every ? units of time, where ? is a small constant relative to the expected experiment durations. The motivation behind policy switching is to exploit the availability of a policy generator that can produce multiple policies at any decision epoch, where at least one of them is expected to be good. Given such a generator, the goal is to deﬁne a new (switching) policy that performs as well or better than the best of the generated policies in any state. In our case, the ob jective is to improve CPE, though other ob jectives can also be used. This is motivated by prior work on policy switching [6] over a ﬁxed policy library, and generalize that work to handle arbitrary policy generators instead of static policy libraries. Below we describe the general approach and then the speciﬁc policy generator that we use. Let t denote the number of remaining decision epochs (stages-to-go), which is originally equal to bh/?c and decremented by one each epoch. We use s to denote the experimental state of the scheduling problem, which encodes the number of completed experiments and ongoing experiments with their elapsed running time. We assume access to a policy generator ?(s, t) which returns a set of base scheduling policies (possibly nonstationary) given inputs s and t. Prior work on policy switching [6] corresponds to the case where ?(s, t) returns a ﬁxed set of policies regardless of s and t. Given ?(s, t), ? ? (s, t, ?) denotes the resulting switching policy based ons, t, and the base policy ? selected in the previous epoch. The decision returned by ? ? is computed by ﬁrst conducting N simulations of each policy returned by ?(s, t) along with ? to estimate their CPEs. The base policy with the highest estimated CPE is then selected and its decision is returned by ? ? . The need to compare to the previous policy ? is due to the use of a dynamic policy generator, rather than a ﬁxed library. The base policy passed into policy switching for the ﬁrst decision epoch can be arbitrary. Despite its simplicity, we can make guarantees about the quality of ? ? assuming a bound on the CPE estimation error. In particular, the CPE of the switching policy will not be much worse than the best of the policies produced by our generator given accurate simulations. We say that a CPE estimator is -accurate if it can estimate the CPE Ct? (s) of any base policy ? for any s and t within an accuracy bound of . Below we denote the expected CPE of ? ? for s, t, and ? to be Ct?? (s, ?). Theorem 2. Let ?(s, t) be a policy generator and ? ? be the switching policy computed with (s, ?) ? max?0 ??(s,t)?{?} Ct? (s) ? 2t. We use a simple policy generator ?(s, -accurate 0 estimates. For any state s, stages-to-go t, and base policy ?, Ct?? t) that makes multiple calls to the oﬄine IL scheduler described earlier. The intuition is to notice that the produced p-safe schedules are fairly pessimistic in terms of the experiment runtimes. In reality many experiments will ﬁnish early and we can adaptively exploit such situations. Speciﬁcally, rather than follow the ﬁxed oﬄine schedule we may choose to use fewer labs and hence improve CPE. Similarly if experiments run too long, we will increase the number of labs. 6Table 1: Benchmark Functions 1 ? (u2 + v 2 ? 0.3cos(3?u) ? 0.3cos(3?v)) Rosenbrock(2)[1] 10 ? 100(y ? x2 )2 ? (1 ? x)2 u = 1.6x ? 0.5,d v = 1.6y ? 0.5 2 2 20 P ?i=1 4?i exp ??j=1 Aij (xj ? Pij ) i Hartman(3,6)[7] Michalewicz(5)[9]? 5i=1 sin(xi ). sin i.x ? ?1?4 , A4?d , P4?d are constants 1 Shekel(4)[7] ?10 ?1?10 , A4?10 are constants i=1 ? +? 4(x ?A )2 Cosines(2)[1] i j=1 j ji We deﬁne ?(s, t) to return k + 1 policies, {?(s,t,0) , . . . , ?(s,t,k) }, where k is the number of experiments running in s. Policy ?(s,t,i) is deﬁned so that it waits for i current experiments to ﬁnish, and then uses the oﬄine IL scheduler to return a schedule. This amounts to adding a small lookahead to the oﬄine IL scheduler where diﬀerent amounts of waiting time are considered 1 . Note that the deﬁnition of these policies depends on s and t and hence can not be viewed as a ﬁxed set of static policies as used by traditional policy switching. In the initial state s0 , ?(s0 ,h,0) corresponds to the oﬄine IL schedule and hence the above theorem guarantees that we will not perform much worse than the oﬄine IL, with the expectation of performing much better. Whenever policy switching selects a ?i with i ¿ 0 then no new experiments will be started and we wait for the next decision epoch. For i = 0, it will apply the oﬄine IL scheduler to return a p-safe schedule to start immediately, which may require starting newlabs to ensure high probability of completing n experiments. 6 Experiments Implementation of SelectBatch. Given the set of completed experiments O and on-going experiments A, SelectBatch selects k new experiments. We implement SelectBatch based on a recent batch BO algorithm [2], which greedily selects k experiments considering only O. We modify this greedy algorithm to also consider A by forcing the selected batch to include the ongoing experiments plus k additional experiments. SelectBatch makes selections based on a posterior over the unknown function f . We use Gaussian Process Pd with the RBF kernel and the kernel width = 0.01 i=1 li , where li is the input space length in dimension i. Benchmark Functions. We evaluate our scheduling policies using 6 well-known synthetic benchmark functions (shown in Tab. 1 with dimension inside the parenthesis) and two real-world benchmark functions Hydrogen and FuelCell over [0, 1]2 [2]. The Hydrogen data is produced by a study on biosolar hydrogen production [5], where the goal was to maximize hydrogen production of a particular bacteria by optimizing PH and Nitrogen levels. The FuelCell data was collected in our motivating application mentioned in Sect. 1. In both cases, the benchmark function was created by ﬁtting regression models to the available data. Evaluation. We consider a p-safeness guarantee of p = 0.95 and the number of available labs l is 10. For pd (x), we use one sided truncated normal distribution such that x ? (0, inf ) with ? = 1, ? 2 = 0.1, and we set the total number of experiments n = 20. We consider three time horizons h of 6, 5, and 4. Given l, n and h, to evaluate policy ? using function f (with a set of initial observed experiments), we execute ? and get a set X of n or fewer completed experiments. We measure the regret of ? as the diﬀerence between the optimal value of f (known for all eight functions) and the f value of the predicted best experiment in X. Results. Table 2 shows the results of our proposed oﬄine and online schedulers. We also include, as a reference point, the result of the un-constrained sequential policy (i.e., selecting one experiment at a time) using SelectBatch, which can be viewed as an eﬀective upper bound on the optimal performance of any constrained scheduler because it ignores the time horizon (h = ?). The values in the table correspond to the regrets (smaller values are better) achieved by each policy, averaged across 100 independent runs with the same initial experiments (5 for 2-d and 3-d functions and 20 for the rest) for all policies in each run. 1 For simplicity our previous discussion of the IL scheduler did not consider states with ongoing experiments, which will occur here. To handle this the scheduler ﬁrst considers using already executing labs taking into account how long they have been running. If more labs are required to ensure p-safeness new ones are added. 7 Table 2: The proposed policies results for diﬀerent horizons. h=4 Functionh = ? OnFCP OfStaged OfIL OnMEL Cosines .142 .339 .181 .195 .275 .182 .191 .258 FuelCell .160 .240 Hydro .025 .115 .069 .070 .123 .008 .013 .010 .009 .013 Rosen Hart(3) .037 .095 .070 .069 .096 .509 .508 .525 Michal .465 .545 Shekel .427 .660 .630 .648 .688 Hart(6) .265 .348 .338 .340 .354 CPE 190 55 100 100 66h=5 PS OfStaged OfIL OnMEL .205 .181 .194 .274 .206 .167 .190 .239 .059 .071 .069 .086 .008 .009 .008 .011 .067 .055 .064 .081 .502 .500 .510 .521 .623 .635 .645 .682 .347 .334 .330 .333 100 100 100 91 h=6 PS OfStaged OfIL OnMEL .150 .167 .147 .270 .185 .154 .163 .230 .042 .036 .035 .064 .008 .007 .009 .010 .045 .045 .050 .070 .494 .477 .460 .502 .540 .530 .564 .576 .297 .304 .266 .301 118 133 137 120 PS .156 .153 .025 .009 .038 .480 .510 .262 138 We ﬁrst note that the two oﬄine algorithms (OfStages and OfIL) perform similarly across all three horizon settings. This suggests that there is limited beneﬁt in these scenarios to using the more ﬂexible IL schedules, which were primarily introduced for use in the online scheduling context. Comparing with the two online baselines (OnFCP and OnMEL), the oﬄine algorithms perform signiﬁcantly better. This may seem surprising at ﬁrst because online policies should oﬀer more ﬂexibility than ﬁxed oﬄine schedules. However, the oﬄine schedules purposefully wait for experiments to complete before starting up new experiments, which tends to improve the CPE values. To see this, the last row of Table 2 gives the average CPEs of each policy. Both OnFCP and OnMEL yield signiﬁcantly lower CPEs compared to the oﬄine algorithms, which correlates with their signiﬁcantly larger regrets. Finally, policy switching consistently outperforms other policies (excluding h = ?) on the medium horizon setting and performs similarly in the other settings. This makes sense since the added ﬂexibility of PS is not as critical for long and short horizons. For short horizons, there is less opportunity for scheduling choices and for longer horizons the scheduling problem is easier and hence the oﬄine approaches are more competitive. In addition, looking at Table 2, we see that PS achieves a signiﬁcantly higher CPE than oﬄine approaches in the medium horizon, and is similar to them in the other horizons, again correlating with the regret. Further examination of the schedules produced by PS indicates that although it begins with the same number of labs as OfIL, PS often selects fewer labs in later steps if early experiments are completed sooner than expected, which leads to higher CPE and consequently better performance. Note that the variances of the proposed policies are very small which are shown in the supplementary materials. 7 Summary and Future Work Motivated by real-world applications we introduced a novel setting for Bayesian optimization that incorporates a budget on the total time and number of experiments and allows for concurrent, stochastic-duration experiments. We considered oﬄine and online approaches for scheduling experiments in this setting, relying on a black box function to intelligently select speciﬁc experiments at their scheduled start times. These approaches aimed to optimize a novel objective function, Cumulative Prior Experiments (CPE), which we empirically demonstrate to strongly correlate with performance on the original optimization problem. Our oﬄine scheduling approaches signiﬁcantly outperformed some natural baselines and our online approach of policy switching was the best overall performer. For further work we plan to consider alternatives to CPE, which, for example, incorporate factors such as diminishing returns. We also plan tostudy further extensions to the experimental model for BO and also for active learning. For example, taking into account varying costs and duration distributions across labs and experiments. In general, we believe that there is much opportunity for more tightly integrating scheduling and planning algorithms into BO and active learning to more accurately model real-world conditions. Acknowledgments The authors acknowledge the support of the NSF under grants IIS-0905678. 8'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[\"body\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Summary\n",
    "Give a short summary of your work done above, such as your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PP3206.pdf',\n",
       " 'PP3233.pdf',\n",
       " 'PP3235.pdf',\n",
       " 'PP3289.pdf',\n",
       " 'PP3316.pdf',\n",
       " 'PP3367.pdf',\n",
       " 'PP3453.pdf',\n",
       " 'PP3475.pdf',\n",
       " 'PP3527.pdf',\n",
       " 'PP3529.pdf',\n",
       " 'PP3594.pdf',\n",
       " 'PP3596.pdf',\n",
       " 'PP3635.pdf',\n",
       " 'PP3664.pdf',\n",
       " 'PP3693.pdf',\n",
       " 'PP3698.pdf',\n",
       " 'PP3727.pdf',\n",
       " 'PP3729.pdf',\n",
       " 'PP3733.pdf',\n",
       " 'PP3794.pdf',\n",
       " 'PP3798.pdf',\n",
       " 'PP3807.pdf',\n",
       " 'PP3814.pdf',\n",
       " 'PP3822.pdf',\n",
       " 'PP3835.pdf',\n",
       " 'PP3851.pdf',\n",
       " 'PP3857.pdf',\n",
       " 'PP3872.pdf',\n",
       " 'PP3874.pdf',\n",
       " 'PP3887.pdf',\n",
       " 'PP3888.pdf',\n",
       " 'PP3920.pdf',\n",
       " 'PP3921.pdf',\n",
       " 'PP3928.pdf',\n",
       " 'PP3955.pdf',\n",
       " 'PP3965.pdf',\n",
       " 'PP3967.pdf',\n",
       " 'PP3988.pdf',\n",
       " 'PP3994.pdf',\n",
       " 'PP4034.pdf',\n",
       " 'PP4076.pdf',\n",
       " 'PP4083.pdf',\n",
       " 'PP4096.pdf',\n",
       " 'PP4098.pdf',\n",
       " 'PP4100.pdf',\n",
       " 'PP4177.pdf',\n",
       " 'PP4182.pdf',\n",
       " 'PP4213.pdf',\n",
       " 'PP4247.pdf',\n",
       " 'PP4257.pdf',\n",
       " 'PP4300.pdf',\n",
       " 'PP4315.pdf',\n",
       " 'PP4318.pdf',\n",
       " 'PP4327.pdf',\n",
       " 'PP4401.pdf',\n",
       " 'PP4423.pdf',\n",
       " 'PP4427.pdf',\n",
       " 'PP4437.pdf',\n",
       " 'PP4448.pdf',\n",
       " 'PP4501.pdf',\n",
       " 'PP4528.pdf',\n",
       " 'PP4543.pdf',\n",
       " 'PP4544.pdf',\n",
       " 'PP4573.pdf',\n",
       " 'PP4580.pdf',\n",
       " 'PP4620.pdf',\n",
       " 'PP4659.pdf',\n",
       " 'PP4679.pdf',\n",
       " 'PP4687.pdf',\n",
       " 'PP4690.pdf',\n",
       " 'PP4707.pdf',\n",
       " 'PP4733.pdf',\n",
       " 'PP4734.pdf',\n",
       " 'PP4745.pdf',\n",
       " 'PP4756.pdf',\n",
       " 'PP4786.pdf',\n",
       " 'PP4798.pdf',\n",
       " 'PP4823.pdf',\n",
       " 'PP4877.pdf',\n",
       " 'PP4904.pdf',\n",
       " 'PP4921.pdf',\n",
       " 'PP4931.pdf',\n",
       " 'PP4946.pdf',\n",
       " 'PP4958.pdf',\n",
       " 'PP4968.pdf',\n",
       " 'PP5006.pdf',\n",
       " 'PP5016.pdf',\n",
       " 'PP5025.pdf',\n",
       " 'PP5043.pdf',\n",
       " 'PP5073.pdf',\n",
       " 'PP5074.pdf',\n",
       " 'PP5084.pdf',\n",
       " 'PP5087.pdf',\n",
       " 'PP5099.pdf',\n",
       " 'PP5107.pdf',\n",
       " 'PP5118.pdf',\n",
       " 'PP5122.pdf',\n",
       " 'PP5172.pdf',\n",
       " 'PP5223.pdf',\n",
       " 'PP5235.pdf',\n",
       " 'PP5268.pdf',\n",
       " 'PP5283.pdf',\n",
       " 'PP5299.pdf',\n",
       " 'PP5319.pdf',\n",
       " 'PP5343.pdf',\n",
       " 'PP5357.pdf',\n",
       " 'PP5415.pdf',\n",
       " 'PP5416.pdf',\n",
       " 'PP5425.pdf',\n",
       " 'PP5444.pdf',\n",
       " 'PP5446.pdf',\n",
       " 'PP5447.pdf',\n",
       " 'PP5489.pdf',\n",
       " 'PP5492.pdf',\n",
       " 'PP5496.pdf',\n",
       " 'PP5504.pdf',\n",
       " 'PP5522.pdf',\n",
       " 'PP5563.pdf',\n",
       " 'PP5572.pdf',\n",
       " 'PP5582.pdf',\n",
       " 'PP5601.pdf',\n",
       " 'PP5614.pdf',\n",
       " 'PP5629.pdf',\n",
       " 'PP5630.pdf',\n",
       " 'PP5706.pdf',\n",
       " 'PP5721.pdf',\n",
       " 'PP5722.pdf',\n",
       " 'PP5772.pdf',\n",
       " 'PP5779.pdf',\n",
       " 'PP5807.pdf',\n",
       " 'PP5847.pdf',\n",
       " 'PP5855.pdf',\n",
       " 'PP5876.pdf',\n",
       " 'PP5882.pdf',\n",
       " 'PP5899.pdf',\n",
       " 'PP5927.pdf',\n",
       " 'PP5931.pdf',\n",
       " 'PP5932.pdf',\n",
       " 'PP5934.pdf',\n",
       " 'PP5941.pdf',\n",
       " 'PP5945.pdf',\n",
       " 'PP5981.pdf',\n",
       " 'PP6001.pdf',\n",
       " 'PP6017.pdf',\n",
       " 'PP6021.pdf',\n",
       " 'PP6024.pdf',\n",
       " 'PP6034.pdf',\n",
       " 'PP6035.pdf',\n",
       " 'PP6045.pdf',\n",
       " 'PP6080.pdf',\n",
       " 'PP6110.pdf',\n",
       " 'PP6132.pdf',\n",
       " 'PP6171.pdf',\n",
       " 'PP6221.pdf',\n",
       " 'PP6233.pdf',\n",
       " 'PP6295.pdf',\n",
       " 'PP6334.pdf',\n",
       " 'PP6395.pdf',\n",
       " 'PP6426.pdf',\n",
       " 'PP6429.pdf',\n",
       " 'PP6435.pdf',\n",
       " 'PP6439.pdf',\n",
       " 'PP6462.pdf',\n",
       " 'PP6492.pdf',\n",
       " 'PP6495.pdf',\n",
       " 'PP6497.pdf',\n",
       " 'PP6518.pdf',\n",
       " 'PP6519.pdf',\n",
       " 'PP6539.pdf',\n",
       " 'PP6569.pdf',\n",
       " 'PP6618.pdf',\n",
       " 'PP6624.pdf',\n",
       " 'PP6641.pdf',\n",
       " 'PP6670.pdf',\n",
       " 'PP6686.pdf',\n",
       " 'PP6693.pdf',\n",
       " 'PP6715.pdf',\n",
       " 'PP6787.pdf',\n",
       " 'PP6791.pdf',\n",
       " 'PP6799.pdf',\n",
       " 'PP6825.pdf',\n",
       " 'PP6846.pdf',\n",
       " 'PP6899.pdf',\n",
       " 'PP6943.pdf',\n",
       " 'PP6945.pdf',\n",
       " 'PP6963.pdf',\n",
       " 'PP6964.pdf',\n",
       " 'PP6983.pdf',\n",
       " 'PP7054.pdf',\n",
       " 'PP7062.pdf',\n",
       " 'PP7078.pdf',\n",
       " 'PP7114.pdf',\n",
       " 'PP7122.pdf',\n",
       " 'PP7181.pdf',\n",
       " 'PP7189.pdf',\n",
       " 'PP7221.pdf',\n",
       " 'PP7245.pdf',\n",
       " 'PP7262.pdf',\n",
       " 'PP7274.pdf',\n",
       " 'PP7275.pdf']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
