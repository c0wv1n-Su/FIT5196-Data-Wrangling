{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5196 Assessment 2\n",
    "<a id=\"FIT5196\"></a>\n",
    "\n",
    "#### Student Name: Jiawei Su\n",
    "#### Student ID: 29590183\n",
    "\n",
    "\n",
    "#### Student Name: Weiwei Jin\n",
    "#### Student ID: 28106946\n",
    "\n",
    "Date: 08/31/2019\n",
    "\n",
    "Version: 1.5\n",
    "\n",
    "Environment: Python 3.7.1 and Jupyter notebook\n",
    "\n",
    "Libraries used:\n",
    "* pandas (for dataframe, included in Anaconda Python 3.7.1) \n",
    "* re (for regular expression, included in Anaconda Python 3.7.1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "\n",
    "* [Student Information](#FIT5196)\n",
    "* [1. Introduction](#Introduction)\n",
    "* [2. Import libraries](#Lib)\n",
    "* [3. Regular Expressions Design](#Reg)\n",
    "   * [3.1. Approach](#app)\n",
    "   * [3.2. Patterns](#pat)\n",
    "   * [3.2. Explanation of Patterns](#Expat)\n",
    "* [4. Functions](#Functions)\n",
    "* [5. Processing the file](#Processing)\n",
    "   * [5.1. Read file](#read)\n",
    "   * [5.2. Process data and generate CSV file](#csv)\n",
    "   * [5.3. Process data and generate JSON file](#json)\n",
    "* [6. Summary](#Summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import importlib\n",
    "import sys\n",
    "importlib.reload(sys)\n",
    "\n",
    "from pdfminer.pdfparser import PDFParser,PDFDocument\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import PDFPageAggregator\n",
    "from pdfminer.layout import LTTextBoxHorizontal,LAParams\n",
    "from pdfminer.pdfinterp import PDFTextExtractionNotAllowed\n",
    "\n",
    "# Re library provides regex related functions\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2. Parse XXXX File\n",
    "\n",
    "In this section, you can write your python scripts to parse the correspondiing file.\n",
    "You should \n",
    "* write proper notes for all code block in this notebook using the Markdown cells\n",
    "* provide proper comment in your scripts\n",
    "* run all cells to make sure scripts are runable. If the scripts cannot be run by the assessors, they will not be assessed and zero mark will be given to the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_SOURCE_PATH = r'Group067.pdf'\n",
    "TEXT_ORIGINAL_PATH = 'links_orig.txt'\n",
    "BODY_SECTION_PATTERN = re.compile(r'\\n[1-9]\\n')\n",
    "\n",
    "# Function to parse a PDF file, preprocess it, add the raw content of each line to a text file.\n",
    "def parse_to_txt(path, outpath):\n",
    "    # Open file in binary read mode\n",
    "    fp = open(path, 'rb')\n",
    "    \n",
    "    # Use file to breate a PDF parser \n",
    "    praser = PDFParser(fp)\n",
    "    \n",
    "    # Generate a PDF document\n",
    "    doc = PDFDocument()\n",
    "    \n",
    "    # pass document to parser\n",
    "    praser.set_document(doc)\n",
    "    doc.set_parser(praser)\n",
    "\n",
    "    doc.initialize()\n",
    "    \n",
    "    if not doc.is_extractable:\n",
    "        raise PDFTextExtractionNotAllowed\n",
    "    else:\n",
    "        # Create PDF resource manager\n",
    "        rsrcmgr = PDFResourceManager()\n",
    "        \n",
    "        # Create a PDF device object\n",
    "        laparams = LAParams()\n",
    "        device = PDFPageAggregator(rsrcmgr, laparams=laparams)\n",
    "        \n",
    "        # Create a PDF interpreter object\n",
    "        interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "\n",
    "        # traverse page list obtained from get_pages()\n",
    "        for page in doc.get_pages(): \n",
    "            interpreter.process_page(page)\n",
    "            \n",
    "            layout = device.get_result()\n",
    "            for x in layout:\n",
    "                if (isinstance(x, LTTextBoxHorizontal)):\n",
    "                    with open(outpath, 'a') as f:\n",
    "                        results = x.get_text()\n",
    "                        f.write(results + '\\n')\n",
    "\n",
    "\n",
    "\n",
    "# Function to parse a PDF file, preprocess it, prepare it for processing.\n",
    "def parse_to_proc(path):\n",
    "    # Open file in binary read mode\n",
    "    fp = open(path, 'rb')\n",
    "    \n",
    "    # Use file to breate a PDF parser \n",
    "    praser = PDFParser(fp)\n",
    "    \n",
    "    # Generate a PDF document\n",
    "    doc = PDFDocument()\n",
    "    \n",
    "    # pass document to parser\n",
    "    praser.set_document(doc)\n",
    "    doc.set_parser(praser)\n",
    "\n",
    "    doc.initialize()\n",
    "    \n",
    "    if not doc.is_extractable:\n",
    "        raise PDFTextExtractionNotAllowed\n",
    "    else:\n",
    "        # Create PDF resource manager\n",
    "        rsrcmgr = PDFResourceManager()\n",
    "        \n",
    "        # Create a PDF device object\n",
    "        laparams = LAParams()\n",
    "        device = PDFPageAggregator(rsrcmgr, laparams=laparams)\n",
    "        \n",
    "        # Create a PDF interpreter object\n",
    "        interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "        \n",
    "        # Define a dictionary to return abstract, titles and body of every PDF file.\n",
    "        essay_dict = {}\n",
    "        essay_dict[\"abstract\"] = \"\"\n",
    "        essay_dict[\"titles\"] = \"\"\n",
    "        essay_dict[\"authors\"] = \"\"\n",
    "        essay_dict[\"body\"] = []\n",
    "        \n",
    "        return_string = \"\"\n",
    "\n",
    "        # Traverse page list obtained from get_pages()\n",
    "        for page in doc.get_pages(): \n",
    "            interpreter.process_page(page)\n",
    "            \n",
    "            layout = device.get_result()\n",
    "            for x in layout:\n",
    "                if (isinstance(x, LTTextBoxHorizontal)):\n",
    "                    results = x.get_text()\n",
    "                    print(results)\n",
    "                    print(\"========\")\n",
    "                    if (len(results) <= 3):\n",
    "                        pass\n",
    "                    else:\n",
    "                        if \"Authored by:\" in results:\n",
    "                            temp_str = return_string.strip(\"\\n\").replace(\"\\n\", \" \")\n",
    "                            final_str = temp_str.replace(\"- \", \"\")\n",
    "                            essay_dict[\"titles\"] = final_str\n",
    "                            return_string = \"\"\n",
    "                        elif \"Abstract\" in results:\n",
    "                            temp_str = return_string.strip(\"\\n\").replace(\"\\n\", \" \")\n",
    "                            final_str = temp_str.replace(\"- \", \"\")\n",
    "                            essay_dict[\"authors\"] = final_str\n",
    "                            return_string = \"\"\n",
    "                            continue\n",
    "                        elif \"1 Paper Body\" in results:\n",
    "                            temp_str = return_string.strip(\"\\n\").replace(\"\\n\", \" \")\n",
    "                            final_str = temp_str.replace(\"- \", \"\")\n",
    "                            essay_dict[\"abstract\"] = final_str\n",
    "                            return_string = \"\"\n",
    "                            continue\n",
    "                        elif \"2 References\" in results:\n",
    "                            res = re.split(BODY_SECTION_PATTERN, return_string)\n",
    "                            for ree in res:\n",
    "                                temp_str = ree.replace(\"\\n\", \" \")\n",
    "                                final_str = temp_str.replace(\"- \", \"\")\n",
    "                                essay_dict[\"body\"].append(final_str)\n",
    "                        return_string += results              \n",
    "        return essay_dict\n",
    "\n",
    "                        \n",
    "# Function to read the a text file, preprocess it, add the content of the text file to a list and return the list.\n",
    "def read_file(path):        \n",
    "    content = []\n",
    "    try:\n",
    "        with open(path, 'r', encoding = 'utf-8') as fp: \n",
    "            for line in fp:\n",
    "                if 'url' in line:\n",
    "                    pass\n",
    "                elif 'filename' in line:\n",
    "                    pass\n",
    "                else:\n",
    "                    if line:\n",
    "                        content.append(line.strip('\\n'))\n",
    "    finally:\n",
    "        fp.close()\n",
    "    return content\n",
    "\n",
    "def catch_pdf(file_list):\n",
    "    COUNT = 1\n",
    "    filenames = []\n",
    "    for entry in file_list:\n",
    "        url = entry.split()[1]\n",
    "        print('downloading file No. ' + str(COUNT) + ' with urllib...')\n",
    "        COUNT += 1\n",
    "        filenames.append(entry.split()[0])\n",
    "        OUTPUT_PATH = \"pdf/\" + entry.split()[0]\n",
    "        urllib.request.urlretrieve(url, OUTPUT_PATH)\n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_to_txt(PDF_SOURCE_PATH, TEXT_ORIGINAL_PATH)\n",
    "\n",
    "str_list = list(filter(None, read_file(TEXT_ORIGINAL_PATH))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading file No. 1 with urllib...\n",
      "downloading file No. 2 with urllib...\n",
      "downloading file No. 3 with urllib...\n",
      "downloading file No. 4 with urllib...\n",
      "downloading file No. 5 with urllib...\n",
      "downloading file No. 6 with urllib...\n",
      "downloading file No. 7 with urllib...\n",
      "downloading file No. 8 with urllib...\n",
      "downloading file No. 9 with urllib...\n",
      "downloading file No. 10 with urllib...\n",
      "downloading file No. 11 with urllib...\n",
      "downloading file No. 12 with urllib...\n",
      "downloading file No. 13 with urllib...\n",
      "downloading file No. 14 with urllib...\n",
      "downloading file No. 15 with urllib...\n",
      "downloading file No. 16 with urllib...\n",
      "downloading file No. 17 with urllib...\n",
      "downloading file No. 18 with urllib...\n",
      "downloading file No. 19 with urllib...\n",
      "downloading file No. 20 with urllib...\n",
      "downloading file No. 21 with urllib...\n",
      "downloading file No. 22 with urllib...\n",
      "downloading file No. 23 with urllib...\n",
      "downloading file No. 24 with urllib...\n",
      "downloading file No. 25 with urllib...\n",
      "downloading file No. 26 with urllib...\n",
      "downloading file No. 27 with urllib...\n",
      "downloading file No. 28 with urllib...\n",
      "downloading file No. 29 with urllib...\n",
      "downloading file No. 30 with urllib...\n",
      "downloading file No. 31 with urllib...\n",
      "downloading file No. 32 with urllib...\n",
      "downloading file No. 33 with urllib...\n",
      "downloading file No. 34 with urllib...\n",
      "downloading file No. 35 with urllib...\n",
      "downloading file No. 36 with urllib...\n",
      "downloading file No. 37 with urllib...\n",
      "downloading file No. 38 with urllib...\n",
      "downloading file No. 39 with urllib...\n",
      "downloading file No. 40 with urllib...\n",
      "downloading file No. 41 with urllib...\n",
      "downloading file No. 42 with urllib...\n",
      "downloading file No. 43 with urllib...\n",
      "downloading file No. 44 with urllib...\n",
      "downloading file No. 45 with urllib...\n",
      "downloading file No. 46 with urllib...\n",
      "downloading file No. 47 with urllib...\n",
      "downloading file No. 48 with urllib...\n",
      "downloading file No. 49 with urllib...\n",
      "downloading file No. 50 with urllib...\n",
      "downloading file No. 51 with urllib...\n",
      "downloading file No. 52 with urllib...\n",
      "downloading file No. 53 with urllib...\n",
      "downloading file No. 54 with urllib...\n",
      "downloading file No. 55 with urllib...\n",
      "downloading file No. 56 with urllib...\n",
      "downloading file No. 57 with urllib...\n",
      "downloading file No. 58 with urllib...\n",
      "downloading file No. 59 with urllib...\n",
      "downloading file No. 60 with urllib...\n",
      "downloading file No. 61 with urllib...\n",
      "downloading file No. 62 with urllib...\n",
      "downloading file No. 63 with urllib...\n",
      "downloading file No. 64 with urllib...\n",
      "downloading file No. 65 with urllib...\n",
      "downloading file No. 66 with urllib...\n",
      "downloading file No. 67 with urllib...\n",
      "downloading file No. 68 with urllib...\n",
      "downloading file No. 69 with urllib...\n",
      "downloading file No. 70 with urllib...\n",
      "downloading file No. 71 with urllib...\n",
      "downloading file No. 72 with urllib...\n",
      "downloading file No. 73 with urllib...\n",
      "downloading file No. 74 with urllib...\n",
      "downloading file No. 75 with urllib...\n",
      "downloading file No. 76 with urllib...\n",
      "downloading file No. 77 with urllib...\n",
      "downloading file No. 78 with urllib...\n",
      "downloading file No. 79 with urllib...\n",
      "downloading file No. 80 with urllib...\n",
      "downloading file No. 81 with urllib...\n",
      "downloading file No. 82 with urllib...\n",
      "downloading file No. 83 with urllib...\n",
      "downloading file No. 84 with urllib...\n",
      "downloading file No. 85 with urllib...\n",
      "downloading file No. 86 with urllib...\n",
      "downloading file No. 87 with urllib...\n",
      "downloading file No. 88 with urllib...\n",
      "downloading file No. 89 with urllib...\n",
      "downloading file No. 90 with urllib...\n",
      "downloading file No. 91 with urllib...\n",
      "downloading file No. 92 with urllib...\n",
      "downloading file No. 93 with urllib...\n",
      "downloading file No. 94 with urllib...\n",
      "downloading file No. 95 with urllib...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-2993abf0c808>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfilenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcatch_pdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-4fd8e85bb5c0>\u001b[0m in \u001b[0;36mcatch_pdf\u001b[0;34m(file_list)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mfilenames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mOUTPUT_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"pdf/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOUTPUT_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0murl_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplittype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0;31m# post-process response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[0;32m--> 544\u001b[0;31m                                   '_open', req)\n\u001b[0m\u001b[1;32m    545\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mhttps_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1359\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mhttps_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m             return self.do_open(http.client.HTTPSConnection, req,\n\u001b[0;32m-> 1361\u001b[0;31m                 context=self._context, check_hostname=self._check_hostname)\n\u001b[0m\u001b[1;32m   1362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m         \u001b[0mhttps_request\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAbstractHTTPHandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_request_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m                 h.request(req.get_method(), req.selector, req.data, headers,\n\u001b[0;32m-> 1318\u001b[0;31m                           encode_chunked=req.has_header('Transfer-encoding'))\n\u001b[0m\u001b[1;32m   1319\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# timeout error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                 encode_chunked=False):\n\u001b[1;32m   1238\u001b[0m         \u001b[0;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1283\u001b[0m             \u001b[0;31m# default charset of iso-8859-1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m             \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'body'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1232\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1233\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1234\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m     def request(self, method, url, body=None, headers={}, *,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1024\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb\"\\r\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmessage_body\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    962\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 964\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    965\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mNotConnected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1399\u001b[0m             self.sock = self._context.wrap_socket(self.sock,\n\u001b[0;32m-> 1400\u001b[0;31m                                                   server_hostname=server_hostname)\n\u001b[0m\u001b[1;32m   1401\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_hostname\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_hostname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1402\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mwrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    405\u001b[0m                          \u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m                          \u001b[0mserver_hostname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m                          _context=self, _session=session)\n\u001b[0m\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m     def wrap_bio(self, incoming, outgoing, server_side=False,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sock, keyfile, certfile, server_side, cert_reqs, ssl_version, ca_certs, do_handshake_on_connect, family, type, proto, fileno, suppress_ragged_eofs, npn_protocols, ciphers, server_hostname, _context, _session)\u001b[0m\n\u001b[1;32m    812\u001b[0m                         \u001b[0;31m# non-blocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"do_handshake_on_connect should not be specified for non-blocking sockets\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mdo_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1066\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1069\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mdo_handshake\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdo_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;34m\"\"\"Start the SSL/TLS handshake.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_hostname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "filenames = catch_pdf(str_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Budgeted Optimization with Concurrent\n",
      "Stochastic-Duration Experiments\n",
      "\n",
      "========\n",
      "Authored by:\n",
      "\n",
      "========\n",
      "Alan Fern\n",
      "Javad Azimi\n",
      "Xiaoli Z. Fern\n",
      "\n",
      "========\n",
      "Abstract\n",
      "\n",
      "========\n",
      "Budgeted optimization involves optimizing an unknown function that\n",
      "is costly to evaluate by requesting a limited number of function evaluations\n",
      "at intelligently selected inputs. Typical problem formulations assume that\n",
      "experiments are selected one at a time with a limited total number of ex-\n",
      "periments, which fail to capture important aspects of many real-world\n",
      "problems. This paper deﬁnes a novel problem formulation with the fol-\n",
      "lowing important extensions: 1) allowing for concurrent experiments; 2)\n",
      "allowing for stochastic experiment durations; and 3) placing constraints\n",
      "on both the total number of experiments and the total experimental time.\n",
      "We develop both oﬄine and online algorithms for selecting concurrent ex-\n",
      "periments in this new setting and provide experimental results on a num-\n",
      "ber of optimization benchmarks. The results show that our algorithms\n",
      "produce highly eﬀective schedules compared to natural baselines.\n",
      "\n",
      "========\n",
      "1 Paper Body\n",
      "\n",
      "========\n",
      "We study the optimization of an unknown function f by requesting n experi-\n",
      "ments, each specifying an input x and producing a noisy observation of f (x). In\n",
      "practice, the function f might be the performance of a device parameterized by\n",
      "x. We consider the setting where running experiments is costly (e.g. in terms\n",
      "of time), which renders methods that rely on many function evaluations, such\n",
      "as stochastic search or empirical gradient methods, impractical. Bayesian op-\n",
      "timization (BO) [8, 4] addresses this issue by leveraging Bayesian modeling to\n",
      "maintain a posterior over the unknown function based on previous experiments.\n",
      "The posterior is then used to intelligently select new experiments to trade-oﬀ\n",
      "exploring new parts of the experimental space and exploiting promising parts.\n",
      "Traditional BO follows a sequential approach where only one experiment is se-\n",
      "lected and run at a time. However, it is often desirable to select more than one\n",
      "experiment at a time so that multiple experiments can be run simultaneously\n",
      "\n",
      "========\n",
      "1\n",
      "\n",
      "========\n",
      "to leverage parallel facilities. Recently, Azimi et al. (2010) proposed a batch\n",
      "BO algorithm that selects a batch of k ? 1 experiments at a time. While this\n",
      "broadens the applicability of BO, it is still limited to selecting a ﬁxed number of\n",
      "experiments at each step. As such, prior work on BO, both batch and sequen-\n",
      "tial, completely ignores the problem of how to schedule experiments under ﬁxed\n",
      "experimental budget and time constraints. Furthermore, existing work assumes\n",
      "that the durations of experiments are identical and deterministic, whereas in\n",
      "practice they are often stochastic. Consider one of our motivating applications\n",
      "of optimizing the power output of nano-enhanced Microbial Fuel Cells (MFCs).\n",
      "MFCs [3] use micro-organisms to generate electricity. Their performance de-\n",
      "pends 1\n",
      "strongly on the surface properties of the anode [10]. Our problem involves\n",
      "optimizing nano-enhanced anodes, where various types of nano-structures, e.g.\n",
      "carbon nano-wire, are grown directly on the anode surface. Because there is\n",
      "little understanding of how diﬀerent nano-enhancements impact power output,\n",
      "optimizing anode design is largely guess work. Our original goal was to de-\n",
      "velop BO algorithms for aiding this process. However, many aspects of this\n",
      "domain complicate the application of BO. First, there is a ﬁxed budget on the\n",
      "number of experiments that can be run due to limited funds and a ﬁxed time\n",
      "period for the pro ject. Second, we can run multiple concurrent experiments,\n",
      "limited by the number of experimental apparatus. Third, the time required to\n",
      "run each experiment is variable because each experiment requires the construc-\n",
      "tion of a nano-structure with speciﬁc properties. Nano-fabrication is highly\n",
      "unpredictable and the amount of time to successfully produce a structure is\n",
      "quite variable. Clearly prior BO models fail to capture critical aspects of the\n",
      "experimental process in this domain. In this paper, we consider the following\n",
      "extensions. First, we have l available labs (which may correspond to experi-\n",
      "mental stations at one location or to physically distinct laboratories), allowing\n",
      "up to l concurrent experiments. Second, experiments have stochastic durations,\n",
      "independently and identically distributed according to a known density function\n",
      "pd . Finally, we are constrained by a budget of n total experiments and a time\n",
      "horizon h by which point we must ﬁnish. The goal is to maximize the unknown\n",
      "function f by selecting experiments and when to start them while satisfying the\n",
      "constraints. We propose oﬄine (Section 4) and online (Section 5) scheduling\n",
      "approaches for this problem, which aim to balance two competing factors. First,\n",
      "a scheduler should ensure that all n experiments complete within the horizon\n",
      "h, which encourages high concurrency. Second, we wish to select new exper-\n",
      "iments given as many previously completed experiments as possible to make\n",
      "more intelligent experiment selections, which encourages low concurrency. We\n",
      "introduce a novel measure of the second factor, cumulative prior experiments\n",
      "(CPE) (Section 3), which our approaches aim to optimize. Our experimental\n",
      "results indicate that these approaches signiﬁcantly outperform a set of baselines\n",
      "across a range of benchmark optimization problems.\n",
      "2\n",
      "Problem Setup\n",
      "Let X ? ¡d be a d-dimensional compact input space, where each dimension i\n",
      "\n",
      "========\n",
      "2\n",
      "\n",
      "========\n",
      "is bounded in [ai , bi ]. An element of X is called an experiment. An unknown\n",
      "real-valued function f : X ? ¡ represents the expected value of the dependent\n",
      "variable after running an experiment. For example, f (x) might be the result of a\n",
      "wetlab experiment described by x. Conducting an experiment x produces a noisy\n",
      "outcome y = f (x) + , where is a random noise term. Bayesian Optimization\n",
      "(BO) aims to ﬁnd an experiment x ? X that approximately maximizes f by\n",
      "requesting a limited number of experiments and observing their outcomes. We\n",
      "extend traditional BO algorithms and study the experiment scheduling problem.\n",
      "Assuming a known density function pd for the experiment durations, the inputs\n",
      "to our problem include the total number of available labs l, the total number of\n",
      "experiments n, and the time horizon h by which we must ﬁnish. The goal is to\n",
      "design a policy ? for selecting when to start experiments and which ones to start\n",
      "to optimize f . Speciﬁcally, the inputs to ? are the set of completed experiments\n",
      "and their outcomes, the set of currently running experiments with their elapsed\n",
      "running time, the number of free labs, and the remaining time till the horizon.\n",
      "Given this information, ? must select a set of experiments (possibly empty)\n",
      "to start that is no larger than the number of free labs. Any run of the policy\n",
      "ends when either n experiments are completed or the time horizon is reached,\n",
      "resulting in a set X of n or fewer completed experiments. The ob jective is to\n",
      "obtain a policy with small regret, which is the expected diﬀerence between the\n",
      "optimal value of f and the value of f for the predicted best experiment in X. In\n",
      "theory, the optimal policy can be found by solving a POMDP with hidden state\n",
      "corresponding to the unknown function f . However, this POMDP is beyond\n",
      "the reach of any existing solvers. Thus, we focus on deﬁning and comparing\n",
      "several principled policies that work well in practice, but without optimality\n",
      "guarantees. Note that this problem has not been studied in the literature to the\n",
      "best of our knowledge. 2\n",
      "3\n",
      "Overview of General Approach\n",
      "A policy for our problem must make two types of decisions: 1) scheduling\n",
      "when to start new experiments, and 2) selecting the speciﬁc experiments to\n",
      "start. In this work, we factor the problem based on these decisions and focus\n",
      "on approaches for scheduling experiments. We assume a black box function\n",
      "SelectBatch for intelligently selecting the k ? 1 experiments based on both com-\n",
      "pleted and currently running experiments. The implementation of SelectBatch\n",
      "is described in Section 6. Optimal scheduling to minimize regret appears to\n",
      "be computationally hard for non-trivial instances of SelectBatch. Further, we\n",
      "desire scheduling approaches that do not depend on the details of SelectBatch,\n",
      "but work well for any reasonable implementation. Thus, rather than directly\n",
      "optimizing regret for a speciﬁc SelectBatch, we consider the following surrogate\n",
      "criteria. First, we want to ﬁnish all n experiments within the horizon h with\n",
      "high probability. Second, we would like to select each experiment based on as\n",
      "much information as possible, measured by the number of previously completed\n",
      "experiments. These two goals are at odds, since maximizing the completion\n",
      "probability requires maximizing concurrency of the experiments, which mini-\n",
      "mizes the second criterion. Our oﬄine and online scheduling approaches provide\n",
      "\n",
      "========\n",
      "3\n",
      "\n",
      "========\n",
      "diﬀerent ways for managing this trade-oﬀ. Cosines\n",
      "0.32\n",
      "0.28\n",
      "0.08\n",
      "Regret\n",
      "0.09\n",
      "0.07\n",
      "0.26\n",
      "0.06\n",
      "0.24\n",
      "0.05\n",
      "0.22\n",
      "0.04\n",
      "0.2\n",
      "0.18\n",
      "Hydrogen\n",
      "0.1\n",
      "0.3\n",
      "Regret\n",
      "To quantify the second criterion, consider a complete execution E of a sched-\n",
      "uler. For any experiment e in E, let priorE (e) denote the number of experi-\n",
      "ments in E that completed before starting e. P We deﬁne the cumulative prior\n",
      "experiments (CPE) of E as: e?E priorE (e). Intuitively, a scheduler with a high\n",
      "expected CPE is desirable, since CPE measures the total amount of information\n",
      "SelectBatch uses to make its decisions.\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "CPE\n",
      "80\n",
      "100\n",
      "120\n",
      "0.03\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "CPE\n",
      "Figure 1: The correlation between CPE and CPE agrees with intuition when\n",
      "considering extreme policies. regret for 30 diﬀerent schedulers on two BO A\n",
      "poor scheduler that starts all n experiments at the same time benchmarks.\n",
      "(assuming enough labs) will have a minimum CPE of zero. Further, CPE is\n",
      "\n",
      "========\n",
      "4\n",
      "\n",
      "========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximized by a scheduler that sequentially executes all experiments (assuming\n",
      "enough time). However, in between these extremes, CPE fails to capture certain\n",
      "intuitive properties. For example, CPE increases linearly in the number of\n",
      "prior experiments, while one might expect diminishing returns as the number\n",
      "of prior experiments becomes large. Similarly, as the number of experiments\n",
      "started together (the batch size) increases, we might also expect diminishing\n",
      "returns since SelectBatch must choose the experiments based on the same prior\n",
      "experiments. Unfortunately, quantifying these intuitions in a general way is still\n",
      "an open problem. Despite its potential shortcomings, we have found CPE to\n",
      "be a robust measure in practice. To empirically examine the utility of CPE, we\n",
      "conducted experiments on a number of BO benchmarks. For each domain, we\n",
      "used 30 manually designed diverse schedulers, some started more experiments\n",
      "early on than later, and vice-versa, while others included random and uniform\n",
      "schedules. We measured the average regret achieved for each scheduler given\n",
      "the same inputs and the expected CPE of the executions. Figure 1 shows the\n",
      "results for two of the domains (other results are highly similar), where each\n",
      "point corresponds to the average regret and CPE of a particular scheduler.\n",
      "We observe a clear and non-trivial correlation between regret and CPE, which\n",
      "provides empirical evidence that CPE is a useful measure to optimize. Further,\n",
      "as we will see in our experiments, the performance of our methods is also highly\n",
      "correlated with CPE.\n",
      "4\n",
      "Oﬄine Scheduling\n",
      "We now consider oﬄine schedules, which assign start times to all n experi-\n",
      "ments before the experimental process begins. Note that while the schedules are\n",
      "oﬄine, the overall BO policy has online characteristics, since the exact exper-\n",
      "iments to run are only speciﬁed when they need to be started by SelectBatch,\n",
      "based 3\n",
      "on the most recent information. This oﬄine scheduling approach is often\n",
      "convenient in real experimental domains where it is useful to plan out a static\n",
      "equipment/personnel schedule for the duration of a pro ject. Below we ﬁrst\n",
      "consider a restricted class of schedules, called staged schedules, for which we\n",
      "present a solution that optimizes CPE. Next, we describe an approach for a\n",
      "more general class of schedules. 4.1\n",
      "Staged Schedules\n",
      "A staged schedule deﬁnes a consecutivePsequence of NPexperimental stages,\n",
      "denoted by a sequence of tuples h(ni , di )iN i=1 , where 0 ¡ ni ? l, i di ? h, and i\n",
      "ni ? n. Stage i begins by starting up ni new experiments selected by SelectBatch\n",
      "using the most recent information, and ends after a duration of di , upon which\n",
      "stage i + 1 starts. In some applications, staged schedules are preferable as they\n",
      "allow pro ject planning to focus on a relatively small number of time points (the\n",
      "beginning of each stage). While our approach tries to ensure that experiments\n",
      "ﬁnish within their stage, experiments are never terminated and hence might\n",
      "run longer than their speciﬁed duration. If, because of this, at the beginning\n",
      "of stage i there are not ni free labs, the experiments will wait till labs free up.\n",
      "We say that an execution E of a staged schedule S is safe if each experiment is\n",
      "\n",
      "========\n",
      "5\n",
      "\n",
      "========\n",
      "completed within its speciﬁed duration in S. We say that a staged schedule S\n",
      "is p-safe if with probability at least p an execution of S is safe which provides\n",
      "a probabilistic guarantee that all n experiments complete within the horizon h.\n",
      "Further, it ensures with probability p that the maximum number of concurrent\n",
      "experiments when executing S is maxi ni (since experiments from two stages will\n",
      "not overlap with probability p). As such, we are interested in ﬁnding staged\n",
      "schedules that are p-safe for a user speciﬁed p, e.g. 95%. Meanwhile, we want to\n",
      "maximize CPE. PN Pi?1 The CPE of any safe execution of S (slightly abusing\n",
      "notation) is: CPE(S) = i=2 ni j=1 nj . Typical applications will use relative\n",
      "high values of p, since otherwise experimental resources would be wasted, and\n",
      "thus with high probability we expect the CPE of an execution of S to equal\n",
      "CPE(S). Our goal is thus to maximize CPE(S) while ensuring p-safeness.\n",
      "It\n",
      "turns out that for any ﬁxed number of stages N , the schedules that maximize\n",
      "CPE(S) must be uniform. A staged schedule is deﬁned to be uniform if ?i, j,\n",
      "—ni ? nj — ? 1, i.e., the batch sizes across stages may diﬀer by at most a single\n",
      "experiment. Proposition 1. For any number of experiments n and labs l, let SN\n",
      "be the set of corresponding N stage schedules, where N ? dn/le. For any S ? SN\n",
      ", CPE(S) = maxS 0 ?SN CPE(S 0 ) if and only if S is uniform. It is easy to\n",
      "verify that for a given n and l, an N stage uniform schedule achieves a strictly\n",
      "higher CPE than any N ? 1 stage schedule. This implies that we should prefer\n",
      "uniform schedules with maximum number of stages allowed by the p-safeness\n",
      "restriction. This motivates us to solve the following problem: Find a p-safe\n",
      "uniform schedule with maximum number of stages.\n",
      "Algorithm 1 Algorithm for computing a p-safe uniform schedule with max-\n",
      "imum number of stages. Input:number of experiments (n), number of labs (l),\n",
      "horizon (h), safety probability (p) Output:A p-safe uniform schedule with max-\n",
      "imum number of stages N = dn/le, S ? null loop S 0 ? MaxProbUniform(N, n,\n",
      "l, h) if S 0 is not p-safe then return S end if S ? S0, N ? N + 1 end loop\n",
      "Our approach, outlined in Algorithm 1, considers N stage schedules in order\n",
      "of increasing N , starting at the minimum possible number of stages N = dn/le\n",
      "for running all experiments. For each value of N , the call to MaxProbUniform\n",
      "computes a uniform schedule S with the highest probability of a safe execution,\n",
      "among all N stage uniform schedules. If the resulting schedule is p-safe then we\n",
      "consider N + 1 stages. Otherwise, there is no uniform N stage schedule that is\n",
      "p-safe and we return a uniform N ? 1 stage schedule, which was computed in\n",
      "the previous iteration. 4\n",
      "It remains to describe the MaxProbUniform function, which computes a\n",
      "uniform N stage schedule S = h(ni , di )iN i=1 that maximizes the probability\n",
      "of a safe execution. First, any N stage uniform schedule must have N 0 = (n\n",
      "mod N ) stages with n0 = bn/N c+1 experiments and N ?N 0 stages with n0\n",
      "?1 experiments. Furthermore, the probability of a safe execution is invariant to\n",
      "the ordering of the stages, since we assume i.i.d. distribution on the experiment\n",
      "durations. The MaxProbUniform problem is now reduced to computing the\n",
      "durations di of S that maximize the probability of safeness for each given ni\n",
      ". For this we will assume that the distribution of the experiment duration pd\n",
      "is log-concave, which allows us to characterize the solution using the following\n",
      "\n",
      "========\n",
      "6\n",
      "\n",
      "========\n",
      "lemma. Lemma 1. For any duration distribution pd that is log-concave, if an N\n",
      "stage schedule S = h(ni , di )iN i=1 0 0 is p-safe, then there is a p-safe N stage\n",
      "schedule S 0 = h(ni , d0i )iN i=1 such that if ni = nj then di = dj . This lemma\n",
      "suggests that any stages with equal ni ?s should have equal di ?s to maximize\n",
      "the probability of safe execution. For a uniform schedule, ni is either n0 or n0\n",
      "? 1. Thus we only need to consider schedules with two durations, d0 for stages\n",
      "with ni = n0 and d00 for stages with ni = n0 ? 1. Since all durations must 0 ?N\n",
      "0 0 sum to h, d0 and d00 are deterministically related by: d00 = h?d N ?N 0 .\n",
      "Based on this, for any value of d the probability of the uniform schedule using\n",
      "durations d0 and d00 is as follows, where Pd is the CDF of pd .\n",
      "Pd (d0 )\n",
      "N 0 ?n0\n",
      "Pd\n",
      "h ? d0 ? N 0 N ? N0\n",
      "(N ?N 0 )?(n0 ?1) (1)\n",
      "We compute MaxProbUniform by maximizing Equation 1 with respect to\n",
      "d0 and using the corresponding duration for d00 . Putting everything together\n",
      "we get the following result. Theorem 1. For any log-concave pd , comput-\n",
      "ing MaxProbUniform by maximizing Equation 1 over d0 , if a p-safe uniform\n",
      "schedule exists, Algorithm 1 returns a maximum-stage p-safe uniform schedule.\n",
      "4.2 Independent Lab Schedules We now consider a more general class of oﬄine\n",
      "schedules and a heuristic algorithm for computing them. This class allows the\n",
      "start times of diﬀerent labs to be decoupled, desirable in settings where labs are\n",
      "run by independent experimenters. Further, our online scheduling approach is\n",
      "based on repeatedly calling an oﬄine scheduler, which requires the ﬂexibility\n",
      "to make schedules for labs in diﬀerent stages of execution. An independent lab\n",
      "(IL) P schedule S speciﬁes a number of labs k ¡ l and for each lab i, a number\n",
      "of i experiments mi such that i mi = n. Further, for each lab i a sequence of\n",
      "mi durations Di = hd1i , .\n",
      ".\n",
      ".\n",
      ", dm i i is given. The execution of S runs\n",
      "each lab independently, by having each lab start up experiments whenever they\n",
      "move to the next stage. Stage j of lab i ends after a duration of dji , or after\n",
      "the experiment ﬁnishes when it runs longer than dji (i.e. we do not terminate\n",
      "experiments). Each experiment is selected according to SelectBatch, given in-\n",
      "formation about all completed and running experiments across all labs. We\n",
      "say that an execution of an IL schedule is safe if all experiments ﬁnish within\n",
      "their speciﬁed durations, which also yields a notion of p-safeness. We are again\n",
      "interested in computing p-safe schedules that maximizes the CPE. Intuitively,\n",
      "CPE will be maximized if the amount of concurrency during an execution is\n",
      "minimized, suggesting the use of as few labs as possible. This motivates the\n",
      "problem of ﬁnding a p-safe IL schedule that use the minimum number of labs.\n",
      "Below we describe our heuristic approach to this problem. Algorithm Descrip-\n",
      "tion. Starting with k = 1, we compute a k labs IL schedule with the goal of\n",
      "maximizing the probability of safe execution.\n",
      "If this probability is less than\n",
      "p, we increment k, and otherwise output the schedule for k labs. To compute\n",
      "a schedule for each value of k, we ﬁrst allocate the number of experiments mi\n",
      "across k labs as uniformly as possible. In particular, (n mod k) labs will have\n",
      "\n",
      "========\n",
      "7\n",
      "\n",
      "========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bn/kc + 1 experiments and k ? (n mod k) labs will have bn/kc experiments.\n",
      "This choice is motivated by the intuition that the best way to maximize the\n",
      "probability of a safe execution is to distribute the work across labs as uniformly\n",
      "as possible. Given mi for each lab, we assign all durations of lab i to be h/mi\n",
      ", which can be shown to be optimal for log-concave pd . In this way, for each\n",
      "value of k the schedule we compute has just two possible values of mi and labs\n",
      "with the same mi have the same stage durations. 5\n",
      "5\n",
      "Online Scheduling Approaches\n",
      "We now consider online scheduling, which selects the start time of exper-\n",
      "iments online. The ﬂexibility of the online approaches oﬀers the potential to\n",
      "outperform oﬄine schedules by adapting to speciﬁc stochastic outcomes ob-\n",
      "served during experimental runs. Below we ﬁrst describe two baseline online\n",
      "approaches, followed by our main approach, policy switching, which aims to\n",
      "directly optimize CPE. Online Fastest Completion Policy (OnFCP). This base-\n",
      "line policy simply tries to ﬁnish all of the n experiments as quickly as possible.\n",
      "As such, it keeps all l labs busy as long as there are experiments left to run.\n",
      "Speciﬁcally whenever a lab (or labs) becomes free the policy immediately uses\n",
      "SelectBatch with the latest information to select new experiments to start right\n",
      "away. This policy will achieve a low value of expected CPE since it maximizes\n",
      "concurrency. Online Minimum Eager Lab Policy (OnMEL). One problem with\n",
      "OnFCP is that it does not attempt to use the full time horizon. The OnMEL\n",
      "policy simply restricts OnFCP to use only k labs, where k is the minimum\n",
      "number of labs required to guarantee with probability at least p that all n\n",
      "experiments complete within the horizon. Monte-Carlo simulation is used to\n",
      "estimate p for each k. Policy Switching (PS). Our policy switching approach\n",
      "decides the number of new experiments to start at each decision epoch. Decision\n",
      "epochs are assumed to occur every ? units of time, where ? is a small constant\n",
      "relative to the expected experiment durations. The motivation behind policy\n",
      "switching is to exploit the availability of a policy generator that can produce\n",
      "multiple policies at any decision epoch, where at least one of them is expected\n",
      "to be good. Given such a generator, the goal is to deﬁne a new (switching)\n",
      "policy that performs as well or better than the best of the generated policies in\n",
      "any state. In our case, the ob jective is to improve CPE, though other ob jectives\n",
      "can also be used. This is motivated by prior work on policy switching [6] over a\n",
      "ﬁxed policy library, and generalize that work to handle arbitrary policy gener-\n",
      "ators instead of static policy libraries. Below we describe the general approach\n",
      "and then the speciﬁc policy generator that we use. Let t denote the number of\n",
      "remaining decision epochs (stages-to-go), which is originally equal to bh/?c and\n",
      "decremented by one each epoch. We use s to denote the experimental state of\n",
      "the scheduling problem, which encodes the number of completed experiments\n",
      "and ongoing experiments with their elapsed running time. We assume access to\n",
      "a policy generator ?(s, t) which returns a set of base scheduling policies (pos-\n",
      "sibly nonstationary) given inputs s and t. Prior work on policy switching [6]\n",
      "corresponds to the case where ?(s, t) returns a ﬁxed set of policies regardless of s\n",
      "and t. Given ?(s, t), ? ? (s, t, ?) denotes the resulting switching policy based on\n",
      "\n",
      "========\n",
      "8\n",
      "\n",
      "========\n",
      "s, t, and the base policy ? selected in the previous epoch. The decision returned\n",
      "by ? ? is computed by ﬁrst conducting N simulations of each policy returned by\n",
      "?(s, t) along with ? to estimate their CPEs. The base policy with the highest\n",
      "estimated CPE is then selected and its decision is returned by ? ? . The need\n",
      "to compare to the previous policy ? is due to the use of a dynamic policy gen-\n",
      "erator, rather than a ﬁxed library. The base policy passed into policy switching\n",
      "for the ﬁrst decision epoch can be arbitrary. Despite its simplicity, we can make\n",
      "guarantees about the quality of ? ? assuming a bound on the CPE estimation\n",
      "error. In particular, the CPE of the switching policy will not be much worse\n",
      "than the best of the policies produced by our generator given accurate simula-\n",
      "tions. We say that a CPE estimator is -accurate if it can estimate the CPE Ct?\n",
      "(s) of any base policy ? for any s and t within an accuracy bound of . Below we\n",
      "denote the expected CPE of ? ? for s, t, and ? to be Ct?? (s, ?). Theorem 2.\n",
      "Let ?(s, t) be a policy generator and ? ? be the switching policy computed with\n",
      "(s, ?) ? max?0 ??(s,t)?{?} Ct? (s) ? 2t. We use a simple policy generator ?(s,\n",
      "-accurate 0 estimates. For any state s, stages-to-go t, and base policy ?, Ct??\n",
      "t) that makes multiple calls to the oﬄine IL scheduler described earlier. The\n",
      "intuition is to notice that the produced p-safe schedules are fairly pessimistic in\n",
      "terms of the experiment runtimes. In reality many experiments will ﬁnish early\n",
      "and we can adaptively exploit such situations. Speciﬁcally, rather than follow\n",
      "the ﬁxed oﬄine schedule we may choose to use fewer labs and hence improve\n",
      "CPE. Similarly if experiments run too long, we will increase the number of labs.\n",
      "6\n",
      "\n",
      "========\n",
      "Table 1: Benchmark Functions\n",
      "1 ? (u2 + v 2 ? 0.3cos(3?u) ? 0.3cos(3?v)) Rosenbrock(2)[1] 10 ? 100(y ?\n",
      "x2 )2 ? (1 ? x)2 u = 1.6x ? 0.5,d v = 1.6y ? 0.5 2 2 20 P ?i=1 4?i exp ??j=1 Aij\n",
      "(xj ? Pij ) i Hartman(3,6)[7] Michalewicz(5)[9]? 5i=1 sin(xi ). sin i.x ? ?1?4 ,\n",
      "A4?d , P4?d are constants 1 Shekel(4)[7] ?10 ?1?10 , A4?10 are constants i=1 ?\n",
      "+? 4(x ?A )2 Cosines(2)[1]\n",
      "i\n",
      "j=1\n",
      "j\n",
      "ji\n",
      "We deﬁne ?(s, t) to return k + 1 policies, {?(s,t,0) , . . . , ?(s,t,k) }, where\n",
      "k is the number of experiments running in s. Policy ?(s,t,i) is deﬁned so that it\n",
      "waits for i current experiments to ﬁnish, and then uses the oﬄine IL scheduler\n",
      "to return a schedule. This amounts to adding a small lookahead to the oﬄine IL\n",
      "scheduler where diﬀerent amounts of waiting time are considered 1 . Note that\n",
      "the deﬁnition of these policies depends on s and t and hence can not be viewed\n",
      "as a ﬁxed set of static policies as used by traditional policy switching. In the\n",
      "initial state s0 , ?(s0 ,h,0) corresponds to the oﬄine IL schedule and hence the\n",
      "above theorem guarantees that we will not perform much worse than the oﬄine\n",
      "IL, with the expectation of performing much better. Whenever policy switching\n",
      "selects a ?i with i ¿ 0 then no new experiments will be started and we wait\n",
      "for the next decision epoch. For i = 0, it will apply the oﬄine IL scheduler to\n",
      "return a p-safe schedule to start immediately, which may require starting new\n",
      "\n",
      "========\n",
      "9\n",
      "\n",
      "========\n",
      "labs to ensure high probability of completing n experiments.\n",
      "6\n",
      "Experiments\n",
      "Implementation of SelectBatch. Given the set of completed experiments\n",
      "O and on-going experiments A, SelectBatch selects k new experiments. We\n",
      "implement SelectBatch based on a recent batch BO algorithm [2], which greedily\n",
      "selects k experiments considering only O. We modify this greedy algorithm to\n",
      "also consider A by forcing the selected batch to include the ongoing experiments\n",
      "plus k additional experiments. SelectBatch makes selections based on a posterior\n",
      "over the unknown function f . We use Gaussian Process Pd with the RBF\n",
      "kernel and the kernel width = 0.01 i=1 li , where li is the input space length in\n",
      "dimension i. Benchmark Functions. We evaluate our scheduling policies using\n",
      "6 well-known synthetic benchmark functions (shown in Tab. 1 with dimension\n",
      "inside the parenthesis) and two real-world benchmark functions Hydrogen and\n",
      "FuelCell over [0, 1]2 [2]. The Hydrogen data is produced by a study on biosolar\n",
      "hydrogen production [5], where the goal was to maximize hydrogen production\n",
      "of a particular bacteria by optimizing PH and Nitrogen levels. The FuelCell\n",
      "data was collected in our motivating application mentioned in Sect. 1. In both\n",
      "cases, the benchmark function was created by ﬁtting regression models to the\n",
      "available data. Evaluation. We consider a p-safeness guarantee of p = 0.95 and\n",
      "the number of available labs l is 10. For pd (x), we use one sided truncated\n",
      "normal distribution such that x ? (0, inf ) with ? = 1, ? 2 = 0.1, and we set\n",
      "the total number of experiments n = 20. We consider three time horizons h\n",
      "of 6, 5, and 4. Given l, n and h, to evaluate policy ? using function f (with\n",
      "a set of initial observed experiments), we execute ? and get a set X of n or\n",
      "fewer completed experiments. We measure the regret of ? as the diﬀerence\n",
      "between the optimal value of f (known for all eight functions) and the f value of\n",
      "the predicted best experiment in X. Results. Table 2 shows the results of our\n",
      "proposed oﬄine and online schedulers. We also include, as a reference point,\n",
      "the result of the un-constrained sequential policy (i.e., selecting one experiment\n",
      "at a time) using SelectBatch, which can be viewed as an eﬀective upper bound\n",
      "on the optimal performance of any constrained scheduler because it ignores the\n",
      "time horizon (h = ?). The values in the table correspond to the regrets (smaller\n",
      "values are better) achieved by each policy, averaged across 100 independent runs\n",
      "with the same initial experiments (5 for 2-d and 3-d functions and 20 for the\n",
      "rest) for all policies in each run. 1 For simplicity our previous discussion of the\n",
      "IL scheduler did not consider states with ongoing experiments, which will occur\n",
      "here. To handle this the scheduler ﬁrst considers using already executing labs\n",
      "taking into account how long they have been running. If more labs are required\n",
      "to ensure p-safeness new ones are added.\n",
      "7\n",
      "Table 2: The proposed policies results for diﬀerent horizons. h=4 Functionh\n",
      "= ? OnFCP OfStaged OfIL OnMEL Cosines .142 .339 .181 .195 .275 .182 .191\n",
      ".258 FuelCell .160 .240 Hydro .025 .115 .069 .070 .123 .008 .013 .010 .009 .013\n",
      "Rosen Hart(3) .037 .095 .070 .069 .096 .509 .508 .525 Michal .465 .545 Shekel\n",
      ".427 .660 .630 .648 .688 Hart(6) .265 .348 .338 .340 .354 CPE 190 55 100 100 66\n",
      "\n",
      "========\n",
      "10\n",
      "\n",
      "========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h=5 PS OfStaged OfIL OnMEL .205 .181 .194 .274 .206 .167 .190 .239 .059\n",
      ".071 .069 .086 .008 .009 .008 .011 .067 .055 .064 .081 .502 .500 .510 .521 .623\n",
      ".635 .645 .682 .347 .334 .330 .333 100 100 100 91\n",
      "h=6 PS OfStaged OfIL OnMEL .150 .167 .147 .270 .185 .154 .163 .230 .042\n",
      ".036 .035 .064 .008 .007 .009 .010 .045 .045 .050 .070 .494 .477 .460 .502 .540\n",
      ".530 .564 .576 .297 .304 .266 .301 118 133 137 120\n",
      "PS .156 .153 .025 .009 .038 .480 .510 .262 138\n",
      "We ﬁrst note that the two oﬄine algorithms (OfStages and OfIL) perform\n",
      "similarly across all three horizon settings. This suggests that there is limited\n",
      "beneﬁt in these scenarios to using the more ﬂexible IL schedules, which were\n",
      "primarily introduced for use in the online scheduling context. Comparing with\n",
      "the two online baselines (OnFCP and OnMEL), the oﬄine algorithms perform\n",
      "signiﬁcantly better. This may seem surprising at ﬁrst because online policies\n",
      "should oﬀer more ﬂexibility than ﬁxed oﬄine schedules. However, the oﬄine\n",
      "schedules purposefully wait for experiments to complete before starting up new\n",
      "experiments, which tends to improve the CPE values. To see this, the last\n",
      "row of Table 2 gives the average CPEs of each policy. Both OnFCP and On-\n",
      "MEL yield signiﬁcantly lower CPEs compared to the oﬄine algorithms, which\n",
      "correlates with their signiﬁcantly larger regrets. Finally, policy switching con-\n",
      "sistently outperforms other policies (excluding h = ?) on the medium horizon\n",
      "setting and performs similarly in the other settings. This makes sense since the\n",
      "added ﬂexibility of PS is not as critical for long and short horizons. For short\n",
      "horizons, there is less opportunity for scheduling choices and for longer horizons\n",
      "the scheduling problem is easier and hence the oﬄine approaches are more com-\n",
      "petitive. In addition, looking at Table 2, we see that PS achieves a signiﬁcantly\n",
      "higher CPE than oﬄine approaches in the medium horizon, and is similar to\n",
      "them in the other horizons, again correlating with the regret. Further examina-\n",
      "tion of the schedules produced by PS indicates that although it begins with the\n",
      "same number of labs as OfIL, PS often selects fewer labs in later steps if early\n",
      "experiments are completed sooner than expected, which leads to higher CPE\n",
      "and consequently better performance. Note that the variances of the proposed\n",
      "policies are very small which are shown in the supplementary materials.\n",
      "7\n",
      "Summary and Future Work\n",
      "Motivated by real-world applications we introduced a novel setting for Bayesian\n",
      "optimization that incorporates a budget on the total time and number of ex-\n",
      "periments and allows for concurrent, stochastic-duration experiments. We con-\n",
      "sidered oﬄine and online approaches for scheduling experiments in this setting,\n",
      "relying on a black box function to intelligently select speciﬁc experiments at\n",
      "their scheduled start times. These approaches aimed to optimize a novel ob-\n",
      "jective function, Cumulative Prior Experiments (CPE), which we empirically\n",
      "demonstrate to strongly correlate with performance on the original optimiza-\n",
      "tion problem. Our oﬄine scheduling approaches signiﬁcantly outperformed some\n",
      "natural baselines and our online approach of policy switching was the best over-\n",
      "all performer. For further work we plan to consider alternatives to CPE, which,\n",
      "for example, incorporate factors such as diminishing returns. We also plan to\n",
      "\n",
      "========\n",
      "11\n",
      "\n",
      "========\n",
      "study further extensions to the experimental model for BO and also for active\n",
      "learning. For example, taking into account varying costs and duration distri-\n",
      "butions across labs and experiments. In general, we believe that there is much\n",
      "opportunity for more tightly integrating scheduling and planning algorithms\n",
      "into BO and active learning to more accurately model real-world conditions.\n",
      "Acknowledgments The authors acknowledge the support of the NSF under\n",
      "grants IIS-0905678. 8\n",
      "\n",
      "========\n",
      "2 References\n",
      "\n",
      "========\n",
      "[1] B. S. Anderson, A. Moore, and D. Cohn. A nonparametric approach to\n",
      "noisy and costly optimization.\n",
      "In ICML, 2000.\n",
      "[2] J. Azimi, A. Fern, and\n",
      "X. Fern. Batch bayesian optimization via simulation matching.\n",
      "In NIPS,\n",
      "2010. [3] D. Bond and D. Lovley. Electricity production by geobacter sulfurre-\n",
      "ducens attached to electrodes. Applications of Environmental Microbiology,\n",
      "69:1548?1555, 2003.\n",
      "[4] E. Brochu, M. Cora, and N. de Freitas. A tutorial\n",
      "on Bayesian optimization of expensive cost functions, with application to ac-\n",
      "tive user modeling and hierarchical reinforcement learning. Technical Report\n",
      "TR-2009-23, Department of Computer Science, University of British Columbia,\n",
      "2009. [5] E. H. Burrows, W.-K. Wong, X. Fern, F. W. Chaplen, and R. L. Ely.\n",
      "Optimization of ph and nitrogen for enhanced hydrogen production by syne-\n",
      "chocystis sp. pcc 6803 via statistical and machine learning methods. Biotech-\n",
      "nology Progress, 25:1009?1017, 2009.\n",
      "[6] H. Chang, R. Givan, and E. Chong.\n",
      "Parallel rollout for online solution of partially observable markov decision pro-\n",
      "cesses. Discrete Event Dynamic Systems, 14:309?341, 2004.\n",
      "[7] L. Dixon and\n",
      "G. Szeg. The Global Optimization Problem: An Introduction Toward Global\n",
      "Optimization. NorthHolland, Amsterdam, 1978.\n",
      "[8] D. Jones. A taxonomy\n",
      "of global optimization methods based on response surfaces. Journal of Global\n",
      "Optimization, pages 345?383, 2001.\n",
      "[9] Z. Michalewicz. Genetic algorithms\n",
      "+ data structures = evolution programs (2nd, extended ed.). Springer-Verlag\n",
      "New York, Inc., New York, NY, USA, 1994.\n",
      "[10] D. Park and J. Zeikus. Im-\n",
      "proved fuel cell and electrode designs for producing electricity from microbial\n",
      "degradation. Biotechnol.Bioeng., 81(3):348?355, 2003.\n",
      "9\n",
      "\n",
      "========\n",
      "12\n",
      "\n",
      "========\n",
      "GRIFT: A graphical model for inferring visual\n",
      "classiﬁcation features from human data\n",
      "\n",
      "========\n",
      "Authored by:\n",
      "\n",
      "========\n",
      "Michael Ross\n",
      "Andrew Cohen\n",
      "\n",
      "========\n",
      "Abstract\n",
      "\n",
      "========\n",
      "This paper describes a new model for human visual classiﬁcation that\n",
      "enables the recovery of image features that explain human sub jects’ per-\n",
      "formance on diﬀerent visual classiﬁcation tasks. Unlike previous methods,\n",
      "this algorithm does not model their performance with a single linear clas-\n",
      "siﬁer operating on raw image pixels. Instead, it models classiﬁcation as\n",
      "the combination of multiple feature detectors. This approach extracts\n",
      "more information about human visual classiﬁcation than has been previ-\n",
      "ously possible with other methods and provides a foundation for further\n",
      "exploration.\n",
      "\n",
      "========\n",
      "1 Paper Body\n",
      "\n",
      "========\n",
      "Although a great deal is known about the low-level features computed by the\n",
      "human visual system, determining the information used to make high-level vi-\n",
      "sual classiﬁcations is an active area of research. When a person distinguishes\n",
      "between two faces, for example, what image regions are most salient? Since the\n",
      "early 1970s, one of the most important research tools for answering such ques-\n",
      "tions has been the classiﬁcation image (or reverse correlation) algorithm, which\n",
      "assumes a linear classiﬁcation model [1]. This paper describes a new approach,\n",
      "GRIFT (GRaphical models for Inferring Feature Templates). Instead of repre-\n",
      "senting human visual discrimination as a single linear classiﬁer, GRIFT models\n",
      "it as the non-linear combination of multiple independently detected features.\n",
      "This allows GRIFT to extract more detailed information about human classiﬁ-\n",
      "cation. This paper describes GRIFT and the algorithms for ﬁtting it to data,\n",
      "demonstrates the model?s eﬃcacy on simulated and human data, and concludes\n",
      "with a discussion of future research directions.\n",
      "2\n",
      "Related work\n",
      "Ahumada?s classiﬁcation image algorithm [1] models an observer?s classiﬁ-\n",
      "cations of visual stimuli with a noisy linear classiﬁer ? a ﬁxed set of weights and\n",
      "\n",
      "========\n",
      "1\n",
      "\n",
      "========\n",
      "a normally distributed threshold. The random threshold accounts for the fact\n",
      "that multiple presentations of the same stimulus are often classiﬁed inconsis-\n",
      "tently. In a typical classiﬁcation image experiment, participants are presented\n",
      "with hundreds or thousands of noise-corrupted examples from two categories\n",
      "and asked to classify each one. The noise ensures that the samples cover a large\n",
      "volume of the sample space in order to allow recovery of a unique linear clas-\n",
      "siﬁer that best explains the data. Although classiﬁcation images are useful in\n",
      "many cases, it is well established that there are domains in which recognition\n",
      "and classiﬁcation are the result of combining the detection of parts or features,\n",
      "rather than applying a single linear template. For example, Pelli et al.\n",
      "[10],\n",
      "have convincingly demonstrated that humans recognize noisy word images by\n",
      "parts, even when whole-word templates would perform better. Similarly, Gold\n",
      "et al. [7] veriﬁed that sub jects employed feature-based clas1\n",
      "four square\n",
      "S class 1\n",
      "?i ?i N\n",
      "class 1\n",
      "samples\n",
      "class 2\n",
      "C\n",
      "samples\n",
      "light-dark targets\n",
      "?0\n",
      "faces\n",
      "class 2\n",
      "?i\n",
      "class 2\n",
      "Fi\n",
      "targets\n",
      "samples class 1\n",
      "targets\n",
      "Figure 1: Left: The GRIFT model is a Bayes net that describes classiﬁca-\n",
      "tion as the result of combining N feature detectors. Right: Targets and sample\n",
      "stimuli from the three experiments. siﬁcation strategies for some simple artiﬁ-\n",
      "cial image classes. GRIFT takes the next step and infers features which predict\n",
      "human performance directly from classiﬁcation data. Most work on modeling\n",
      "non-linear, feature-based classiﬁcation in humans has focused on verifying the\n",
      "use of a predeﬁned set of features. Recent work by Cohen et al.\n",
      "[4] demon-\n",
      "strates that Gaussian mixture models can be used to recover features from hu-\n",
      "man classiﬁcation data without specifying a ﬁxed set of possible features. The\n",
      "GRIFT model, described in the remainder of this paper, has the same goals\n",
      "as the previous work, but removes several limitations of the Gaussian mixture\n",
      "model approach, including the need to only use stimuli the sub jects classiﬁed\n",
      "with high conﬁdence and the bias that the signals can exert on the recovered\n",
      "features. GRIFT achieves these and other improvements by generatively mod-\n",
      "eling the entire classiﬁcation process with a graphical model. Furthermore, the\n",
      "\n",
      "========\n",
      "2\n",
      "\n",
      "========\n",
      "similarity between single-feature GRIFT models and the classiﬁcation image\n",
      "process, described in more detail below, make GRIFT a natural successor to\n",
      "the traditional approach.\n",
      "3\n",
      "GRIFT model\n",
      "GRIFT models classiﬁcation as the result of combining N conditionally in-\n",
      "dependent feature detectors, F = {F1 , F2 , . . . , FN }. Each feature detector\n",
      "is binary valued (1 indicates detection), as is the classiﬁcation, C (1 indicates\n",
      "one class and 2 the other). The stimulus, S, is an array of continuously valued\n",
      "pixels representing the input image. The stimulus only inﬂuences C through the\n",
      "feature detectors, therefore the joint probability of a stimulus and classiﬁcation\n",
      "pair is ! N X Y P (C—F )P (S) P (Fi —S) . P (C, S) = i\n",
      "F\n",
      "Figure 1 represents the causal relationship between these variables (C, F ,\n",
      "and S) with a Bayesian network. The network also includes nodes representing\n",
      "model parameters (?, ?, and ?), whose role will be described below. The boxed\n",
      "region in the ﬁgure indicates the parts of the model that are replicated when\n",
      "N ¿ 1 ? each feature detector is represented by an independent copy of those\n",
      "variables and parameters. The distribution of the stimulus, P (S), is under\n",
      "the control of the experimenter. The algorithm for ﬁtting the model to data\n",
      "only assumes that the stimuli are independent and identically distributed across\n",
      "trials. The conditional distribution of each feature detector?s value, P (Fi —S),\n",
      "is modeled with a logistic regression function on the pixel values of S. Logistic\n",
      "regression is desirable because it is a probabilistic linear classiﬁer. Humans can\n",
      "successfully classify images in the presence of extremely high additive noise,\n",
      "which suggests the use of averaging and contrast, linear computations which 2\n",
      "are known to play important roles in human visual perception [9]. Just as\n",
      "the classiﬁcation image used a random threshold to represent uncertainty in\n",
      "the output of its single linear classiﬁer, logistic regression also allows GRIFT\n",
      "to represent uncertainty in the output of each of its feature detectors. The\n",
      "conditional distribution of C is represented by logistic regression on the feature\n",
      "outputs. Each Fi ?s distribution has two parameters, a weight vector ?i and a\n",
      "threshold ?i , such that P (Fi = 1—S, ?i , ?i ) = (1 + exp(?i +\n",
      "—S— X\n",
      "?ij Sj ))?1 ,\n",
      "j=1\n",
      "where —S— is the number of pixels in a stimulus. Similarly, the conditional\n",
      "distribution of C is determined by ? = {?0 , ?1 , .\n",
      ", ?N } where P (C =\n",
      ".\n",
      ".\n",
      "1—F, ?) = (1 + exp(?0 +\n",
      "N X\n",
      "?i Fi ))?1 .\n",
      "i=1\n",
      "Detecting a feature with negative ?i increases the probability that the sub-\n",
      "ject will respond ?class 1,? those with positive ?i are associated with ?class\n",
      "2? responses. A GRIFT model with N features applied to the classiﬁcation of\n",
      "images each containing —S— pixels has N (—S— + 2) + 1 parameters. This\n",
      "\n",
      "========\n",
      "3\n",
      "\n",
      "========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "large number of parameters, coupled with the fact that the F variables are un-\n",
      "observable, makes ﬁtting the model to data very challenging. Therefore, GRIFT\n",
      "deﬁnes prior distributions on its parameters. These priors reﬂect reasonable as-\n",
      "sumptions about the parameter values and, if they are wrong, can be overturned\n",
      "if enough contrary data is available. The prior on each of the ?i parameters for\n",
      "which i ¿ 0 is a mixture of two normal distributions, 1 (?i ? 2)2 (?i + 2)2 P (?i )\n",
      "= ? (exp(? ) + exp(? )). 2 2 2 2? This prior reﬂects the assumption that each\n",
      "feature detector should have a signiﬁcant impact on the classiﬁcation, but no\n",
      "single detector should make it deterministic ? a single-feature model with ?0 =\n",
      "0 and ?1 = ?2 has an 88% chance of choosing class 1 if the feature is active. The\n",
      "?0 parameter has an improper non-informative prior, P (?0 ) = 1, indicating no\n",
      "preference for any particular value [5] because the best ?0 is largely determined\n",
      "by the other ?i s and the distributions of F and S. For analogous reasons, P (?i\n",
      ") = 1. The ?i parameters, which each have dimensionality equal to the stimu-\n",
      "lus, present the biggest inferential challenge. As mentioned previously, human\n",
      "visual processing is sensitive to contrasts between image regions. If one image\n",
      "region is assigned positive ?ij s and another is assigned negative ?ij s, the feature\n",
      "detector will be sensitive to the contrast between them. This contrast between\n",
      "regions requires all the pixels within each region to share similar ?ij values. To\n",
      "encourage this local structure, the ?i parameters have Markov random ﬁeld prior\n",
      "distributions: ? ?? ? 2 2 2 Y Y (?ij + 1) (?ij ? 1) ? ? (?ij ? ?ik ) ? P (?i ) ? ?\n",
      "(exp(? ) + exp(? )) ) , exp(? 2 2 2 j (j,k)?A\n",
      "where A is the set of neighboring pixel locations. The ﬁrst factor encourages\n",
      "weight values to be near the -1 to 1 range, while the second encourages the\n",
      "assignment of similar weights to neighboring pixels. Fitting the model to data\n",
      "does not require the normalization of this distribution. The Bayesian joint\n",
      "probability distribution of all the parameters and variables is P (C, F, S, ?, ?,\n",
      "?) = P (C—F, ?)P (S)P (?0 )\n",
      "N Y\n",
      "P (Fi —S, ?i , ?i )P (?i )P (?i )P (?i ).\n",
      "(1)\n",
      "i=1\n",
      "4\n",
      "GRIFT algorithm\n",
      "The goal of the algorithm is to ﬁnd the parameters that satisfy the prior\n",
      "distributions and best account for the (S, C) samples gathered from a human\n",
      "sub ject. Mathematically, this goal corresponds to ﬁnding the mode of P (?, ?,\n",
      "?—S, C), where S and C refer to all of the observed samples. The 3\n",
      "algorithm is derived using the expectation-maximization (EM) method [3],\n",
      "a widely used optimization technique for dealing with unobserved variables, in\n",
      "this case F, the feature detector outputs for all the trials. In order to determine\n",
      "the most probable parameter assignments, the algorithm chooses random initial\n",
      "parameters ?? = (? ? , ? ? , ?? ) and then ﬁnds the ? that maximizes X\n",
      "Q(?—?? ) = P (F—S, C, ?? ) log P (C, F, S—?) + log P (?). F ?\n",
      "Q(?—? ) is the expected log posterior probability of the parameters com-\n",
      "puted by using the current ?? to estimate the distribution of F, the unobserved\n",
      "\n",
      "========\n",
      "4\n",
      "\n",
      "========\n",
      "feature detector activations. The ? that maximizes Q then becomes ?? for the\n",
      "next iteration, and the process is repeated until convergence. The presence of\n",
      "both the P (C, F, S—?) and P (?) terms encourages the algorithm to ﬁnd\n",
      "parameters that explain the data and match the assumptions encoded in the\n",
      "parameter prior distributions. As the amount of available data increases, the\n",
      "inﬂuence of the priors decreases, so it is possible to discover features that are\n",
      "contrary to prior belief given enough evidence. Using the conditional indepen-\n",
      "dences from the Bayes net: ?\n",
      "Q(?—? ) ?\n",
      "X\n",
      "?\n",
      "P (F—S, C, ? ) log P (C—F, ?) +\n",
      "! log P (Fi —S, ?i , ?i )\n",
      "i=1\n",
      "F\n",
      "+\n",
      "N X\n",
      "N X\n",
      "(log P (?i ) + log P (?i )) ,\n",
      "i=1\n",
      "dropping the log P (S) term, which is independent of the parameters, and\n",
      "the log P (?0 ) and log P (?i ) terms, which are 0. As mentioned before,\n",
      "the normalization terms for the log P (?i ) elements can be ignored during\n",
      "optimization ? the log makes them additive constants to Q. The functional\n",
      "form of every additive term is described in Section 3, and P (F—S, C, ?? ) can\n",
      "be calculated using the model?s joint probability function (Equation 1). Each\n",
      "iteration of EM requires maximizing Q, but it is not possible to compute the\n",
      "maximizing ? in closed form. Fortunately, it is relatively easy to search for the\n",
      "best ?. Because Q is separable into many additive components, it is possible to\n",
      "eﬃciently compute its gradient with respect to each of the elements of ? and\n",
      "use this information to ﬁnd a locally maximum ? assignment using the scaled\n",
      "conjugate gradient algorithm [2]. Even a locally maximum value of ? usually\n",
      "provides good EM results ? P (?, ?, ?—S, C) is still guaranteed to improve\n",
      "after every iteration. The result of any EM procedure is only guaranteed to\n",
      "be a locally optimal answer, and ﬁnding the globally optimal ? is made more\n",
      "challenging by the large number of parameters. GRIFT adopts the standard\n",
      "solution of running EM many times, each instance starting with a random ??\n",
      ", and then accepting the ? from the run which produced the most probable\n",
      "parameters. For this model and the data presented in the following sections,\n",
      "20-30 random restarts were suﬃcient.\n",
      "5\n",
      "Experiments\n",
      "The GRIFT model was ﬁt to data from 3 experiments. In each experiment,\n",
      "human participants classiﬁed stimuli into two classes. Each class contained\n",
      "one or more target stimuli.\n",
      "In each trial, the participant saw a stimulus (a\n",
      "sample from S) that consisted of a randomly chosen target with high levels of\n",
      "\n",
      "========\n",
      "5\n",
      "\n",
      "========\n",
      "independent identically distributed noise added to each pixel. The noise samples\n",
      "were drawn from a truncated normal distribution to ensure that the stimulus\n",
      "pixel values remained within the display?s output range. Figure 1 shows the\n",
      "classes and targets from each experiment and a sample stimulus from each\n",
      "class. In the four-square experiment four participants were asked to distinguish\n",
      "between two artiﬁcial stimulus classes, one in which there were bright squares\n",
      "in the upper-left or upper-right corners and one in which there were bright\n",
      "squares in the lower-left or lower-right corners.\n",
      "In the light-dark experiment\n",
      "three participants were asked to distinguish between three strips that each had\n",
      "two light blobs and three strips that each had only one light blob. Finally, in the\n",
      "faces experiment three participants were asked to distinguish between two faces.\n",
      "The four-square data were collected by [7] and were also analyzed in [4]. The\n",
      "other data are newly gathered. Each data set consists of approximately 4000\n",
      "trials from each sub ject. To maintain their interest in the task, participants\n",
      "were given auditory feedback after each trial that indicated success or failure. 4\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "2 2\n",
      "0.2\n",
      "simulations\n",
      "0.15\n",
      "a corners bz ACc da EA b a cbaz JG d cb c RS d d\n",
      "0.1\n",
      "0.05 1 0.25\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "humans\n",
      "0.2 0.15 0.1 0.05 1\n",
      "2\n",
      "3\n",
      "4\n",
      "N + Figure 2: The most probable ? parameters found for the four-square\n",
      "experiments for diﬀerent values of N and the4 mutual information between these\n",
      "feature detectors and the observed classiﬁcations.\n",
      "especially sensitive to the random initialization procedure used to start 3\n",
      "Fitting GRIFT 4 models is not ?\n",
      "2 3\n",
      "four square: most probable ?i values N=2 N=3 N=4 mutual information\n",
      "simulations humans\n",
      "2\n",
      "top v. bottom\n",
      "N=1\n",
      "\n",
      "========\n",
      "6\n",
      "\n",
      "========\n",
      "5\n",
      "5\n",
      "6\n",
      "6\n",
      "7\n",
      "7\n",
      "each EM instance. The ? parameters were initialized by normal random\n",
      "samples and then half\n",
      "8 9 10 3 were 4 so the features would tend to start evenly assigned to the two\n",
      "classes, except for ??0 , negated 3 which was initialized 4 to 0. In the four-square\n",
      "experiments, the ? ? parameters were initialized by\n",
      "a mixture of normal distributions and in the light-dark experiments they\n",
      "were initialized from a uniform distribution.\n",
      "In the faces experiments the ?\n",
      "? were initialized by adding normal noise to the 8 9 10 optimal linear classi-\n",
      "ﬁer separating the two targets. Because of the large number of pixels in the\n",
      "faces stimuli, the other initialization procedures frequently produced initial as-\n",
      "signments with extremely low probabilities, which led to numerical precision\n",
      "problems. In the four-square experiments, the ? ? were initialized randomly.\n",
      "In the other experiments, the intent was to set them to the optimal threshold\n",
      "for distinguishing the classes using the initial ? ? as a linear classiﬁer, but a\n",
      "programming error set them to the negation of that value. In most cases, the\n",
      "results were insensitive to the choice of initialization method. In the four-square\n",
      "experiment, the noise levels were continually adjusted to keep the participants?\n",
      "performance at approximately 71% using the stair-casing algorithm [8]. This\n",
      "performance level is high enough to keep the participants engaged in the task,\n",
      "but allows for suﬃcient noise to explore their responses in a large volume of\n",
      "the stimulus space. After an initial adaptation period, the noise level remains\n",
      "relatively constant across trials, so the inter-trial dependence introduced by\n",
      "the stair-casing can be safely ignored. Two simulated observers were created\n",
      "to validate GRIFT on the four-square task. Each used a GRIFT model with\n",
      "pre-speciﬁed parameters to probabilistically classify four-square data at a ﬁxed\n",
      "noise level, which was chosen to produce approximately 70% correct perfor-\n",
      "mance. The corners observer used four feature detectors, one for each bright\n",
      "corner, whereas the top-v.-bottom observer contrasted the brightness of the top\n",
      "and bottom pixels. The result of using GRIFT to recover the feature detectors\n",
      "are displayed in Figure 2. Only the ? parameters are displayed because they\n",
      "are the most informative. Dark pixels indicate negative weights and bright pix-\n",
      "els correspond to positive weights. The presence of dark and light regions in\n",
      "a feature detector indicates the computation of contrasts between those areas.\n",
      "The sign of the weights is not signiﬁcant ? given a ﬁxed number of features,\n",
      "there are typically several equivalent sets of feature detectors that only diﬀer\n",
      "from each other in the signs of their ? terms and in the associated ? and ?\n",
      "values. Because the optimal number of features for human sub jects is unknown,\n",
      "GRIFT models with 1?4 features were ﬁt to the data from each sub ject. The\n",
      "correct number of features could be determined by holding out a test set or by\n",
      "performing cross-validation. Simulation demonstrated that a reliable test set\n",
      "\n",
      "========\n",
      "7\n",
      "\n",
      "========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "would need to contain nearly all of the gathered samples, and computational\n",
      "expense made cross-validation impractical with our current MATLAB imple-\n",
      "mentation. Instead, after recovering the parameters, we estimated the mutual\n",
      "information between the unobserved F variables and the observed classiﬁcations\n",
      "C. Mutual information measures how well the feature detector outputs can 5\n",
      "predict the sub ject?s classiﬁcation decision. Unlike the log likelihood of the\n",
      "observations, which is dependent on the choice to model C with a logistic re-\n",
      "gression function, mutual information does not assume a particular relationship\n",
      "between F and C and does not necessarily increase with N . Plotting the mutual\n",
      "information as N increases can indicate if new detectors are making a sub-\n",
      "stantial contribution or are overﬁtting the data. On the simulated observers?\n",
      "data, for which the true values of N were known, mutual information was a\n",
      "more accurate model selection indicator than traditional statistics such as the\n",
      "Bayesian or Akaike information criteria [3]. Fitting GRIFT to the simulated\n",
      "observers demonstrated that if the model is accurate, the correct features can\n",
      "be recovered reliably. The top-v.-bottom observer showed no substantial in-\n",
      "crease in mutual information as the number of features increased from 1 to 4.\n",
      "Each set of recovered feature detectors included a top-bottom contrast detector\n",
      "and other detectors with noisy ?i s that did not contribute much to predicting\n",
      "C. Although the observer truly used two detectors, one top-brighter detector\n",
      "and one bottom-brighter detector, the recovery of only one top-bottom con-\n",
      "trast detector is a success because one contrast detector plus a suitable ?0 term\n",
      "is logically equivalent to the original two-feature model. The corners observer\n",
      "showed a substantial increase in mutual information as N increased from 1 to\n",
      "4 and the ? values clearly indicate four corner-sensitive feature detectors. The\n",
      "corners data was also tested with a ﬁve-feature GRIFT model (? not shown)\n",
      "which produced four corner detectors and one feature with noisy ?i . Its gain\n",
      "in mutual information was smaller than that observed on any of the previous\n",
      "steps. Note the corner areas in the ?i s recovered from the corners data are\n",
      "sometimes black and sometimes white. Recall that these are not image pixel\n",
      "values that the detectors are attempting to match, but positive and negative\n",
      "weights indicating that the brightness in the corner region is being contrasted\n",
      "to the brightness of the rest of the image. Even though targets consisted of four\n",
      "bright-corner stimuli, recovering the parameters from the topv.-bottom observer\n",
      "never produced ? values indicating corner-speciﬁc feature detectors. An impor-\n",
      "tant advantages of GRIFT over previous methods such as [4] is that targets\n",
      "will not ?contaminate? the recovered detectors. The simulations demonstrate\n",
      "that the recovered detectors are determined by the classiﬁcation strategy, not\n",
      "by the structure of the targets and classes. The data of the four human par-\n",
      "ticipants revealed some interesting diﬀerences. Participants EA and RS were\n",
      "naive, while AC and JG were not. The largest disparity was between EA and\n",
      "JG. EA?s data indicated no consistent pattern of mutual information increase\n",
      "after two features, and the twofeature model appears to contain two top-bottom\n",
      "contrast detectors. Therefore, it is reasonable to conclude that EA was not ex-\n",
      "plicitly detecting the corners. At the other extreme is participant JG, whose\n",
      "data shows four very clear corner detectors and a steady increase in mutual\n",
      "\n",
      "========\n",
      "8\n",
      "\n",
      "========\n",
      "information up to four features. Therefore, it seems very likely that this partic-\n",
      "ipant was matching corners and probably should be tested with a ﬁve-feature\n",
      "model to gain additional insight. AC and RS?s data suggest three corner detec-\n",
      "tors and a top-bottom contrast detector. GRIFT?s output indicates qualitative\n",
      "diﬀerences in the classiﬁcation strategies used by the four human participants.\n",
      "Across all participants, the best one-feature model was based on the contrast\n",
      "between the top of the image and the bottom. This is extremely similar to\n",
      "the result produced by a classiﬁcation image of the data, reinforcing the strong\n",
      "similarity between one-feature GRIFT and that approach.\n",
      "In the light-dark\n",
      "and faces experiments, stair-casing was used the adjust the noise level to the\n",
      "71% performance level at the beginning of each session and then the noise level\n",
      "was ﬁxed for the remaining trials to improve the independence of the samples.\n",
      "Participants were paid and promised a $10 reward for achieving the highest\n",
      "score on the task. Participants P1, P2, and P3 classiﬁed the light-dark stimuli.\n",
      "P1 and P2 achieved at or above the expected performance level (82% and 73%\n",
      "accuracy), while P3?s performance was near chance (55%). Because the noise\n",
      "levels were ﬁxed after the ﬁrst 101 trials, a participant with good luck at the end\n",
      "of that period could experience very high noise levels for the remainder of the\n",
      "experiment, leading to poor performance. All three participants appear to have\n",
      "used diﬀerent classiﬁcation methods, providing a very informative contrast. The\n",
      "results of ﬁtting the GRIFT model are in Figure 3. The ﬂat mutual information\n",
      "graph and the presence of a feature detector thresholding the overall bright-\n",
      "ness for each value of N indicate that P1 pursued a one-feature, linear-classiﬁer\n",
      "strategy. P2, on the other hand, clearly employed a multi-feature, non-linear\n",
      "strategy. For N = 1 and N = 2, the most interpretable feature detector is an\n",
      "overall brightness detector, which disappears when N = 3 and the best ﬁt model\n",
      "consists of three detectors looking for speciﬁc patterns, one for each position a\n",
      "6\n",
      "a b light-dark: most probable ?i values P1 c P2 a P3 N=1 0.25 d b N=2\n",
      "0.25 c 0.2 d N=3 0.2\n",
      "9\n",
      "8\n",
      "7\n",
      "N=4\n",
      "5\n",
      "a ab bc P5 d c\n",
      "d\n",
      "0.15 1 0.1 0.9 0.1 0.05 0.7 1 0.05 0.6 1\n",
      "P6\n",
      "2\n",
      "3\n",
      "N=5\n",
      "4\n",
      "2\n",
      "3\n",
      "2\n",
      "\n",
      "========\n",
      "9\n",
      "\n",
      "========\n",
      "3\n",
      "mutual information\n",
      "3\n",
      "0.5\n",
      "2\n",
      "0.4\n",
      "2\n",
      "1\n",
      "z\n",
      "-\n",
      "0.8\n",
      "4\n",
      "0\n",
      "P4\n",
      "0.15\n",
      "6\n",
      "faces: most probable ?i values N=2 N=3\n",
      "N=1\n",
      "z\n",
      "0.2\n",
      "light-dark\n",
      "4\n",
      "0.15\n",
      "4\n",
      "0.1 0.05\n",
      "0 1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "mutual information\n",
      "1\n",
      "0.04\n",
      "+ faces\n",
      "0.03 0.02 0.01 0 1\n",
      "2\n",
      "3\n",
      "N N 34 + Figure 3: The most probable ? parameters found for the light-dark\n",
      "and faces experiments for diﬀer0.3\n",
      "0.2 the mutual information between these feature detectors and the observed\n",
      "classiﬁcations. ent N and\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "\n",
      "========\n",
      "10\n",
      "\n",
      "========\n",
      "6\n",
      "0.1\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "0 bright or dark spot Then 1 2 3can appear. 4 5 6 7when 8 N 9 = 410the\n",
      "overall brightness detector reappears, added to the three spot detectors. Ap-\n",
      "parently the spot detectors are only eﬀective if they are all present. With only\n",
      "three available detectors, the overall brightness detector is excluded, but the op-\n",
      "timal assignment includes all four detectors. This is the best-ﬁt model because\n",
      "increasing to N = 5 keeps the mutual information constant and adds a detector\n",
      "that is active for every stimulus. Always active detectors function as constant\n",
      "additions to ?0 , therefore this is equivalent to the N = 4 solution.\n",
      "The GRIFT models of participant P3 do not show a substantial increase in\n",
      "mutual information as the number of features rises. This lack of increase leads\n",
      "to the conclusion that the one-feature model is probably the best ﬁt, and since\n",
      "performance was extremely low, it can be assumed that the sub ject was reduced\n",
      "to near random guessing much of the time. The clear distinction between the\n",
      "results for all three sub jects demonstrates the eﬀectiveness of GRIFT and the\n",
      "mutual information measure in distinguishing between classiﬁcation strategies.\n",
      "The faces presented the largest computational challenges. The targets were two\n",
      "unﬁltered faces from Gold et al.?s data set [6], down-sampled to 128x128. After\n",
      "the experiment, the stimuli were down-sampled further to 32x32 and the back-\n",
      "ground surrounding the faces was removed by cropping, reducing the stimuli\n",
      "to 26x17. These steps made the algorithm computationally feasible, and re-\n",
      "duced the number of parameters so they would be suﬃciently constrained by\n",
      "the samples. The results for three participants (P4, P5, and P6) are in Figure\n",
      "3. Participants P4 and P5?s data were clearly best ﬁt by one-feature GRIFT\n",
      "models. Increasing the number of features simply caused the algorithm to add\n",
      "features that were never or always active. Never active features cannot aﬀect\n",
      "the classiﬁcation, and, as explained previously, always active features are also\n",
      "superﬂuous. P4?s onefeature model clearly places signiﬁcant weight near the\n",
      "eyebrows, nose, and other facial features. P5?s one-feature weights are much\n",
      "noisier and harder to interpret. This might be related to P5?s poor performance\n",
      "on the task ? only 53% accuracy compared to P4?s 72% accuracy. Perhaps the\n",
      "noise level was too high and P5 was guessing rather than using image informa-\n",
      "tion much of the time. Participant P6?s data did produce a two-feature GRIFT\n",
      "model, albeit one that is diﬃcult to interpret and which only caused a small\n",
      "rise in mutual information. Instead of recovering independent part detectors,\n",
      "such as a nose detector and an eye detector, GRIFT extracted two subtly dif-\n",
      "ferent holistic feature detectors. Given P6?s poor performance (58% accuracy)\n",
      "these features may, like P5?s results, be indicative of a guessing strategy that\n",
      "was not strongly inﬂuenced by the image information. The results on faces sup-\n",
      "port the hypothesis that face classiﬁcation is holistic and conﬁgural, rather than\n",
      "the result of part classiﬁcations, especially when individual feature detection is\n",
      "\n",
      "========\n",
      "11\n",
      "\n",
      "========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diﬃcult [11]. 7\n",
      "Across these experiments, the data collected were compatible with the orig-\n",
      "inal classiﬁcation image method.\n",
      "In fact, the four-square human data were\n",
      "originally analyzed using that algorithm. One of the advantages of GRIFT is\n",
      "that it can reanalyze old data to reveal new information.\n",
      "In the onefeature\n",
      "case, GRIFT enables the use of prior probabilities on the parameters, which\n",
      "may improve performance when data is too scarce for the classiﬁcation image\n",
      "approach. Most importantly, ﬁtting multi-feature GRIFT models can reveal\n",
      "previously hidden non-linear classiﬁcation strategies.\n",
      "6\n",
      "Conclusion\n",
      "This paper has described the GRIFT model for determining the features\n",
      "used in human image classiﬁcation. GRIFT is an advance over previous meth-\n",
      "ods that assume a single linear classiﬁer on pixels because it describes classiﬁca-\n",
      "tion as the combination of multiple independently detected features. It provides\n",
      "a probabilistic model of human visual classiﬁcation that accounts for data and\n",
      "incorporates prior beliefs about the features. The feature detectors it ﬁnds are\n",
      "associated with the classiﬁcation strategy employed by the observer and are\n",
      "not the result of structure in the classes? target images. GRIFT?s value has\n",
      "been demonstrated by modeling the performance of humans on the four-square,\n",
      "light-dark, and faces classiﬁcation tasks and by successfully recovering the pa-\n",
      "rameters of computer simulated observers in the four-square task. Its inability\n",
      "to ﬁnd multiple local features when analyzing human performance on the faces\n",
      "task agrees with previous results. One of the strengths of the graphical model\n",
      "approach is that it allows easy replacement of model components. An expert\n",
      "can easily change the prior distributions on the parameters to reﬂect knowledge\n",
      "gained in previous experiments. For example, it might be desirable to encourage\n",
      "the formation of edge detectors. New resolution-independent feature parame-\n",
      "terizations can be introduced, as can transformation parameters to make the\n",
      "features translationally and rotationally invariant. If the features have explicitly\n",
      "parameterized locations and orientations, the model could be extended to model\n",
      "their joint relative positions, which might provide more information about do-\n",
      "mains such as face classiﬁcation. The success of this version of GRIFT provides\n",
      "a ﬁrm foundation for these improvements. Acknowledgments This research was\n",
      "supported by NSF Grant SES-0631602 and NIMH grant MH16745. The authors\n",
      "thank the reviewers, Tom Griﬃths, Erik Learned-Miller, and Adam Sanborn for\n",
      "their suggestions.\n",
      "\n",
      "========\n",
      "2 References\n",
      "\n",
      "========\n",
      "[1] A.J. Ahumada, Jr. Classiﬁcation image weights and internal noise level es-\n",
      "timation. Journal of Vision, 2(1), 2002. [2] C.M. Bishop. Neural Networks for\n",
      "Pattern Recognition. Oxford University Press, 1995.\n",
      "[3] C.M. Bishop. Pat-\n",
      "tern Recognition and Machine Learning. Springer, 2006. [4] A.L. Cohen, R.M.\n",
      "Shiﬀrin, J.M. Gold, D.A. Ross, and M.G. Ross. Inducing features from visual\n",
      "\n",
      "========\n",
      "12\n",
      "\n",
      "========\n",
      "noise. Journal of Vision, 7(8), 2007.\n",
      "[5] A. Gelman, J.B. Carlin, H.S. Stern,\n",
      "and D.B. Rubin. Bayesian Data Analysis. Chapman & Hall/CRC, 2003.\n",
      "[6]\n",
      "J.M. Gold, P.J. Bennett, and A.B. Sekuler. Identiﬁcation of band-pass ﬁltered\n",
      "letters and faces by human and ideal observers. Vision Research, 39, 1999. [7]\n",
      "J.M. Gold, A.L. Cohen, and R. Shiﬀrin. Visual noise reveals category represen-\n",
      "tations. Psychonomics Bulletin & Review, 15(4), 2006. [8] N.A. Macmillan and\n",
      "C.D. Creelman. Detection Theory: A User?s Guide. Lawrence Erlbaum Asso-\n",
      "ciates, 2005. [9] S.E. Palmer. Vision Science: Photons to Phenomenology. The\n",
      "MIT Press, 1999. [10] D.G. Pelli, B. Farell, and D.C. Moore. The remarkable\n",
      "ineﬃciency of word recognition. Nature, 425, 2003. [11] J. Sergent. An inves-\n",
      "tigation into component and conﬁgural processes underlying face perception.\n",
      "British Journal of Psychology, 75, 1984.\n",
      "8\n",
      "\n",
      "========\n",
      "13\n",
      "\n",
      "========\n",
      "The Distribution Family of Similarity Distances\n",
      "\n",
      "========\n",
      "Authored by:\n",
      "\n",
      "========\n",
      "Gertjan Burghouts\n",
      "Arnold Smeulders\n",
      "Jan-mark Geusebroek\n",
      "\n",
      "========\n",
      "Abstract\n",
      "\n",
      "========\n",
      "Assessing similarity between features is a key step in ob ject recognition\n",
      "and scene categorization tasks. We argue that knowledge on the distribu-\n",
      "tion of distances generated by similarity functions is crucial in deciding\n",
      "whether features are similar or not.\n",
      "Intuitively one would expect that\n",
      "similarities between features could arise from any distribution.\n",
      "In this\n",
      "paper, we will derive the contrary, and report the theoretical result that\n",
      "$L p$-norms –a class of commonly applied distance metrics– from one fea-\n",
      "ture vector to other vectors are Weibull-distributed if the feature values\n",
      "are correlated and non-identically distributed. Besides these assumptions\n",
      "being realistic for images, we experimentally show them to hold for vari-\n",
      "ous popular feature extraction algorithms, for a diverse range of images.\n",
      "This fundamental insight opens new directions in the assessment of feature\n",
      "similarity, with pro jected improvements in ob ject and scene recognition\n",
      "algorithms.\n",
      "Erratum: The authors of paper have declared that they have become\n",
      "convinced that the reasoning in the reference is too simple as a proof of\n",
      "their claims. As a consequence, they withdraw their theorems.\n",
      "\n",
      "========\n",
      "1 Paper Body\n",
      "\n",
      "========\n",
      "Assessing similarity between features is a key step in ob ject recognition and\n",
      "scene categorization tasks. We argue that knowledge on the distribution of\n",
      "distances generated by similarity functions is crucial in deciding whether fea-\n",
      "tures are similar or not. Intuitively one would expect that similarities between\n",
      "features could arise from any distribution.\n",
      "In this paper, we will derive the\n",
      "contrary, and report the theoretical result that Lp -norms ?a class of commonly\n",
      "applied distance metrics? from one feature vector to other vectors are Weibull-\n",
      "distributed if the feature values are correlated and non-identically distributed.\n",
      "Besides these assumptions being realistic for images, we experimentally show\n",
      "them to hold for various popular feature extraction algorithms, for a diverse\n",
      "range of images. This fundamental insight opens new directions in the assess-\n",
      "\n",
      "========\n",
      "1\n",
      "\n",
      "========\n",
      "ment of feature similarity, with pro jected improvements in ob ject and scene\n",
      "recognition algorithms.\n",
      "1\n",
      "Introduction\n",
      "Measurement of similarity is a critical asset of state of the art in computer\n",
      "vision. In all three ma jor streams of current research - the recognition of known\n",
      "ob jects [13], assigning an ob ject to a class [8, 24], or assigning a scene to a type\n",
      "[6, 25] - the problem is transposed into the equality of features derived from sim-\n",
      "ilarity functions. Hence, besides the issue of feature distinctiveness, comparing\n",
      "two images heavily relies on such similarity functions. We argue that knowledge\n",
      "on the distribution of distances generated by such similarity functions is even\n",
      "more important, as it is that knowledge which is crucial in deciding whether fea-\n",
      "tures are similar or not. For example, Nowak and Jurie [21] establish whether\n",
      "one can draw conclusions on two never seen ob jects based on the similarity dis-\n",
      "tances from known ob jects. Where they build and traverse a randomized tree\n",
      "to establish region correspondence, one could alternatively use the distribution\n",
      "of similarity distances to establish whether features come from the mode or the\n",
      "tail of the distribution. Although this indeed only hints at an algorithm, it is\n",
      "likely that knowledge of the distance distribution will considerably improve or\n",
      "speed-up such tasks. As a second example, consider the clustering of features\n",
      "based on their distances. Better clustering algorithms signiﬁcantly boost perfor-\n",
      "mance for ob ject and scene categorization [12]. Knowledge on the distribution\n",
      "of distances aids in the construction of good clustering algorithms. Using this\n",
      "knowledge allows for the exact distribution shape to be used to determine clus-\n",
      "ter size and centroid, where now the Gaussian is often groundlessly assumed.\n",
      "We will show that in general distance distributions will strongly deviate from\n",
      "the Gaussian probability distribution. A third example is from ob ject and scene\n",
      "recognition. Usually this is done by measuring invariant feature sets [9, 13, 24]\n",
      "at a predeﬁned raster of regions in the image or at selected key points in the\n",
      "image [11, 13] as extensively evaluated [17]. Typically, an image contains a\n",
      "hundred regions or a ? ?\n",
      "Dr. Burghouts is now with TNO Defense, The Netherlands, gertjan.burghouts@tno.nl.\n",
      "Corresponding author. Email: mark@science.uva.nl.\n",
      "1\n",
      "thousand key points. Then, the most expensive computational step is to\n",
      "compare these feature sets to the feature sets of the reference ob jects, ob ject\n",
      "classes or scene types. Usually this is done by going over all entries in the image\n",
      "to all entries in the reference set and select the best matching pair. Knowledge\n",
      "of the distribution of similarity distances and having established its parameters\n",
      "enables a remarkable speed-up in the search for matching reference points and\n",
      "hence for matching images. When verifying that a given reference key-point\n",
      "or region is statistically unlikely to occur in this image, one can move on to\n",
      "search in the next image. Furthermore, this knowledge can well be applied in\n",
      "the construction of fast search trees, see e.g. [16]. Hence, apart from obtaining\n",
      "theoretical insights in the general distribution of similarities, the results derived\n",
      "in this paper are directly applicable in ob ject and scene recognition. Intuitively\n",
      "\n",
      "========\n",
      "2\n",
      "\n",
      "========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one would expect that the set of all similarity values to a key-point or region\n",
      "in an image would assume any distribution. One would expect that there is no\n",
      "preferred probability density distribution at stake in measuring the similarities\n",
      "to points or regions in one image. In this paper, we will derive the contrary. We\n",
      "will prove that under broad conditions the similarity values to a given reference\n",
      "point or region adhere to a class of distributions known as the Weibull distribu-\n",
      "tion family. The density function has only three parameters: mean, standard\n",
      "deviation and skewness. We will verify experimentally that the conditions un-\n",
      "der which this result from mathematical statistics holds are present in common\n",
      "sets of images. It appears the theory predicts the resulting density functions\n",
      "accurately. Our work on density distributions of similarity values compares to\n",
      "the work by Pekalska and Duin [23] assuming a Gaussian distribution for sim-\n",
      "ilarities.\n",
      "It is based on an original combination of two facts from statistical\n",
      "physics. An old fact regards the statistics of extreme values [10], as generated\n",
      "when considering the minima and maxima of many measurements. The ma jor\n",
      "result of the ﬁeld of extreme value statistics is that the probability density in\n",
      "this case can only be one out of three diﬀerent types, independent of the un-\n",
      "derlying data or process. The second fact is a new result, which links these\n",
      "extreme value statistics to sums of correlated variables [2, 3]. We exploit these\n",
      "two facts in order to derive the distribution family of similarity measures. This\n",
      "paper is structured as follows. In Section 2, we overview literature on similarity\n",
      "distances and distance distributions. In Section 3, we discuss the theory of dis-\n",
      "tributions of similarity distances from one to other feature vectors. In Section 4,\n",
      "we validate the resulting distribution experimentally for image feature vectors.\n",
      "Finally, conclusions are given in Section 5.\n",
      "2 2.1\n",
      "Related work Similarity distance measures\n",
      "To measure the similarity between two feature vectors, many distance mea-\n",
      "sures have been proposed [15]. A common metric class of measures is the Lp\n",
      "-norm [1]. The distance from one reference feature vector s to one other feature\n",
      "vector t can be formalized as: n X d(s, t) = ( —si ? ti —p )1/p ,\n",
      "(1)\n",
      "i=1\n",
      "where n and i are the dimensionality and indices of the vectors. Let the\n",
      "random variable Dp represent distances d(s, t) where t is drawn from the random\n",
      "variable T representing feature vectors.\n",
      "Independent of the reference feature\n",
      "vector s, the probability density function of Lp -distances will be denoted by f\n",
      "(Dp = d). 2.2\n",
      "Distance distributions\n",
      "Ferencz et al. [7] have considered the Gamma distribution to model the L2\n",
      "-distances from image 1 d??1 e?d/? , where ? is the shape parameter, regions\n",
      "to one reference region: f (D2 = d) = ? ? ?(?) and ? the scale parameter; ?(?)\n",
      "denotes the Gamma function. In [7], the distance function was ﬁtted eﬃciently\n",
      "from few examples of image regions. Although the distribution ﬁts were shown\n",
      "to represent the region distances to some extent, the method lacks a theoretical\n",
      "motivation. 2\n",
      "\n",
      "========\n",
      "3\n",
      "\n",
      "========\n",
      "Based on the central limit theorem, Pekalska and Duin [23] assumed that\n",
      "Lp -distances between 2 2 1 feature vectors are normally distributed: f (Dp =\n",
      "d) = ?2? e?(d /? )/2 . As the authors argue, ? the use of the central limit\n",
      "theorem is theoretically justiﬁed if the feature values are independent, identi-\n",
      "cally distributed, and have limited variance. Although feature values generally\n",
      "have limited variance, unfortunately, they cannot be assumed to be indepen-\n",
      "dent and/or identically distributed as we will show below. Hence, an alternative\n",
      "derivation of the distance distribution function has to be followed. 2.3\n",
      "Contribution of this paper\n",
      "Our contribution is the theoretical derivation of a parameterized distribution\n",
      "for Lp -norm distances between feature vectors. In the experiments, we establish\n",
      "whether distances to image features adhere to this distribution indeed. We\n",
      "consider SIFT-based features [17], computed from various interest region types\n",
      "[18].\n",
      "3\n",
      "Statistics of distances between features\n",
      "In this section, we derive the distribution function family of Lp -distances\n",
      "from a reference feature vector to other feature vectors. We consider the notation\n",
      "as used in the previous section, with t a feature vector drawn from the random\n",
      "variable T . For each vector t, we consider the value at index i, ti , resulting in\n",
      "a random variable Ti . The value of the reference vector at index i, si , can be\n",
      "interpreted as a sample of the random variable Ti . The computation of distances\n",
      "from one to other vectors involves manipulations of the random variable Ti\n",
      "resulting in a new random variable: Xi = —si ?Ti —p . Furthermore, the\n",
      "computation of the distances D requires the summation of random PI variables,\n",
      "and a reparameterization: D = ( i=1 Xi )1/p . In order to derive the distribution\n",
      "of D, we start with the statistics of the summation of random variables, before\n",
      "turning to the properties of Xi . 3.1\n",
      "Statistics of sums\n",
      "As a starting point to derive the Lp -distance distribution function, we con-\n",
      "sider a lemma from statistics about the sum of random variables. PN Lemma\n",
      "1 For non-identical and correlated random variables Xi , the sum i=1 Xi , with\n",
      "ﬁnite N , is distributed according to the generalized extreme value distribution,\n",
      "i.e. the Gumbel, Frechet or Weibull distribution. For a proof, see [2, 3]. Note\n",
      "that the lemma is an extension of the central limit theorem to nonidentically\n",
      "distributed random variables. And, indeed, the proof follows the path of the\n",
      "central limit theorem. Hence, the resulting distribution of sums is diﬀerent from\n",
      "a normal distribution, and rather one of the Gumbel, Frechet or Weibull distri-\n",
      "butions instead. This lemma is important for our purposes, as later the feature\n",
      "values will turn out to be non-identical and correlated indeed. To conﬁne the\n",
      "distribution function further, we also need the following lemma. Lemma 2 If\n",
      "in the above lemma the random variable Xi are upper-bounded, i.e. Xi ¡ max,\n",
      "the sum of the variables is Weibull distributed (and not Gumbel nor Frechet):\n",
      "y ? ? y (2) f (Y = y) = ( )??1 e?( ? ) , ? ? with ? the shape parameter and\n",
      "? the scale parameter. For a proof, see [10]. Figure 1 illustrates the Weibull\n",
      "distribution for various shape parameters ?. This lemma is equally important\n",
      "\n",
      "========\n",
      "4\n",
      "\n",
      "========\n",
      "to our purpose, as later the feature values will turn out to be upper-bounded\n",
      "indeed. The combination of Lemmas 1 and 2 yields the distribution of sums of\n",
      "non-identical, correlated and upper-bounded random variables, summarized in\n",
      "the following theorem. 3\n",
      "p shape parameter\n",
      "0.8\n",
      "?=2\n",
      "0.6\n",
      "?=4 ?=6 ?=8\n",
      "0.4\n",
      "0.2\n",
      "distance 1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "Figure 1: Examples of the Weibull distribution for various shape parameters\n",
      "\n",
      "========\n",
      "?.\n",
      "\n",
      "========\n",
      "Theorem 1 For non-identical, correlated and upper-bounded random vari-\n",
      "ables Xi , the random PN variable Y = i=1 Xi , with ﬁnite N , adheres to the\n",
      "Weibull distribution. The proof follows trivially from combining the diﬀerent\n",
      "ﬁndings of statistics as laid down in Lemmas 1 and 2. Theorem 1 is the starting\n",
      "point to derive the distribution of Lp -norms from one reference vector to other\n",
      "feature vectors. 3.2\n",
      "Lp -distances from one to other feature vectors\n",
      "Theorem 1 states that Y is Weibull-distributed, given that {Xi = —si ?\n",
      "Ti —p }i?[1,...,I] are nonidentical, correlated and upper-bounded random vari-\n",
      "ables. We transform Y such that it represents Lp -distances, achieved by the\n",
      "transformation (?)1/p : N X Y 1/p = ( —si ? Ti —p )1/p .\n",
      "(3)\n",
      "i=1\n",
      "The consequence of the substitution Z = Y 1/p for the distribution of Y is\n",
      "a change of variables f (z p ) z = y 1/p in Equation 2 [22]: g(Z = z) = (1/p?1)z\n",
      "(1?p) . This transformation yields a diﬀerent distribution still of the Weibull\n",
      "type: g(Z = z) =\n",
      "z z p??1 ?( ?1/p ? 1 )p? ( ) e , (1/p ? 1) ? 1/p ? 1/p\n",
      "(4)\n",
      "where ? ? = p? is the new shape parameter and ? ? = ? 1/p is the\n",
      "new scale parameter, respectively. Thus, also Y 1/p and hence Lp -distances\n",
      "are Weibull-distributed under the assumed case. We argue that the random\n",
      "variables Xi = —si ? Ti —p and Xj (i 6= j) are indeed non-identical, correlated\n",
      "and upper-bounded random variables when considering a set of values extracted\n",
      "from feature vectors at indices i and j: ? Xi and Xj are upper-bounded. Features\n",
      "are usually an abstraction of a particular type of ﬁnite measurements, resulting\n",
      "in a ﬁnite feature. Hence, for general feature vectors, the values at index i, Ti ,\n",
      "are ﬁnite. And, with ﬁnite p, it follows trivially that Xi is ﬁnite. ? Xi and Xj\n",
      "\n",
      "========\n",
      "5\n",
      "\n",
      "========\n",
      "are correlated. The experimental veriﬁcation of this assumption is postponed\n",
      "to Section 4.1. ? Xi and Xj are non-identically distributed. The experimental\n",
      "veriﬁcation of this assumption is postponed to Section 4.1. We have obtained\n",
      "the following result. Corollary 1 For ﬁnite-length feature vectors with non-\n",
      "identical, correlated and upper-bounded values, Lp distances, for limited p,\n",
      "from one reference feature vector to other feature vectors adhere to the Weibull\n",
      "distribution. 4\n",
      "3.3\n",
      "Extending the class of features\n",
      "We extend the class of features for which the distances are Weibull-distributed.\n",
      "From now on, we allow the possibility that the vectors are preprocessed by a\n",
      "PCA transformation. We denote the PCA transform g(?) applied to a single\n",
      "feature vector as s? = g(s). For the random variable Ti , we obtain Ti? . We\n",
      "are still dealing with upper-bounded variables Xi? = —s?i ? Ti? —p as PCA is\n",
      "a ﬁnite transform. The experimental veriﬁcation of the assumption that PCA-\n",
      "transformed feature values Ti? and Tj? , i 6= j are non-identically distributed\n",
      "is postponed to Section 4.1. Our point here, is that we have assumed originally\n",
      "correlating feature values, but after the decorrelating PCA transform we are\n",
      "no longer dealing with correlated feature values Ti? and Tj? . In Section 4.1,\n",
      "we will verify experimentally whether Xi? and Xj? correlate. The following\n",
      "observation is hypothesized. PCA translates the data to the origin, before ap-\n",
      "plying an aﬃne transformation that yields data distributed along orthogonal\n",
      "axes. The tuples (Xi? , Xj? ) will be in the ﬁrst quadrant due to the absolute\n",
      "value transformation. Obviously, variances ?(Xi? ) and ?(Xj? ) are limited\n",
      "and means ?(Xi? ) ¿ 0 and ?(Xj? ) ¿ 0. For data constrained to the ﬁrst quad-\n",
      "rant and distributed along orthogonal axes, a negative covariance is expected\n",
      "to be observed. Under the assumed case, we have obtained the following result.\n",
      "Corollary 2 For ﬁnite-length feature vectors with non-identical, correlated and\n",
      "upper-bounded values, and for PCA-transformations thereof, Lp distances, for\n",
      "limited p, from one to other features adhere to the Weibull distribution. 3.4\n",
      "Heterogeneous feature vector data\n",
      "We extend the corollary to hold also for composite datasets of feature vectors.\n",
      "Consider the composite dataset modelled by random variables {Tt }, where\n",
      "each random variable Tt represents nonidentical and correlated feature values.\n",
      "Hence, from Corollary 2 it follows that feature vectors from each of the Tt can\n",
      "be ﬁtted by a Weibull function f ?,? (d). However, the distances to each of the\n",
      "Tt may have a diﬀerent range and modus, as we will verify by experimentation\n",
      "in Section 4.1. For heterogeneous distance data {Tt }, we obtain a mixture of\n",
      "Weibull functions [14]. Corollary 3 (Distance distribution) For feature vectors\n",
      "that are drawn from a mixture of datasets, of which each results in non-identical\n",
      "and correlated feature values, ﬁnite-length feature vectors with non-identical,\n",
      "correlated and upper-bounded values, and for PCA-transformations thereof, Lp\n",
      "distances, for limited p, from one reference feature vector to other feature vectors\n",
      "adhere to the Pc Weibull mixture distribution: f (D = d) = Pi=1 ?i ? ﬁ?i ,?i\n",
      "(d), where ﬁ are the Weibull functions c and ?i are their respective weights such\n",
      "that i=1 ?i = 1.\n",
      "\n",
      "========\n",
      "6\n",
      "\n",
      "========\n",
      "4\n",
      "Experiments\n",
      "In our experiments, we validate assumptions and Weibull goodness-of-ﬁt for\n",
      "the region-based SIFT, GLOH, SPIN, and PCA-SIFT features on COREL data\n",
      "[5]. We include these features for two reasons as: a) they are performing well\n",
      "for realistic computer vision tasks and b) they provide diﬀerent mechanisms to\n",
      "describe an image region [17]. The region features are computed from regions\n",
      "detected by the Harris- and Hessian-aﬃne regions, maximally stable regions\n",
      "(MSER), and intensity extrema-based regions (IBR) [18]. Also, we consider\n",
      "PCA-transformed versions for each of the detector/feature combinations. For\n",
      "reason of its extensive use, the experimentation is based on the L2 -distance.\n",
      "We consider distances from 1 randomly drawn reference vector to 100 other\n",
      "randomly drawn feature vectors, which we repeat 1,000 times for generalization.\n",
      "In all experiments, the features are taken from multiple images, except for the\n",
      "illustration in Section 4.1.2 to show typical distributions of distances between\n",
      "features taken from single images. 4.1 4.1.1\n",
      "Validation of the corollary assumptions for image features Intrinsic feature\n",
      "assumptions\n",
      "Corollary 2 rests on a few explicit assumptions. Here we will verify whether\n",
      "the assumptions occur in practice. 5\n",
      "Diﬀerences between feature values are correlated. We consider a set of fea-\n",
      "ture vectors Tj and the diﬀerences at index i to a reference vector s: Xi = —si ?\n",
      "Tji —p . We determine the signiﬁcance of Pearson?s correlation [4] between the\n",
      "diﬀerence values Xi and Xj , i 6= j. We establish the percentage of signiﬁcantly\n",
      "correlating diﬀerences at a conﬁdence level of 0.05. We report for each feature\n",
      "the average percentage of diﬀerence values that correlate signiﬁcantly with dif-\n",
      "ference values at an other feature vector index. As expected, the feature value\n",
      "diﬀerences correlate. For SIFT, 99% of the diﬀerence values are signiﬁcantly\n",
      "correlated. For SPIN and GLOH, we obtain 98% and 96%, respectively. Also\n",
      "PCASIFT contains signiﬁcantly correlating diﬀerence values: 95%. Although\n",
      "the feature?s name hints at uncorrelated values, it does not achieve a decorrela-\n",
      "tion of the values in practice. For each of the features, a low standard deviation\n",
      "¡ 5% is found. This expresses the low variation of correlations across the ran-\n",
      "dom samplings and across the various region types. We repeat the experiment\n",
      "for PCA-transformed feature values. Although the resulting values are uncor-\n",
      "related by construction, their diﬀerences are signiﬁcantly correlated. For SIFT,\n",
      "SPIN, GLOH, and PCA-SIFT, the percentages of signiﬁcantly correlating dif-\n",
      "ference values are: 94%, 86%, 95%, and 75%, respectively. Diﬀerences between\n",
      "feature values are non-identically distributed. We repeat the same procedure\n",
      "as above, but instead of measuring the signiﬁcance of correlation, we estab-\n",
      "lish the percentage of signiﬁcantly diﬀerently distributed diﬀerence values Xi by\n",
      "the Wilcoxon rank sum test [4] at a conﬁdence level of 0.05. For SIFT, SPIN,\n",
      "GLOH, and PCA-SIFT, the percentages of signiﬁcantly diﬀerently distributed\n",
      "diﬀerence values are: 99%, 98%, 92%, and 87%. For the PCA-transformed ver-\n",
      "sions of SIFT, SPIN, GLOH, and PCA-SIFT, we ﬁnd: 62%, 40%, 64%, and\n",
      "51%, respectively. Note that in all cases, correlation is suﬃcient to fulﬁll the as-\n",
      "\n",
      "========\n",
      "7\n",
      "\n",
      "========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sumptions of Corollary 2. We have illustrated that feature value diﬀerences are\n",
      "signiﬁcantly correlated and signiﬁcantly non-identically distributed. We con-\n",
      "clude that the assumptions of Corollary 2 about properties of feature vectors\n",
      "are realistic in practice, and that Weibull functions are expected to ﬁt distance\n",
      "distributions well. 4.1.2\n",
      "Inter-feature assumptions\n",
      "In Corollary 3, we have assumed that distances from one to other feature\n",
      "vectors are described well by a mixture of Weibulls, if the features are taken from\n",
      "diﬀerent clusters in the data. Here, we illustrate that clusters of feature vectors,\n",
      "and clusters of distances, occur in practice. Figure 2a shows Harris-aﬃne regions\n",
      "from a natural scene which are described by the SIFT feature. The distances\n",
      "are described well by a single Weibull distribution. The same hold for distances\n",
      "from one to other regions computed from a man-made ob ject, see Figure 2b. In\n",
      "Figure 2c, we illustrate the distances of one to other regions computed from a\n",
      "composite image containing two types of regions. This results in two modalitites\n",
      "of feature vectors hence of similarity distances. The distance distribution is\n",
      "therefore bimodal, illustrating the general case of multimodality to be expected\n",
      "in realistic, heterogeneous image data. We conclude that the assumptions of\n",
      "Corollary 3 are realistic in practice, and that the Weibull function, or a mixture,\n",
      "ﬁts distance distributions well. 4.2\n",
      "Validation of Weibull-shaped distance distributions\n",
      "In this experiment, we validate the ﬁtting of Weibull distributions of dis-\n",
      "tances from one reference feature vector to other vectors. We consider the same\n",
      "data as before. Over 1,000 repetitions we consider the goodness-of-ﬁt of L2\n",
      "-distances by the Weibull distribution. The parameters of the Weibull distribu-\n",
      "tion function are obtained by maximum likelihood estimation. The established\n",
      "ﬁt is assessed by the Anderson-Darling test at a conﬁdence level of ? = 0.05\n",
      "[20]. The Anderson-Darling test has also proven to be suited to measure the\n",
      "goodness-of-ﬁt of mixture distributions [19]. Table 1 indicates that for most of\n",
      "the feature types computed from various regions, more than 90% of the distance\n",
      "distributions is ﬁt by a single Weibull function. As expected, distances between\n",
      "each of the SPIN, SIFT, PCA-SIFT and GLOH features, are ﬁtted well by\n",
      "Weibull distributions. The exception here is the low number of ﬁts for the SIFT\n",
      "and SPIN features computed from Hessianaﬃne regions. The distributions of\n",
      "distances between these two region/feature combinations tend to have multiple\n",
      "modes. Likewise, there is a low percentage of ﬁts of L2 -distance distributions\n",
      "of the 6\n",
      "0.014\n",
      "0.014\n",
      "0.012\n",
      "0.012\n",
      "0.012\n",
      "0.01\n",
      "0.01\n",
      "0.008\n",
      "0.006\n",
      "\n",
      "========\n",
      "8\n",
      "\n",
      "========\n",
      "0.008\n",
      "0.006\n",
      "0.004\n",
      "0.004\n",
      "0.002\n",
      "0.002\n",
      "0 250\n",
      "300\n",
      "350\n",
      "400\n",
      "450 500 distances\n",
      "(a)\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "0.01\n",
      "probability\n",
      "probability\n",
      "probability\n",
      "0.014\n",
      "0 250\n",
      "0.008\n",
      "0.006\n",
      "0.004\n",
      "0.002\n",
      "300\n",
      "350\n",
      "400\n",
      "450 500 distances\n",
      "(b)\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "0 250\n",
      "300\n",
      "350\n",
      "400\n",
      "450 500 distances\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "(c)\n",
      "\n",
      "========\n",
      "9\n",
      "\n",
      "========\n",
      "Figure 2: Distance distributions from one randomly selected image region to\n",
      "other regions, each described by the SIFT feature. The distance distribution is\n",
      "described by a single Weibull function for a natural scene (a) and a man-made\n",
      "ob ject (b). For a composite image, the distance distribution is bimodal (c).\n",
      "Samples from each of the distributions are shown in the upper images. Table 1:\n",
      "Accepted Weibull ﬁts for COREL data [5]. Harris-aﬃne Hessian-aﬃne MSER\n",
      "IBR c=1 c?2 c=1 c?2 c=1 c?2 c=1 c?2 SIFT 95% 100% 60% 99% 98% 100%\n",
      "92% 100% SIFT (g =PCA) 95% 99% 60% 98% 98% 100% 92% 99% PCA-SIFT\n",
      "89% 100% 96% 100% 94% 100% 95% 100% PCA-SIFT (g =PCA) 89% 100%\n",
      "96% 100% 94% 100% 95% 100% SPIN 71% 99% 12% 99% 77% 99% 45% 98%\n",
      "SPIN (g =PCA) 71% 100% 12% 97% 77% 99% 45% 98% GLOH 87% 100% 91%\n",
      "100% 82% 99% 86% 100% GLOH (g =PCA) 87% 100% 91% 99% 82% 99% 86%\n",
      "100% Percentages of L2 -distance distributions ﬁtted by a Weibull function (c\n",
      "= 1) and a mixture of two Weibull functions (c ? 2) are given.\n",
      "SPIN feature computed from IBR regions. Again, multiple modes in the\n",
      "distributions are observed. For these distributions, a mixture of two Weibull\n",
      "functions provides a good ﬁt (? 97%).\n",
      "5\n",
      "Conclusion\n",
      "In this paper, we have derived that similarity distances between one and\n",
      "other image features in databases are Weibull distributed. Indeed, for various\n",
      "types of features, i.e. the SPIN, SIFT, GLOH and PCA-SIFT features, and for\n",
      "a large variety of images from the COREL image collection, we have demon-\n",
      "strated that the similarity distances from one to other features, computed from\n",
      "Lp norms, are Weibull-distributed. These results are established by the exper-\n",
      "iments presented in Table 1. Also, between PCA-transformed feature vectors,\n",
      "the distances are Weibull-distributed. The Malahanobis distance is very similar\n",
      "to the L2 -norm computed in the PCA-transformed feature space. Hence, we\n",
      "expect Mahalanobis distances to be Weibull distributed as well. Furthermore,\n",
      "when the dataset is a composition, a mixture of few (typically two) Weibull\n",
      "functions suﬃces, as established by the experiments presented in Table 1. The\n",
      "resulting Weibull distributions are distinctively diﬀerent from the distributions\n",
      "suggested in literature, as they are positively or negatively skewed while the\n",
      "Gamma [7] and normal [23] distributions are positively and non-skewed, respec-\n",
      "tively. We have demonstrated that the Weibull distribution is the preferred\n",
      "choice for estimating properties of similarity distances. The assumptions un-\n",
      "der which the theory is valid are realistic for images. We experimentally have\n",
      "shown them to hold for various popular feature extraction algorithms, and for a\n",
      "diverse range of images. This fundamental insight opens new directions in the\n",
      "assessment of feature similarity, with pro jected improvements and speed-ups in\n",
      "ob ject/scene recognition algorithms. 7\n",
      "Acknowledgments This work is partly sponsored by the EU funded NEST\n",
      "pro ject PERCEPT, by the Dutch BSIK pro ject Multimedian, and by the EU\n",
      "Network of Excellence MUSCLE.\n",
      "\n",
      "========\n",
      "10\n",
      "\n",
      "========\n",
      "2 References\n",
      "\n",
      "========\n",
      "[1] B. G. Batchelor. Pattern Recognition: Ideas in Practice. Plenum Press,\n",
      "New York, 1995.\n",
      "[2] E. Bertin. Global ﬂuctuations and Gumbel statistics.\n",
      "Physical Review Letters, 95(170601):1?4, 2005.\n",
      "[3] E. Bertin and M. Clusel.\n",
      "Generalised extreme value statistics and sum of correlated variables. Journal of\n",
      "Physics A, 39:7607, 2006.\n",
      "[4] W. J. Conover. Practical nonparametric statis-\n",
      "tics. Wiley, New York, 1971.\n",
      "[5] Corel Gallery. www.corel.com.\n",
      "[6] L. Fei-Fei\n",
      "and P. Perona. A bayesian hierarchical model for learning natural scene cat-\n",
      "egories.\n",
      "In CVPR, 2005.\n",
      "[7] A. Ferencz, E.G. Learned-Miller, and J. Malik.\n",
      "Building a classiﬁcation cascade for visual identiﬁcation from one example. In\n",
      "Proceedings of the International Conference Computer Vision, pages 286?293.\n",
      "IEEE Computer Society, 2003. [8] R. Fergus, P. Perona, and A. Zisserman. A\n",
      "sparse ob ject category model for eﬃcient learning and exhaustive recognition.\n",
      "In Proceedings of the Computer Vision and Pattern Recognition. IEEE, 2005.\n",
      "[9] J. M. Geusebroek, R. van den Boomgaard, A. W. M. Smeulders, and H.\n",
      "Geerts. Color invariance. IEEE Transactions on Pattern Analysis and Machine\n",
      "Intelligence, 23(12):1338?1350, 2001. [10] E. J. Gumbel. Statistics of Extremes.\n",
      "Columbia University Press, New York, 1958.\n",
      "[11] C. Harris and M. Stephans.\n",
      "A combined corner and edge detector.\n",
      "In Proceedings of the 4th Alvey Vi-\n",
      "sion Conference, pages 189?192, Manchester, 1988. [12] F. Jurie and B. Triggs.\n",
      "Creating eﬃcient codebooks for visual recognition. In ICCV, pages 604?610,\n",
      "2005.\n",
      "[13] D. G. Lowe. Distinctive image features from scale-invariant key-\n",
      "points. International Journal of Computer Vision, 60(2):91?110, 2004.\n",
      "[14] J.\n",
      "M. Marin, M. T. Rodriquez-Bernal, and M. P. Wiper. Using weibull mixture dis-\n",
      "tributions to model heterogeneous survival data. Communications in statistics,\n",
      "34(3):673?684, 2005. [15] R. S. Michalski, R. E. Stepp, and E. Diday. A recent\n",
      "advance in data analysis: Clustering ob jects into classes characterized by con-\n",
      "junctive concepts. In L. N. Kanal and A. Rosenfeld, editors, Progress in Pattern\n",
      "Recognition, pages 33?56. North-Holland Publishing Co., Amsterdam, 1981.\n",
      "[16] K. Mikola jczyk, B. Leibe, and B. Schiele. Multiple ob ject class detection\n",
      "with a generative model. In CVPR, 2006. [17] K. Mikola jczyk and C. Schmid.\n",
      "A performance evaluation of local descriptors. IEEE Transactions on Pattern\n",
      "Analysis and Machine Intelligence, 27(10):1615?1630, 2005. [18] K. Mikola jczyk,\n",
      "T. Tuytelaars, C. Schmid, A. Zisserman, J. Matas, F. Schaﬀalitzky, T. Kadir,\n",
      "and L. Van Gool. A comparison of aﬃne region detectors. International Jour-\n",
      "nal of Computer Vision, 65(1/2):43?72, 2005. [19] K. Mosler. Mixture models\n",
      "in econometric duration analysis. Applied Stochastic Models in Business and\n",
      "Industry, 19(2):91?104, 2003. [20] NIST/SEMATECH. e-Handbook of Statisti-\n",
      "cal Methods. NIST, http://www.itl.nist.gov/div898/handbook/, 2006.\n",
      "[21] E.\n",
      "Nowak and F. Jurie. Learning visual similarity measures for comparing never\n",
      "seen ob jects.\n",
      "In CVPR, 2007.\n",
      "[22] A. Papoulis and S. U. Pillai. Probabil-\n",
      "ity, Random Variables and Stochastic Processes. McGraw-Hill, New York, 4\n",
      "edition, 2002. [23] E. Pekalska and R. P. W. Duin. Classiﬁers for dissimilarity-\n",
      "based pattern recognition. In Proceedings of the International Conference on\n",
      "Pattern Recognition, volume 2, page 2012, 2000. [24] C. Schmid and R. Mohr.\n",
      "\n",
      "========\n",
      "11\n",
      "\n",
      "========\n",
      "Local grayvalue invariants for image retrieval. IEEE Transactions on Pattern\n",
      "Analysis and Machine Intelligence, 19(5):530?535, 1997. [25] J.C. van Gemert,\n",
      "J.M. Geusebroek, C.J. Veenman, C.G.M. Snoek, and Arnold W.M. Smeulders.\n",
      "Robust scene categorization by learning image statistics in context. In CVPR\n",
      "Workshop on Semantic Learning Applications in Multimedia (SLAM), 2006.\n",
      "8\n",
      "\n",
      "========\n",
      "12\n",
      "\n",
      "========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted Q-iteration in continuous action-space\n",
      "MDPs\n",
      "\n",
      "========\n",
      "Authored by:\n",
      "\n",
      "========\n",
      "R?mi Munos\n",
      "Csaba Szepesv?ri\n",
      "Andr?s Antos\n",
      "\n",
      "========\n",
      "Abstract\n",
      "\n",
      "========\n",
      "We consider continuous state, continuous action batch reinforcement\n",
      "learning where the goal is to learn a good policy from a suﬃciently rich\n",
      "tra jectory generated by another policy. We study a variant of ﬁtted Q-\n",
      "iteration, where the greedy action selection is replaced by searching for a\n",
      "policy in a restricted set of candidate policies by maximizing the average\n",
      "action values. We provide a rigorous theoretical analysis of this algorithm,\n",
      "proving what we believe is the ﬁrst ﬁnite-time bounds for value-function\n",
      "based algorithms for continuous state- and action-space problems.\n",
      "\n",
      "========\n",
      "1 Paper Body\n",
      "\n",
      "========\n",
      "We consider continuous state, continuous action batch reinforcement learning\n",
      "where the goal is to learn a good policy from a suﬃciently rich tra jectory gener-\n",
      "ated by some policy. We study a variant of ﬁtted Q-iteration, where the greedy\n",
      "action selection is replaced by searching for a policy in a restricted set of can-\n",
      "didate policies by maximizing the average action values. We provide a rigorous\n",
      "analysis of this algorithm, proving what we believe is the ﬁrst ﬁnite-time bound\n",
      "for value-function based algorithms for continuous state and action problems.\n",
      "1\n",
      "Preliminaries\n",
      "We will build on the results from [1, 2, 3] and for this reason we use the\n",
      "same notation as these papers. The unattributed results cited in this section\n",
      "can be found in the book [4]. A discounted MDP is deﬁned by a quintuple\n",
      "(X , A, P, S, ?), where X is the (possible inﬁnite) state space, A is the set of\n",
      "actions, P : X ? A ? M (X ) is the transition probability kernel with P (?—x, a)\n",
      "deﬁning the next-state distribution upon taking action a from state x, S(?—x,\n",
      "a) gives the corresponding distribution of immediate rewards, and ? ? (0, 1)\n",
      "is the discount factor. Here X is a measurable space and M (X ) denotes the\n",
      "set of all probability measures over X . The Lebesguemeasure shall be denoted\n",
      "\n",
      "========\n",
      "1\n",
      "\n",
      "========\n",
      "by ?. We start with the following mild assumption on the MDP: Assumption\n",
      "A1 (MDP Regularity) X is a compact subset of the dX -dimensional Euclidean\n",
      "space, ? max A is a compact subset of [?A? , A? ]dA . The random immediate\n",
      "rewards are bounded by R R and that the expected immediate reward function,\n",
      "r(x, a) = rS(dr—x, a), is uniformly bounded by Rmax : krk? ? Rmax . A\n",
      "policy determines the next action given the past observations. Here we shall\n",
      "deal with stationary (Markovian) policies which choose an action in a stochastic\n",
      "way based on the last observation only. The value of a policy ? when it is started\n",
      "from a state x is deﬁned as the expected discounted Ptotal ? reward that is\n",
      "encountered while the policy is executed: V ? (x) = E? [ t=0 ? t Rt —X0 = x].\n",
      "Here Rt ? S(?—Xt , At ) is the reward received at time step t, the state, Xt ,\n",
      "evolves according to Xt+1 ? ? Also with: Computer and Automation Research\n",
      "Inst. of the Hungarian Academy of Sciences Kende u. 13-17, Budapest 1111,\n",
      "Hungary.\n",
      "1\n",
      "P (?—Xt , At ), where At is sampled from the distribution determined by\n",
      "?. We use Q? : X ? A ? R P? to denote the action-value function of policy ?:\n",
      "Q? (x, a) = E? [ t=0 ? t Rt —X0 = x, A0 = a]. The goal is to ﬁnd a policy\n",
      "that attains the best possible values, V ? (x) = sup? V ? (x), at all states ?\n",
      "x ? X . Here V ? is called the optimal value function and a policy ? ? that\n",
      "satisﬁes V ? (x) = ? ? ? V (x) for all x ? X is called optimal. The optimal\n",
      "action-value function Q (x, a) is Q (x, a) = sup? Q? (x, a). We say that a\n",
      "(deterministic stationary) policy ? is greedy w.r.t. an action-value function Q\n",
      "? B(X ? A), and we write ? = ? ? (?; Q), if, for all x ? X , ?(x) ? argmaxa?A\n",
      "Q(x, a). Under mild technical assumptions, such a greedy policy always exists.\n",
      "Any greedy policy w.r.t. Q? ? is optimal. For ? : X ? A we ? A), by R deﬁne\n",
      "its evaluation operator, T : B(X ?? A) ?? B(X ? (T Q)(x, a) = r(x, a) + ? X\n",
      "Q(y, ?(y)) P (dy—x, a). It is known that Q = T Q? . Further, if weR let the\n",
      "Bellman operator, T : B(X ? A) ? B(X ? A), deﬁned by (T Q)(x, a) = r(x, a) +\n",
      "? X supb?A Q(y, b) P (dy—x, a) then Q? = T Q? . It is known that V ? and\n",
      "Q? are bounded by Rmax /(1 ? ?), just like Q? and V ? . For ? : X ? A, the\n",
      "operator E ? : B(X ? A) ? B(X ) is deﬁned by (E ? Q)(x) = Q(x, ?(x)), while E\n",
      ": B(X ? A) ? B(X ) is deﬁned by (EQ)(x) = supa?A Q(x, a). Throughout the\n",
      "paper F ? {f : X ? A ? R} will denote a subset of real-valued functions over the\n",
      "state-action space X ? A and ? ? AX will R be a set of policies. For ? ? M (X\n",
      ") and f : X ? R p measurable, we let (for p ? 1) kf kp,? = X —f (x)—p ?(dx).\n",
      "We simply write kf k? for kf k2,? . R R 2 Further, we extend k?k? to F by kf\n",
      "k? = A X —f —2 (x, a) d?(x) d?A (a), where ?RA is the uniform distribution\n",
      "over A. We shall use the shorthand notation ?f to denote the integral f (x)?(dx).\n",
      "We denote the space of bounded measurable functions with domain X by B(X\n",
      "). Further, the space of measurable functions bounded by 0 ¡ K ¡ ? shall be\n",
      "denoted by B(X ; K). We let k?k? denote the supremum norm.\n",
      "2\n",
      "Fitted Q-iteration with approximate policy maximization\n",
      "We assume that we are given a ﬁnite tra jectory, {(Xt , At , Rt )}1?t?N ,\n",
      "generated by some stochastic stationary policy ?b , called the behavior policy:\n",
      "\n",
      "========\n",
      "2\n",
      "\n",
      "========\n",
      "At ? ?b (?—Xt ), Xt+1 ? P (?—Xt , At ), Rt ? def S(?—Xt , At ), where ?b\n",
      "(?—x) is a density with ?0 = inf (x,a)?X ?A ?b (a—x) ¿ 0. The generic recipe\n",
      "for ﬁtted Q-iteration (FQI) [5] is Qk+1 = Regress(Dk (Qk )),\n",
      "(1)\n",
      "where Regress is an appropriate regression procedure and Dk (Qk ) is a\n",
      "dataset deﬁning a regression problem in the form of a list of data-point pairs:\n",
      "?h ? i Dk (Qk ) = (Xt , At ), Rt + ? max Qk (Xt+1 , b) .1 b?A\n",
      "1?t?N\n",
      "Fitted Q-iteration can be viewed as approximate value iteration applied to\n",
      "action-value functions. To see this note that value iteration would assign the\n",
      "value (T Qk )(x, a) = r(x, a) + R ? maxb?A Qk (y, b) P (dy—x, a) to Qk+1 (x,\n",
      "a) [6]. Now, remember that the regression function for the jointly distributed\n",
      "random variables (Z, Y ) is deﬁned by the conditional expectation of Y given Z:\n",
      "m(Z) = E [Y —Z]. Since for any ﬁxed function Q, E [Rt + ? maxb?A Q(Xt+1\n",
      ", b)—Xt , At ] = (T Q)(Xt , At ), the regression function corresponding to\n",
      "the data Dk (Q) is indeed T Q and hence if FQI solved the regression problem\n",
      "deﬁned by Qk exactly, it would simulate value iteration exactly. However, this\n",
      "argument itself does not directly lead to a rigorous analysis of FQI: Since Qk\n",
      "is obtained based on the data, it is itself a random function. Hence, after the\n",
      "ﬁrst iteration, the ?target? function in FQI becomes random. Furthermore, this\n",
      "function depends on the same data that is used to deﬁne the regression problem.\n",
      "Will FQI still work despite these issues? To illustrate the potential diﬃculties\n",
      "consider a dataset where X1 , . . . , XN is a sequence of independent random\n",
      "variables, which are all distributed uniformly at random in [0, 1]. Further, let\n",
      "M be a random integer greater than N which is independent of the dataset (Xt\n",
      ")N t=1 . Let U be another random variable, uniformly distributed in [0, 1].\n",
      "Now deﬁne the regression problem by Yt = fM,U (Xt ), where fM,U (x) =\n",
      "sgn(sin(2M 2?(x + U ))). Then it is not hard to see that no matter how big N\n",
      "is, no procedure can 1\n",
      "Since the designer controls Qk , we may assume that it is continuous, hence\n",
      "the maximum exists.\n",
      "2\n",
      "estimate the regression function fM,U with a small error (in expectation, or\n",
      "with high probability), even if the procedure could exploit the knowledge of the\n",
      "speciﬁc form of fM,U . On the other hand, if we restricted M to a ﬁnite range\n",
      "then the estimation problem could be solved successfully. The example shows\n",
      "that if the complexity of the random functions deﬁning the regression problem\n",
      "is uncontrolled then successful estimation might be impossible. Amongst the\n",
      "many regression methods in this paper we have chosen to work with least-\n",
      "squares methods.\n",
      "In this case Equation (1) takes the form ? ? ??2 N X 1\n",
      "Q(Xt , At ) ? Rt + ? max Qk (Xt+1 , b) Qk+1 = argmin . (2) b?A Q?F\n",
      "t=1 ?b (At —Xt ) We call this method the least-squares ﬁtted Q-iteration\n",
      "(LSFQI) method. Here we introduced the weighting 1/?b (At —Xt ) since\n",
      "we do not want to give more weight to those actions that are preferred by\n",
      "the behavior policy. Besides this weighting, the only parameter of the method\n",
      "is the function set F. This function set should be chosen carefully, to keep a\n",
      "\n",
      "========\n",
      "3\n",
      "\n",
      "========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "balance between the representation power and the number of samples. As a\n",
      "speciﬁc example for F consider neural networks with some ﬁxed architecture.\n",
      "In this case the function set is generated by assigning weights in all possible\n",
      "ways to the neural net. Then the above minimization becomes the problem of\n",
      "tuning the weights. Another example is to use linearly parameterized function\n",
      "approximation methods with appropriately selected basis functions. In this case\n",
      "the weight tuning problem would be less demanding. Yet another possibility is\n",
      "to let F be an appropriate restriction of a Reproducing Kernel Hilbert Space\n",
      "(e.g., in a ball). In this case the training procedure becomes similar to LS-SVM\n",
      "training [7]. As indicated above, the analysis of this algorithm is complicated\n",
      "by the fact that the new dataset is deﬁned in terms of the previous iterate,\n",
      "which is already a function of the dataset. Another complication is that the\n",
      "samples in a tra jectory are in general correlated and that the bias introduced by\n",
      "the imperfections of the approximation architecture may yield to an explosion\n",
      "of the error of the procedure, as documented in a number of cases in, e.g.,\n",
      "[8]. Nevertheless, at least for ﬁnite action sets, the tools developed in [1, 3, 2]\n",
      "look suitable to show that under appropriate conditions these problems can be\n",
      "overcome if the function set is chosen in a judicious way. However, the results of\n",
      "these works would become essentially useless in the case of an inﬁnite number of\n",
      "actions since these previous bounds grow to inﬁnity with the number of actions.\n",
      "Actually, we believe that this is not an artifact of the proof techniques of these\n",
      "works, as suggested by the counterexample that involved random targets. The\n",
      "following result elaborates this point further: Proposition 2.1. Let F ? B(X ?\n",
      "A). Then even if the pseudo-dimension of F is ﬁnite, the fatshattering function\n",
      "of ? ? ? Fmax = VQ : VQ (?) = max Q(?, a), Q ? F a?A\n",
      "2\n",
      "can be inﬁnite over (0, 1/2).\n",
      "Without going into further details, let us just note that the ﬁniteness of the\n",
      "fat-shattering function is a suﬃcient and necessary condition for learnability and\n",
      "the ﬁniteness of the fat-shattering function is implied by the ﬁniteness of the\n",
      "pseudo-dimension [9].The above proposition thus shows that without imposing\n",
      "further special conditions on F, the learning problem may become infeasible.\n",
      "One possibility is of course to discretize the action space, e.g., by using a uniform\n",
      "grid. However, if the action space has a really high dimensionality, this approach\n",
      "becomes unfeasible (even enumerating 2dA points could be impossible when dA\n",
      "is large). Therefore we prefer alternate solutions. Another possibility is to make\n",
      "the functions in F, e.g., uniformly Lipschitz in their state coordinates. ? Then\n",
      "the same property will hold for functions in Fmax and hence by a classical result\n",
      "we can bound the capacity of this set (cf. pp. 353?357 of [10]). One potential\n",
      "problem with this approach is that this way it might be diﬃcult to get a ﬁne\n",
      "control of the capacity of the resulting set. 2 The proof of this and the other\n",
      "results are given in the appendix, available in the extended version of this paper,\n",
      "downloadable from http://hal.inria.fr/inria-00185311/en/.\n",
      "3\n",
      "In the approach explored here we modify the ﬁtted Q-iteration algorithm\n",
      "by introducing a policy set ? and a search over this set for an approximately\n",
      "\n",
      "========\n",
      "4\n",
      "\n",
      "========\n",
      "greedy policy in a sense that will be made precise in a minute. Our algorithm\n",
      "thus has four parameters: F, ?, K, Q0 . Here F is as before, ? is a user-chosen\n",
      "set of policies (mappings from X to A), K is the number of iterations and Q0 is\n",
      "an initial value function (a typical choice is Q0 ? 0). The algorithm computes\n",
      "a sequence of iterates (Qk , ? ?k ), k = 0, .\n",
      ".\n",
      ".\n",
      ", K, deﬁned by the following\n",
      "equations: ? ?0 Qk+1 ? ?k+1\n",
      "=\n",
      "argmax ???\n",
      "=\n",
      "argmin\n",
      "N X\n",
      "argmax ???\n",
      "Q0 (Xt , ?(Xt )),\n",
      "t=1\n",
      "Q?F\n",
      "=\n",
      "N X\n",
      "t=1 N X\n",
      "? ? ??2 1 Q(Xt , At ) ? Rt + ?Qk (Xt+1 , ? ?k (Xt+1 )) , ?b (At —Xt )\n",
      "(3)\n",
      "Qk+1 (Xt , ?(Xt )).\n",
      "(4)\n",
      "t=1\n",
      "Thus, (3) is similar to (2), while (4) deﬁnes the policy search problem. The\n",
      "policy search will generally be solved by a gradient procedure or some other\n",
      "appropriate method. The cost of this step will be primarily determined by how\n",
      "well-behaving the iterates Qk+1 are in their action arguments. For example, if\n",
      "they were quadratic and if ? was linear then the problem would be a quadratic\n",
      "optimization problem. However, except for special cases3 the action value func-\n",
      "tions will be more complicated, in which case this step can be expensive. Still,\n",
      "this cost could be similar to that of searching for the maximizing actions for\n",
      "each t = 1, . . . , N if the approximately maximizing actions are similar across\n",
      "similar states. This algorithm, which we could also call a ﬁtted actor-critic\n",
      "algorithm, will be shown to overcome the above mentioned complexity control\n",
      "problem provided that the complexity of ? is controlled appropriately. Indeed,\n",
      "in this case the set of possible regression problems is determined by the set F??\n",
      "= { V : V (?) = Q(?, ?(?)), Q ? F, ? ? ? } , and the proof will rely on\n",
      "controlling the complexity of F?? by selecting F and ? appropriately.\n",
      "3 3.1\n",
      "The main theoretical result Outline of the analysis\n",
      "In order to gain some insight into the behavior of the algorithm, we provide\n",
      "a brief summary of its error analysis. The main result will be presented sub-\n",
      "sequently. For f ,Q ? F and a policy ?, we deﬁne the tth TD-error as follows:\n",
      "dt (f ; Q, ?) = Rt + ?Q(Xt+1 , ?(Xt+1 )) ? f (Xt , At ). Further, we deﬁne\n",
      "the empirical loss function by N X d2t (f ; Q, ?) ? N (f ; Q, ?) = 1 , L N t=1\n",
      "?(A)?b (At —Xt )\n",
      "\n",
      "========\n",
      "5\n",
      "\n",
      "========\n",
      "where the normalization with ?(A) is introduced for mathematical conve-\n",
      "nience. Then (3) can be ? N (f ; Qk , ? written compactly as Qk+1 = argminf\n",
      "?F L ?k ). ? N (f ; Q, ?) is an The algorithm can then be motivated by the\n",
      "observation that for any f ,Q, and ?, L unbiased estimate of def 2 L(f ; Q, ?) = kf\n",
      "? T ? Qk? + L? (Q, ?), (5) where the ﬁrst term is the error we are interested in\n",
      "and the second term captures the variance of the random samples: Z L? (Q, ?)\n",
      "= E [Var [R1 + ?Q(X2 , ?(X2 ))—X1 , A1 = a]] d?A (a). A 3 Linear quadratic\n",
      "regulation is such a nice case. It is interesting to note that in this special case\n",
      "the obvious choices for F and ? yield zero error in the limit, as can be proven\n",
      "based on the main result of this paper.\n",
      "4\n",
      "h i ? N (f ; Q, ?) = L(f ; Q, ?). This result is stated formally by E L Since the\n",
      "variance term in (5) is independent of f , argminf ?F L(f ; Q, ?) = 2 ? argminf\n",
      "?F kf ? T Qk? . Thus, if ? ?k were greedy w.r.t. Qk then argminf ?F L(f ; Qk , ?\n",
      "?k ) = 2 argminf ?F kf ? T Qk k? . Hence we can still think of the procedure as\n",
      "approximate value iteration over the space of action-value functions, pro jecting\n",
      "T Qk using empirical risk minimization on the space F w.r.t. k?k? distances\n",
      "in an approximate manner. Since ? ?k is only approximately greedy, we will\n",
      "have to deal with both the error coming from the approximate pro jection and\n",
      "the error coming from the choice of ? ?k . To make this clear, we write the\n",
      "iteration in the form Qk+1 = T ??k Qk + ?0k = T Qk + ?0k + (T ??k Qk ?\n",
      "T Qk ) = T Qk + ?k , def\n",
      "where ?0k is the error committed while computing T ??k Qk , ?00k = T\n",
      "??k Qk ? T Qk is the error committed because the greedy policy is computed\n",
      "approximately and ?k = ?0k + ?00k is the total error of step k. Hence, in order\n",
      "to show that the procedure is well behaved, one needs to show that both errors\n",
      "are controlled and that when the errors are propagated through these equations,\n",
      "the resulting error stays controlled, too. Since we are ultimately interested in\n",
      "the performance of the policy obtained, we will also need to show that small\n",
      "action-value approximation errors yield small performance losses. For these we\n",
      "need a number of assumptions that concern either the training data, the MDP,\n",
      "or the function sets used for learning. 3.2\n",
      "Assumptions\n",
      "3.2.1\n",
      "Assumptions on the training data\n",
      "We shall assume that the data is rich, is in a steady state, and is fast-\n",
      "mixing, where, informally, mixing means that future depends weakly on the\n",
      "past. Assumption A2 (Sample Path Properties) Assume that {(Xt , At , Rt\n",
      ")}t=1,...,N is the sample path of ?b , a stochastic stationary policy. Further,\n",
      "assume that {Xt } is strictly stationary (Xt ? ? ? M (X )) and exponentially\n",
      "?-mixing with the actual rate given by the parameters (?, b, ?).4 We further\n",
      "assume that the sampling policy ?b satisﬁes ?0 = inf (x,a)?X ?A ?b (a—x) ¿\n",
      "0. The ?-mixing property will be used to establish tail inequalities for certain\n",
      "empirical processes.5 Note that the mixing coeﬃcients do not need to be known.\n",
      "In the case when no mixing condition is satisﬁed, learning might be impossible.\n",
      "To see this just consider the case when X1 = X2 = . . . = XN . Thus, in this\n",
      "\n",
      "========\n",
      "6\n",
      "\n",
      "========\n",
      "case the learner has many copies of the same random variable and successful\n",
      "generalization is thus impossible. We believe that the assumption that the\n",
      "process is in a steady state is not essential for our result, as when the process\n",
      "reaches its steady state quickly then (at the price of a more involved proof ) the\n",
      "result would still hold. 3.2.2\n",
      "Assumptions on the MDP\n",
      "In order to prevent the uncontrolled growth of the errors as they are prop-\n",
      "agated through the updates, we shall need some assumptions on the MDP. A\n",
      "convenient assumption is the following one [11]: Assumption A3 (Uniformly\n",
      "stochastic transitions) For all x ? X and a ? A, assume that P (?—x, a) is\n",
      "absolutely continuous w.r.t. ? and the derivative of P w.r.t. ? is bounded ? ?\n",
      "Radon-Nikodym def ? dP (?—x,a) ? uniformly with bound C? : C? = supx?X\n",
      ",a?A ? d? ? ¡ +?. ?\n",
      "Note that by the deﬁnition of measure diﬀerentiation, Assumption A3 means\n",
      "that P (?—x, a) ? C? ?(?). This assumption essentially requires the transitions\n",
      "to be noisy. We will also prove (weaker) results under the following, weaker\n",
      "assumption: 4\n",
      "For the deﬁnition of ?-mixing, see e.g. [2]. We say ?empirical process? and\n",
      "?empirical measure?, but note that in this work these are based on dependent\n",
      "(mixing) samples. 5\n",
      "5\n",
      "Assumption A4 (Discounted-average concentrability of future-state distribu-\n",
      "tions) Given ?, ?, m ? 1 and an arbitrary sequence of stationary policies {?m\n",
      "}m?1 , assume that the futuredef state distribution ?P ?1 P ?2 .\n",
      ".\n",
      ".?P ?m is\n",
      "absolutely continuous w.r.t. ?. Assume that c(m) = ? ?1 ?2 ?m ? P def ? sup?1\n",
      ",...,?m ? d(?P Pd? ...P ) ? satisﬁes m?1 m? m?1 c(m) ¡ +?. We shall call C?,?\n",
      "= ? ? ? P P max (1 ? ?)2 m?1 m? m?1 c(m), (1 ? ?) m?1 ? m c(m) the\n",
      "discounted-average concentrability coeﬃcient of the future-state distributions.\n",
      "The number c(m) measures how much ? can get ampliﬁed in m steps as com-\n",
      "pared to the reference distribution ?. Hence, in general we expect c(m) to grow\n",
      "with m. In fact, the condition that C?,? is ﬁnite is a growth rate condition\n",
      "on c(m). Thanks to discounting, C?,? is ﬁnite for a reasonably large class of\n",
      "systems (see the discussion in [11]). A related assumption is needed in the error\n",
      "analysis of the approximate greedy step of the algorithm: Assumption A5 (The\n",
      "random policy ?makes no peak-states?) Consider the distribution ? = (? ?\n",
      "?A )P which is the distribution of a state that results from sampling an initial\n",
      "state according to ? and then executing an action which is selected uniformly at\n",
      "random.6 Then ?? = kd?/d?k? ¡ +?. Note that under Assumption A3 we have\n",
      "?? ? C? . This (very mild) assumption means that after one step, starting from\n",
      "? and executing this random policy, the probability of the next state being in a\n",
      "set is upper bounded by ?? -times the probability of the starting state being in\n",
      "the same set. def\n",
      "Besides, we assume that A has the following regularity property: Let Py(a,\n",
      "h, ?) = ? ? (a0 , v) ? RdA +1 : ka ? a0 k1 ? ?, 0 ? v/h ? 1 ? ka ? a0 k1\n",
      "/? denote the pyramid with hight ? def ? h and base given by the ‘1 -ball B(a,\n",
      "?) = a0 ? RdA : ka ? a0 k1 ? ? centered at a. Assumption A6 (Regularity of\n",
      "\n",
      "========\n",
      "7\n",
      "\n",
      "========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the action space) We assume that there exists ? ¿ 0, such that for all a ? A, for\n",
      "all ? ¿ 0, ? ? ?(Py(a, 1, ?) ? (A ? R)) ?(A) ? min ?, . ?(Py(a, 1, ?)) ?(B(a,\n",
      "?)) For example, if A is an ‘1 -ball itself, then this assumption will be satisﬁed\n",
      "with ? = 2?dA . Without assuming any smoothness of the MDP, learning in\n",
      "inﬁnite MDPs looks hard (see, e.g., [12, 13]). Here we employ the following extra\n",
      "condition: Assumption A7 (Lipschitzness of the MDP in the actions) Assume\n",
      "that the transition probabilities and rewards are Lipschitz w.r.t. their action\n",
      "variable, i.e., there exists LP , Lr ¿ 0 such that for all (x, a, a0 ) ? X ? A ? A\n",
      "and measurable set B of X , —P (B—x, a) ? P (B—x, a0 )— ? LP ka ? a0 k1 ,\n",
      "—r(x, a) ? r(x, a0 )— ? Lr ka ? a0 k1 .\n",
      "Note that previously Lipschitzness w.r.t. the state variables was used, e.g.,\n",
      "in [11] to construct consistent planning algorithms. 3.2.3\n",
      "Assumptions on the function sets used by the algorithm\n",
      "These assumptions are less demanding since they are under the control of\n",
      "the user of the algorithm. However, the choice of these function sets will greatly\n",
      "inﬂuence the performance of the algorithm, as we shall see it from the bounds.\n",
      "The ﬁrst assumption concerns the class F: Assumption A8 (Lipschitzness of\n",
      "candidate action-value functions) Assume F ? B(X ? A) and that any elements\n",
      "of F is uniformly Lipschitz in its action-argument in the sense that —Q(x, a) ?\n",
      "Q(x, a0 )— ? LA ka ? a0 k1 holds for any x ? X , a,a0 ? A, and Q ? F . 6\n",
      "Remember that ?A denotes the uniform distribution over the action set A.\n",
      "6\n",
      "We shall also need to control the capacity of our function sets. We assume\n",
      "that the reader is familiar with the concept of VC-dimension.7 Here we use\n",
      "the pseudo-dimension of function sets that builds upon the concept of VC-\n",
      "dimension: Deﬁnition 3.1 (Pseudo-dimension). The pseudo-dimension VF + of\n",
      "F is deﬁned as the VCdimension of the subgraphs of functions in F (hence it is\n",
      "also called the VC-subgraph dimension of F). Since A is multidimensional, we\n",
      "deﬁne V?+ to be the sum of the pseudo-dimensions of the coordinate pro jection\n",
      "spaces, ?k of ?: V\n",
      "?+\n",
      "=\n",
      "dA X\n",
      "V? + ,\n",
      "k=1\n",
      "k\n",
      "?k = { ?k : X ? R : ? = (?1 , . . . , ?k , . . . , ?dA ) ? ? } .\n",
      "Now we are ready to state our assumptions on our function sets: Assumption\n",
      "A9 (Capacity of the function and policy sets) Assume that F ? B(X ? A; Qmax\n",
      ") for Qmax ¿ 0 and VF + ¡ +?. Also, A ? [?A? , A? ]dA and V?+ ¡ +?. Besides\n",
      "their capacity, one shall also control the approximation power of the function\n",
      "sets involved. Let us ﬁrst consider the policy set ?. Introduce e? (F, ?) = sup\n",
      "inf ?(EQ ? E ? Q). Q?F ???\n",
      "Note that inf ??? ?(EQ ? E ? Q) measures the quality of approximating\n",
      "?EQ by ?E ? Q. Hence, e? (F, ?) measures the worst-case approximation error\n",
      "of ?EQ as Q is changed within F. This can be made small by choosing ? large.\n",
      "\n",
      "========\n",
      "8\n",
      "\n",
      "========\n",
      "Another related quantity is the one-step Bellman-error of F w.r.t. ?. This is\n",
      "deﬁned as follows: For a ﬁxed policy ?, the one-step Bellman-error of F w.r.t.\n",
      "T ? is deﬁned as E1 (F; ?) = sup inf kQ0 ? T ? Qk? . 0 Q?F Q ?F\n",
      "Taking again a pessimistic approach, the one-step Bellman-error of F is\n",
      "deﬁned as E1 (F, ?) = sup E1 (F; ?). ???\n",
      "Typically by increasing F, E1 (F, ?) can be made smaller (this is discussed\n",
      "at some length in [3]). However, it also holds for both ? and F that making\n",
      "them bigger will increase their capacity (pseudo-dimensions) which leads to an\n",
      "increase of the estimation errors. Hence, F and ? must be selected to balance\n",
      "the approximation and estimation errors, just like in supervised learning. 3.3\n",
      "The main result\n",
      "Theorem 3.2. Let ?K be a greedy policy w.r.t. QK , i.e. ?K (x) ? argmaxa?A\n",
      "QK (x, a). Then under Assumptions A1, A2, and A5?A9, for all ? ¿ 0 we have\n",
      "with probability at least 1 ? ?: given Assumption A3 (respectively A4), kV ? ?\n",
      "V ?K k? (resp. kV ? ? V ?K k1,? ), is bounded by ? ?? ? d 1+1 ?+1 ? ? A ?\n",
      "? 4? (log N + log(K/?)) K ? + ? , C ?E1 (F, ?) + e? (F, ?) + 1/4 ? ? N ? ?\n",
      "A where C depends on dA , VF + , (V?+ )dk=1 , ?, ?, b, ?, C? (resp. C?,? ),\n",
      "?? , LA , LP ,Lr , ?, ?(A), ?0 , k\n",
      "?+1\n",
      "? max , and A? . In particular, C scales with V 4?(dA +1) , where V =\n",
      "2VF + + V?+ Qmax , Rmax , R plays the role of the ?combined eﬀective?\n",
      "dimension of F and ?. 7 Readers not familiar with VC-dimension are suggested\n",
      "to consult a book, such as the one by Anthony and Bartlett [14].\n",
      "7\n",
      "4\n",
      "Discussion\n",
      "We have presented what we believe is the ﬁrst ﬁnite-time bounds for continuous-\n",
      "state and actionspace RL that uses value functions. Further, this is the ﬁrst\n",
      "analysis of ﬁtted Q-iteration, an algorithm that has proved to be useful in a\n",
      "number of cases, even when used with non-averagers for which no previous the-\n",
      "oretical analysis existed (e.g., [15, 16]).\n",
      "In fact, our main motivation was to\n",
      "show that there is a systematic way of making these algorithms work and to\n",
      "point at possible problem sources the same time. We discussed why it can be\n",
      "diﬃcult to make these algorithms work in practice. We suggested that either the\n",
      "set of action-value candidates has to be carefully controlled (e.g., assuming uni-\n",
      "form Lipschitzness w.r.t. the state variables), or a policy search step is needed,\n",
      "just like in actorcritic algorithms. The bound in this paper is similar in many\n",
      "respects to a previous bound of a Bellman-residual minimization algorithm [2].\n",
      "It looks that the techniques developed here can be used to obtain results for\n",
      "that algorithm when it is applied to continuous action spaces. Finally, although\n",
      "we have not explored them here, consistency results for FQI can be obtained\n",
      "from our results using standard methods, like the methods of sieves. We believe\n",
      "that the methods developed here will eventually lead to algorithms where the\n",
      "function approximation methods are chosen based on the data (similar to adap-\n",
      "tive regression methods) so as to optimize performance, which in our opinion is\n",
      "one of the biggest open questions in RL. Currently we are exploring this pos-\n",
      "\n",
      "========\n",
      "9\n",
      "\n",
      "========\n",
      "sibility. Acknowledgments Andr?as Antos would like to acknowledge support\n",
      "for this pro ject from the Hungarian Academy of Sciences (Bolyai Fellowship).\n",
      "Csaba Szepesv?ari greatly acknowledges the support received from the Alberta\n",
      "Ingenuity Fund, NSERC, the Computer and Automation Research Institute of\n",
      "the Hungarian Academy of Sciences.\n",
      "\n",
      "========\n",
      "2 References\n",
      "\n",
      "========\n",
      "[1] A. Antos, Cs. Szepesv?ari, and R. Munos. Learning near-optimal policies\n",
      "with Bellman-residual minimization based ﬁtted policy iteration and a single\n",
      "sample path. In COLT-19, pages 574?588, 2006. [2] A. Antos, Cs. Szepesv?ari,\n",
      "and R. Munos. Learning near-optimal policies with Bellman-residual minimiza-\n",
      "tion based ﬁtted policy iteration and a single sample path. Machine Learning,\n",
      "2007. (accepted). [3] A. Antos, Cs. Szepesv?ari, and R. Munos. Value-iteration\n",
      "based ﬁtted policy iteration: learning with a single tra jectory. In IEEE ADPRL,\n",
      "pages 330?337, 2007. [4] D. P. Bertsekas and S.E. Shreve. Stochastic Optimal\n",
      "Control (The Discrete Time Case). Academic Press, New York, 1978.\n",
      "[5] D.\n",
      "Ernst, P. Geurts, and L. Wehenkel. Tree-based batch mode reinforcement learn-\n",
      "ing. Journal of Machine Learning Research, 6:503?556, 2005.\n",
      "[6] R.S. Sutton\n",
      "and A.G. Barto. Reinforcement Learning: An Introduction. Bradford Book.\n",
      "MIT Press, 1998.\n",
      "[7] N. Cristianini and J. Shawe-Taylor. An introduction\n",
      "to support vector machines (and other kernel-based learning methods). Cam-\n",
      "bridge University Press, 2000. [8] J.A. Boyan and A.W. Moore. Generalization\n",
      "in reinforcement learning: Safely approximating the value function. In NIPS-\n",
      "7, pages 369?376, 1995.\n",
      "[9] P.L. Bartlett, P.M. Long, and R.C. Williamson.\n",
      "Fat-shattering and the learnability of real-valued functions. Journal of Com-\n",
      "puter and System Sciences, 52:434?452, 1996. [10] A.N. Kolmogorov and V.M.\n",
      "Tihomirov. ?-entropy and ?-capacity of sets in functional space. American\n",
      "Mathematical Society Translations, 17(2):277?364, 1961.\n",
      "[11] R. Munos and\n",
      "Cs. Szepesv?ari. Finite time bounds for sampling based ﬁtted value iteration.\n",
      "Technical report, Computer and Automation Research Institute of the Hungar-\n",
      "ian Academy of Sciences, Kende u. 13-17, Budapest 1111, Hungary, 2006. [12]\n",
      "A.Y. Ng and M. Jordan. PEGASUS: A policy search method for large MDPs\n",
      "and POMDPs. In Proceedings of the 16th Conference in Uncertainty in Artiﬁ-\n",
      "cial Intelligence, pages 406?415, 2000. [13] P.L. Bartlett and A. Tewari. Sample\n",
      "complexity of policy search with known dynamics.\n",
      "In NIPS-19. MIT Press,\n",
      "2007. [14] M. Anthony and P. L. Bartlett. Neural Network Learning: Theoreti-\n",
      "cal Foundations. Cambridge University Press, 1999. [15] M. Riedmiller. Neural\n",
      "ﬁtted Q iteration ? ﬁrst experiences with a data eﬃcient neural reinforcement\n",
      "learning method.\n",
      "In 16th European Conference on Machine Learning, pages\n",
      "317?328, 2005.\n",
      "[16] S. Kalyanakrishnan and P. Stone. Batch reinforcement\n",
      "learning in a complex domain. In AAMAS-07, 2007.\n",
      "8\n",
      "\n",
      "========\n",
      "10\n",
      "\n",
      "========\n"
     ]
    }
   ],
   "source": [
    "List = ['PP4300.pdf','PP3289.pdf','PP3367.pdf','PP3233.pdf']\n",
    "for i in range(0,len(List)):\n",
    "    target = List[i]\n",
    "    #print (target)\n",
    "    a = parse_to_proc(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fitted Q-iteration in continuous action-space MDPs'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[\"titles\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We consider continuous state, continuous action batch reinforcement learning where the goal is to learn a good policy from a suﬃciently rich tra jectory generated by another policy. We study a variant of ﬁtted Qiteration, where the greedy action selection is replaced by searching for a policy in a restricted set of candidate policies by maximizing the average action values. We provide a rigorous theoretical analysis of this algorithm, proving what we believe is the ﬁrst ﬁnite-time bounds for value-function based algorithms for continuous stateand action-space problems.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[\"abstract\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We consider continuous state, continuous action batch reinforcement learning where the goal is to learn a good policy from a suﬃciently rich tra jectory generated by some policy. We study a variant of ﬁtted Q-iteration, where the greedy action selection is replaced by searching for a policy in a restricted set of candidate policies by maximizing the average action values. We provide a rigorous analysis of this algorithm, proving what we believe is the ﬁrst ﬁnite-time bound for value-function based algorithms for continuous state and action problems.',\n",
       " 'Preliminaries We will build on the results from [1, 2, 3] and for this reason we use the same notation as these papers. The unattributed results cited in this section can be found in the book [4]. A discounted MDP is deﬁned by a quintuple (X , A, P, S, ?), where X is the (possible inﬁnite) state space, A is the set of actions, P : X ? A ? M (X ) is the transition probability kernel with P (?—x, a) deﬁning the next-state distribution upon taking action a from state x, S(?—x, a) gives the corresponding distribution of immediate rewards, and ? ? (0, 1) is the discount factor. Here X is a measurable space and M (X ) denotes the set of all probability measures over X . The Lebesguemeasure shall be denoted by ?. We start with the following mild assumption on the MDP: Assumption A1 (MDP Regularity) X is a compact subset of the dX -dimensional Euclidean space, ? max A is a compact subset of [?A? , A? ]dA . The random immediate rewards are bounded by R R and that the expected immediate reward function, r(x, a) = rS(dr—x, a), is uniformly bounded by Rmax : krk? ? Rmax . A policy determines the next action given the past observations. Here we shall deal with stationary (Markovian) policies which choose an action in a stochastic way based on the last observation only. The value of a policy ? when it is started from a state x is deﬁned as the expected discounted Ptotal ? reward that is encountered while the policy is executed: V ? (x) = E? [ t=0 ? t Rt —X0 = x]. Here Rt ? S(?—Xt , At ) is the reward received at time step t, the state, Xt , evolves according to Xt+1 ? ? Also with: Computer and Automation Research Inst. of the Hungarian Academy of Sciences Kende u. 13-17, Budapest 1111, Hungary.',\n",
       " 'P (?—Xt , At ), where At is sampled from the distribution determined by ?. We use Q? : X ? A ? R P? to denote the action-value function of policy ?: Q? (x, a) = E? [ t=0 ? t Rt —X0 = x, A0 = a]. The goal is to ﬁnd a policy that attains the best possible values, V ? (x) = sup? V ? (x), at all states ? x ? X . Here V ? is called the optimal value function and a policy ? ? that satisﬁes V ? (x) = ? ? ? V (x) for all x ? X is called optimal. The optimal action-value function Q (x, a) is Q (x, a) = sup? Q? (x, a). We say that a (deterministic stationary) policy ? is greedy w.r.t. an action-value function Q ? B(X ? A), and we write ? = ? ? (?; Q), if, for all x ? X , ?(x) ? argmaxa?A Q(x, a). Under mild technical assumptions, such a greedy policy always exists. Any greedy policy w.r.t. Q? ? is optimal. For ? : X ? A we ? A), by R deﬁne its evaluation operator, T : B(X ?? A) ?? B(X ? (T Q)(x, a) = r(x, a) + ? X Q(y, ?(y)) P (dy—x, a). It is known that Q = T Q? . Further, if weR let the Bellman operator, T : B(X ? A) ? B(X ? A), deﬁned by (T Q)(x, a) = r(x, a) + ? X supb?A Q(y, b) P (dy—x, a) then Q? = T Q? . It is known that V ? and Q? are bounded by Rmax /(1 ? ?), just like Q? and V ? . For ? : X ? A, the operator E ? : B(X ? A) ? B(X ) is deﬁned by (E ? Q)(x) = Q(x, ?(x)), while E : B(X ? A) ? B(X ) is deﬁned by (EQ)(x) = supa?A Q(x, a). Throughout the paper F ? {f : X ? A ? R} will denote a subset of real-valued functions over the state-action space X ? A and ? ? AX will R be a set of policies. For ? ? M (X ) and f : X ? R p measurable, we let (for p ? 1) kf kp,? = X —f (x)—p ?(dx). We simply write kf k? for kf k2,? . R R 2 Further, we extend k?k? to F by kf k? = A X —f —2 (x, a) d?(x) d?A (a), where ?RA is the uniform distribution over A. We shall use the shorthand notation ?f to denote the integral f (x)?(dx). We denote the space of bounded measurable functions with domain X by B(X ). Further, the space of measurable functions bounded by 0 ¡ K ¡ ? shall be denoted by B(X ; K). We let k?k? denote the supremum norm.',\n",
       " 'Fitted Q-iteration with approximate policy maximization We assume that we are given a ﬁnite tra jectory, {(Xt , At , Rt )}1?t?N , generated by some stochastic stationary policy ?b , called the behavior policy: At ? ?b (?—Xt ), Xt+1 ? P (?—Xt , At ), Rt ? def S(?—Xt , At ), where ?b (?—x) is a density with ?0 = inf (x,a)?X ?A ?b (a—x) ¿ 0. The generic recipe for ﬁtted Q-iteration (FQI) [5] is Qk+1 = Regress(Dk (Qk )), (1) where Regress is an appropriate regression procedure and Dk (Qk ) is a dataset deﬁning a regression problem in the form of a list of data-point pairs: ?h ? i Dk (Qk ) = (Xt , At ), Rt + ? max Qk (Xt+1 , b) .1 b?A 1?t?N Fitted Q-iteration can be viewed as approximate value iteration applied to action-value functions. To see this note that value iteration would assign the value (T Qk )(x, a) = r(x, a) + R ? maxb?A Qk (y, b) P (dy—x, a) to Qk+1 (x, a) [6]. Now, remember that the regression function for the jointly distributed random variables (Z, Y ) is deﬁned by the conditional expectation of Y given Z: m(Z) = E [Y —Z]. Since for any ﬁxed function Q, E [Rt + ? maxb?A Q(Xt+1 , b)—Xt , At ] = (T Q)(Xt , At ), the regression function corresponding to the data Dk (Q) is indeed T Q and hence if FQI solved the regression problem deﬁned by Qk exactly, it would simulate value iteration exactly. However, this argument itself does not directly lead to a rigorous analysis of FQI: Since Qk is obtained based on the data, it is itself a random function. Hence, after the ﬁrst iteration, the ?target? function in FQI becomes random. Furthermore, this function depends on the same data that is used to deﬁne the regression problem. Will FQI still work despite these issues? To illustrate the potential diﬃculties consider a dataset where X1 , . . . , XN is a sequence of independent random variables, which are all distributed uniformly at random in [0, 1]. Further, let M be a random integer greater than N which is independent of the dataset (Xt )N t=1 . Let U be another random variable, uniformly distributed in [0, 1]. Now deﬁne the regression problem by Yt = fM,U (Xt ), where fM,U (x) = sgn(sin(2M 2?(x + U ))). Then it is not hard to see that no matter how big N is, no procedure can 1 Since the designer controls Qk , we may assume that it is continuous, hence the maximum exists.',\n",
       " 'estimate the regression function fM,U with a small error (in expectation, or with high probability), even if the procedure could exploit the knowledge of the speciﬁc form of fM,U . On the other hand, if we restricted M to a ﬁnite range then the estimation problem could be solved successfully. The example shows that if the complexity of the random functions deﬁning the regression problem is uncontrolled then successful estimation might be impossible. Amongst the many regression methods in this paper we have chosen to work with leastsquares methods. In this case Equation (1) takes the form ? ? ??2 N X 1 Q(Xt , At ) ? Rt + ? max Qk (Xt+1 , b) Qk+1 = argmin . (2) b?A Q?F t=1 ?b (At —Xt ) We call this method the least-squares ﬁtted Q-iteration (LSFQI) method. Here we introduced the weighting 1/?b (At —Xt ) since we do not want to give more weight to those actions that are preferred by the behavior policy. Besides this weighting, the only parameter of the method is the function set F. This function set should be chosen carefully, to keep a balance between the representation power and the number of samples. As a speciﬁc example for F consider neural networks with some ﬁxed architecture. In this case the function set is generated by assigning weights in all possible ways to the neural net. Then the above minimization becomes the problem of tuning the weights. Another example is to use linearly parameterized function approximation methods with appropriately selected basis functions. In this case the weight tuning problem would be less demanding. Yet another possibility is to let F be an appropriate restriction of a Reproducing Kernel Hilbert Space (e.g., in a ball). In this case the training procedure becomes similar to LS-SVM training [7]. As indicated above, the analysis of this algorithm is complicated by the fact that the new dataset is deﬁned in terms of the previous iterate, which is already a function of the dataset. Another complication is that the samples in a tra jectory are in general correlated and that the bias introduced by the imperfections of the approximation architecture may yield to an explosion of the error of the procedure, as documented in a number of cases in, e.g., [8]. Nevertheless, at least for ﬁnite action sets, the tools developed in [1, 3, 2] look suitable to show that under appropriate conditions these problems can be overcome if the function set is chosen in a judicious way. However, the results of these works would become essentially useless in the case of an inﬁnite number of actions since these previous bounds grow to inﬁnity with the number of actions. Actually, we believe that this is not an artifact of the proof techniques of these works, as suggested by the counterexample that involved random targets. The following result elaborates this point further: Proposition 2.1. Let F ? B(X ? A). Then even if the pseudo-dimension of F is ﬁnite, the fatshattering function of ? ? ? Fmax = VQ : VQ (?) = max Q(?, a), Q ? F a?A',\n",
       " 'can be inﬁnite over (0, 1/2). Without going into further details, let us just note that the ﬁniteness of the fat-shattering function is a suﬃcient and necessary condition for learnability and the ﬁniteness of the fat-shattering function is implied by the ﬁniteness of the pseudo-dimension [9].The above proposition thus shows that without imposing further special conditions on F, the learning problem may become infeasible. One possibility is of course to discretize the action space, e.g., by using a uniform grid. However, if the action space has a really high dimensionality, this approach becomes unfeasible (even enumerating 2dA points could be impossible when dA is large). Therefore we prefer alternate solutions. Another possibility is to make the functions in F, e.g., uniformly Lipschitz in their state coordinates. ? Then the same property will hold for functions in Fmax and hence by a classical result we can bound the capacity of this set (cf. pp. 353?357 of [10]). One potential problem with this approach is that this way it might be diﬃcult to get a ﬁne control of the capacity of the resulting set. 2 The proof of this and the other results are given in the appendix, available in the extended version of this paper, downloadable from http://hal.inria.fr/inria-00185311/en/.',\n",
       " 'In the approach explored here we modify the ﬁtted Q-iteration algorithm by introducing a policy set ? and a search over this set for an approximately greedy policy in a sense that will be made precise in a minute. Our algorithm thus has four parameters: F, ?, K, Q0 . Here F is as before, ? is a user-chosen set of policies (mappings from X to A), K is the number of iterations and Q0 is an initial value function (a typical choice is Q0 ? 0). The algorithm computes a sequence of iterates (Qk , ? ?k ), k = 0, . . . , K, deﬁned by the following equations: ? ?0 Qk+1 ? ?k+1 = argmax ??? = argmin N X argmax ??? Q0 (Xt , ?(Xt )), t=1 Q?F = N X t=1 N X ? ? ??2 1 Q(Xt , At ) ? Rt + ?Qk (Xt+1 , ? ?k (Xt+1 )) , ?b (At —Xt ) (3) Qk+1 (Xt , ?(Xt )). (4) t=1 Thus, (3) is similar to (2), while (4) deﬁnes the policy search problem. The policy search will generally be solved by a gradient procedure or some other appropriate method. The cost of this step will be primarily determined by how well-behaving the iterates Qk+1 are in their action arguments. For example, if they were quadratic and if ? was linear then the problem would be a quadratic optimization problem. However, except for special cases3 the action value functions will be more complicated, in which case this step can be expensive. Still, this cost could be similar to that of searching for the maximizing actions for each t = 1, . . . , N if the approximately maximizing actions are similar across similar states. This algorithm, which we could also call a ﬁtted actor-critic algorithm, will be shown to overcome the above mentioned complexity control problem provided that the complexity of ? is controlled appropriately. Indeed, in this case the set of possible regression problems is determined by the set F?? = { V : V (?) = Q(?, ?(?)), Q ? F, ? ? ? } , and the proof will rely on controlling the complexity of F?? by selecting F and ? appropriately. 3 3.1 The main theoretical result Outline of the analysis In order to gain some insight into the behavior of the algorithm, we provide a brief summary of its error analysis. The main result will be presented subsequently. For f ,Q ? F and a policy ?, we deﬁne the tth TD-error as follows: dt (f ; Q, ?) = Rt + ?Q(Xt+1 , ?(Xt+1 )) ? f (Xt , At ). Further, we deﬁne the empirical loss function by N X d2t (f ; Q, ?) ? N (f ; Q, ?) = 1 , L N t=1 ?(A)?b (At —Xt ) where the normalization with ?(A) is introduced for mathematical convenience. Then (3) can be ? N (f ; Qk , ? written compactly as Qk+1 = argminf ?F L ?k ). ? N (f ; Q, ?) is an The algorithm can then be motivated by the observation that for any f ,Q, and ?, L unbiased estimate of def 2 L(f ; Q, ?) = kf ? T ? Qk? + L? (Q, ?), (5) where the ﬁrst term is the error we are interested in and the second term captures the variance of the random samples: Z L? (Q, ?) = E [Var [R1 + ?Q(X2 , ?(X2 ))—X1 , A1 = a]] d?A (a). A 3 Linear quadratic regulation is such a nice case. It is interesting to note that in this special case the obvious choices for F and ? yield zero error in the limit, as can be proven based on the main result of this paper.',\n",
       " 'h i ? N (f ; Q, ?) = L(f ; Q, ?). This result is stated formally by E L Since the variance term in (5) is independent of f , argminf ?F L(f ; Q, ?) = 2 ? argminf ?F kf ? T Qk? . Thus, if ? ?k were greedy w.r.t. Qk then argminf ?F L(f ; Qk , ? ?k ) = 2 argminf ?F kf ? T Qk k? . Hence we can still think of the procedure as approximate value iteration over the space of action-value functions, pro jecting T Qk using empirical risk minimization on the space F w.r.t. k?k? distances in an approximate manner. Since ? ?k is only approximately greedy, we will have to deal with both the error coming from the approximate pro jection and the error coming from the choice of ? ?k . To make this clear, we write the iteration in the form Qk+1 = T ??k Qk + ?0k = T Qk + ?0k + (T ??k Qk ? T Qk ) = T Qk + ?k , def where ?0k is the error committed while computing T ??k Qk , ?00k = T ??k Qk ? T Qk is the error committed because the greedy policy is computed approximately and ?k = ?0k + ?00k is the total error of step k. Hence, in order to show that the procedure is well behaved, one needs to show that both errors are controlled and that when the errors are propagated through these equations, the resulting error stays controlled, too. Since we are ultimately interested in the performance of the policy obtained, we will also need to show that small action-value approximation errors yield small performance losses. For these we need a number of assumptions that concern either the training data, the MDP, or the function sets used for learning. 3.2 Assumptions 3.2.1 Assumptions on the training data We shall assume that the data is rich, is in a steady state, and is fastmixing, where, informally, mixing means that future depends weakly on the past. Assumption A2 (Sample Path Properties) Assume that {(Xt , At , Rt )}t=1,...,N is the sample path of ?b , a stochastic stationary policy. Further, assume that {Xt } is strictly stationary (Xt ? ? ? M (X )) and exponentially ?-mixing with the actual rate given by the parameters (?, b, ?).4 We further assume that the sampling policy ?b satisﬁes ?0 = inf (x,a)?X ?A ?b (a—x) ¿ 0. The ?-mixing property will be used to establish tail inequalities for certain empirical processes.5 Note that the mixing coeﬃcients do not need to be known. In the case when no mixing condition is satisﬁed, learning might be impossible. To see this just consider the case when X1 = X2 = . . . = XN . Thus, in this case the learner has many copies of the same random variable and successful generalization is thus impossible. We believe that the assumption that the process is in a steady state is not essential for our result, as when the process reaches its steady state quickly then (at the price of a more involved proof ) the result would still hold. 3.2.2 Assumptions on the MDP In order to prevent the uncontrolled growth of the errors as they are propagated through the updates, we shall need some assumptions on the MDP. A convenient assumption is the following one [11]: Assumption A3 (Uniformly stochastic transitions) For all x ? X and a ? A, assume that P (?—x, a) is absolutely continuous w.r.t. ? and the derivative of P w.r.t. ? is bounded ? ? Radon-Nikodym def ? dP (?—x,a) ? uniformly with bound C? : C? = supx?X ,a?A ? d? ? ¡ +?. ? Note that by the deﬁnition of measure diﬀerentiation, Assumption A3 means that P (?—x, a) ? C? ?(?). This assumption essentially requires the transitions to be noisy. We will also prove (weaker) results under the following, weaker assumption: 4 For the deﬁnition of ?-mixing, see e.g. [2]. We say ?empirical process? and ?empirical measure?, but note that in this work these are based on dependent (mixing) samples. 5',\n",
       " 'Assumption A4 (Discounted-average concentrability of future-state distributions) Given ?, ?, m ? 1 and an arbitrary sequence of stationary policies {?m }m?1 , assume that the futuredef state distribution ?P ?1 P ?2 . . .?P ?m is absolutely continuous w.r.t. ?. Assume that c(m) = ? ?1 ?2 ?m ? P def ? sup?1 ,...,?m ? d(?P Pd? ...P ) ? satisﬁes m?1 m? m?1 c(m) ¡ +?. We shall call C?,? = ? ? ? P P max (1 ? ?)2 m?1 m? m?1 c(m), (1 ? ?) m?1 ? m c(m) the discounted-average concentrability coeﬃcient of the future-state distributions. The number c(m) measures how much ? can get ampliﬁed in m steps as compared to the reference distribution ?. Hence, in general we expect c(m) to grow with m. In fact, the condition that C?,? is ﬁnite is a growth rate condition on c(m). Thanks to discounting, C?,? is ﬁnite for a reasonably large class of systems (see the discussion in [11]). A related assumption is needed in the error analysis of the approximate greedy step of the algorithm: Assumption A5 (The random policy ?makes no peak-states?) Consider the distribution ? = (? ? ?A )P which is the distribution of a state that results from sampling an initial state according to ? and then executing an action which is selected uniformly at random.6 Then ?? = kd?/d?k? ¡ +?. Note that under Assumption A3 we have ?? ? C? . This (very mild) assumption means that after one step, starting from ? and executing this random policy, the probability of the next state being in a set is upper bounded by ?? -times the probability of the starting state being in the same set. def Besides, we assume that A has the following regularity property: Let Py(a, h, ?) = ? ? (a0 , v) ? RdA +1 : ka ? a0 k1 ? ?, 0 ? v/h ? 1 ? ka ? a0 k1 /? denote the pyramid with hight ? def ? h and base given by the ‘1 -ball B(a, ?) = a0 ? RdA : ka ? a0 k1 ? ? centered at a. Assumption A6 (Regularity of the action space) We assume that there exists ? ¿ 0, such that for all a ? A, for all ? ¿ 0, ? ? ?(Py(a, 1, ?) ? (A ? R)) ?(A) ? min ?, . ?(Py(a, 1, ?)) ?(B(a, ?)) For example, if A is an ‘1 -ball itself, then this assumption will be satisﬁed with ? = 2?dA . Without assuming any smoothness of the MDP, learning in inﬁnite MDPs looks hard (see, e.g., [12, 13]). Here we employ the following extra condition: Assumption A7 (Lipschitzness of the MDP in the actions) Assume that the transition probabilities and rewards are Lipschitz w.r.t. their action variable, i.e., there exists LP , Lr ¿ 0 such that for all (x, a, a0 ) ? X ? A ? A and measurable set B of X , —P (B—x, a) ? P (B—x, a0 )— ? LP ka ? a0 k1 , —r(x, a) ? r(x, a0 )— ? Lr ka ? a0 k1 . Note that previously Lipschitzness w.r.t. the state variables was used, e.g., in [11] to construct consistent planning algorithms. 3.2.3 Assumptions on the function sets used by the algorithm These assumptions are less demanding since they are under the control of the user of the algorithm. However, the choice of these function sets will greatly inﬂuence the performance of the algorithm, as we shall see it from the bounds. The ﬁrst assumption concerns the class F: Assumption A8 (Lipschitzness of candidate action-value functions) Assume F ? B(X ? A) and that any elements of F is uniformly Lipschitz in its action-argument in the sense that —Q(x, a) ? Q(x, a0 )— ? LA ka ? a0 k1 holds for any x ? X , a,a0 ? A, and Q ? F . 6 Remember that ?A denotes the uniform distribution over the action set A.',\n",
       " 'We shall also need to control the capacity of our function sets. We assume that the reader is familiar with the concept of VC-dimension.7 Here we use the pseudo-dimension of function sets that builds upon the concept of VCdimension: Deﬁnition 3.1 (Pseudo-dimension). The pseudo-dimension VF + of F is deﬁned as the VCdimension of the subgraphs of functions in F (hence it is also called the VC-subgraph dimension of F). Since A is multidimensional, we deﬁne V?+ to be the sum of the pseudo-dimensions of the coordinate pro jection spaces, ?k of ?: V ?+ = dA X V? + , k=1 k ?k = { ?k : X ? R : ? = (?1 , . . . , ?k , . . . , ?dA ) ? ? } . Now we are ready to state our assumptions on our function sets: Assumption A9 (Capacity of the function and policy sets) Assume that F ? B(X ? A; Qmax ) for Qmax ¿ 0 and VF + ¡ +?. Also, A ? [?A? , A? ]dA and V?+ ¡ +?. Besides their capacity, one shall also control the approximation power of the function sets involved. Let us ﬁrst consider the policy set ?. Introduce e? (F, ?) = sup inf ?(EQ ? E ? Q). Q?F ??? Note that inf ??? ?(EQ ? E ? Q) measures the quality of approximating ?EQ by ?E ? Q. Hence, e? (F, ?) measures the worst-case approximation error of ?EQ as Q is changed within F. This can be made small by choosing ? large. Another related quantity is the one-step Bellman-error of F w.r.t. ?. This is deﬁned as follows: For a ﬁxed policy ?, the one-step Bellman-error of F w.r.t. T ? is deﬁned as E1 (F; ?) = sup inf kQ0 ? T ? Qk? . 0 Q?F Q ?F Taking again a pessimistic approach, the one-step Bellman-error of F is deﬁned as E1 (F, ?) = sup E1 (F; ?). ??? Typically by increasing F, E1 (F, ?) can be made smaller (this is discussed at some length in [3]). However, it also holds for both ? and F that making them bigger will increase their capacity (pseudo-dimensions) which leads to an increase of the estimation errors. Hence, F and ? must be selected to balance the approximation and estimation errors, just like in supervised learning. 3.3 The main result Theorem 3.2. Let ?K be a greedy policy w.r.t. QK , i.e. ?K (x) ? argmaxa?A QK (x, a). Then under Assumptions A1, A2, and A5?A9, for all ? ¿ 0 we have with probability at least 1 ? ?: given Assumption A3 (respectively A4), kV ? ? V ?K k? (resp. kV ? ? V ?K k1,? ), is bounded by ? ?? ? d 1+1 ?+1 ? ? A ? ? 4? (log N + log(K/?)) K ? + ? , C ?E1 (F, ?) + e? (F, ?) + 1/4 ? ? N ? ? A where C depends on dA , VF + , (V?+ )dk=1 , ?, ?, b, ?, C? (resp. C?,? ), ?? , LA , LP ,Lr , ?, ?(A), ?0 , k ?+1 ? max , and A? . In particular, C scales with V 4?(dA +1) , where V = 2VF + + V?+ Qmax , Rmax , R plays the role of the ?combined eﬀective? dimension of F and ?. 7 Readers not familiar with VC-dimension are suggested to consult a book, such as the one by Anthony and Bartlett [14].',\n",
       " '4 Discussion We have presented what we believe is the ﬁrst ﬁnite-time bounds for continuousstate and actionspace RL that uses value functions. Further, this is the ﬁrst analysis of ﬁtted Q-iteration, an algorithm that has proved to be useful in a number of cases, even when used with non-averagers for which no previous theoretical analysis existed (e.g., [15, 16]). In fact, our main motivation was to show that there is a systematic way of making these algorithms work and to point at possible problem sources the same time. We discussed why it can be diﬃcult to make these algorithms work in practice. We suggested that either the set of action-value candidates has to be carefully controlled (e.g., assuming uniform Lipschitzness w.r.t. the state variables), or a policy search step is needed, just like in actorcritic algorithms. The bound in this paper is similar in many respects to a previous bound of a Bellman-residual minimization algorithm [2]. It looks that the techniques developed here can be used to obtain results for that algorithm when it is applied to continuous action spaces. Finally, although we have not explored them here, consistency results for FQI can be obtained from our results using standard methods, like the methods of sieves. We believe that the methods developed here will eventually lead to algorithms where the function approximation methods are chosen based on the data (similar to adaptive regression methods) so as to optimize performance, which in our opinion is one of the biggest open questions in RL. Currently we are exploring this possibility. Acknowledgments Andr?as Antos would like to acknowledge support for this pro ject from the Hungarian Academy of Sciences (Bolyai Fellowship). Csaba Szepesv?ari greatly acknowledges the support received from the Alberta Ingenuity Fund, NSERC, the Computer and Automation Research Institute of the Hungarian Academy of Sciences. ']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[\"body\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Authored by: R?mi Munos Csaba Szepesv?ri Andr?s Antos'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[\"authors\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "tokenizer = RegexpTokenizer(r\"[A-Za-z]\\w+(?:[-'?]\\w+)?\") \n",
    "def TokenizePatent(raw_text):\n",
    "    unigram_tokens = tokenizer.tokenize(raw)\n",
    "    uni_voc = list(set(unigram_tokens))\n",
    "    mwe_tokenizer = MWETokenizer(uni_voc)\n",
    "    mwe_tokens = mwe_tokenizer.tokenize(unigram_tokens)\n",
    "    stopwords_list = []\n",
    "    with open('./stopwords_en.txt') as f:\n",
    "        stopwords_list = f.read().splitlines()\n",
    "        stopped_tokens = [w for w in  mwe_tokens if w.lower() not in stopwords_list]\n",
    "        return(stopped_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.collocations import *\n",
    "def TopGenery(text,n):\n",
    "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "    bigram_finder = nltk.collocations.BigramCollocationFinder.from_words(all_words)\n",
    "    bigram_finder.apply_freq_filter(n)\n",
    "    bigram_finder.apply_word_filter(lambda w: len(w) < 3)\n",
    "    top_10bigrams = bigram_finder.nbest(bigram_measures.pmi, 10) \n",
    "    return top_10bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Fitted Q-iteration in continuous action-space MDPs']\n",
      "['Fitted', 'iteration', 'continuous', 'action-space', 'MDPs']\n",
      "[('Fitted', 1), ('iteration', 1), ('continuous', 1), ('action-space', 1), ('MDPs', 1)]\n",
      "[('Fitted', 'iteration'), ('action-space', 'MDPs'), ('continuous', 'action-space'), ('iteration', 'continuous')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.collocations import *\n",
    "from nltk.probability import FreqDist\n",
    "from collections import Counter\n",
    "titles = []\n",
    "titles.append(a[\"titles\"])\n",
    "print(titles)\n",
    "tokenizer = RegexpTokenizer(r\"[A-Za-z]\\w+(?:[-'?]\\w+)?\") \n",
    "unigram_tokens = tokenizer.tokenize(str(titles))\n",
    "uni_voc = list(set(unigram_tokens))\n",
    "mwe_tokenizer = MWETokenizer(uni_voc)\n",
    "mwe_tokens = mwe_tokenizer.tokenize(unigram_tokens)\n",
    "stopwords_list = []\n",
    "with open('./stopwords_en.txt') as f:\n",
    "    stopwords_list = f.read().splitlines()\n",
    "stopped_tokens = [w for w in  mwe_tokens if w.lower() not in stopwords_list]\n",
    "print(stopped_tokens)\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "bigram_finder = nltk.collocations.BigramCollocationFinder.from_words(stopped_tokens)\n",
    "bigram_finder.apply_word_filter(lambda w: len(w) < 3)\n",
    "top_10bigrams = bigram_finder.nbest(bigram_measures.pmi, 10)\n",
    "\n",
    "fdist1 = FreqDist(stopped_tokens)\n",
    "print(fdist1.most_common(10))\n",
    "print(top_10bigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Authored by: R?mi Munos Csaba Szepesv?ri Andr?s Antos']\n"
     ]
    }
   ],
   "source": [
    "authors = []\n",
    "authors.append(a[\"authors\"])\n",
    "print(authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Authored by: R?mi Munos Csaba Szepesv?ri Andr?s Antos']\n",
      "[' R?mi Munos Csaba Szepesv?ri Andr?s Antos']\n",
      "['mi', 'Munos', 'Csaba', 'Szepesv?ri', 'Andr?s', 'Antos']\n",
      "[('continuous', 3), ('action', 3), ('policy', 3), ('state', 1), ('batch', 1), ('reinforcement', 1), ('learning', 1), ('goal', 1), ('learn', 1), ('good', 1)]\n"
     ]
    }
   ],
   "source": [
    "authors = []\n",
    "authors.append(a[\"authors\"])\n",
    "print(authors)\n",
    "author = re.findall(\"Authored\\sby:(.*)']\",str(authors))\n",
    "print(author)\n",
    "tokenizer = RegexpTokenizer(r\"[A-Za-z]\\w+(?:[-'?]\\w+)?\") \n",
    "unigram_tokens = tokenizer.tokenize(str(author))\n",
    "uni_voc = list(set(unigram_tokens))\n",
    "mwe_tokenizer = MWETokenizer(uni_voc)\n",
    "mwe_tokens = mwe_tokenizer.tokenize(unigram_tokens)\n",
    "print(str(mwe_tokens))\n",
    "fdist1 = FreqDist(stopped_tokens)\n",
    "print(fdist1.most_common(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We consider continuous state, continuous action batch reinforcement learning where the goal is to learn a good policy from a suﬃciently rich tra jectory generated by another policy. We study a variant of ﬁtted Qiteration, where the greedy action selection is replaced by searching for a policy in a restricted set of candidate policies by maximizing the average action values. We provide a rigorous theoretical analysis of this algorithm, proving what we believe is the ﬁrst ﬁnite-time bounds for value-function based algorithms for continuous stateand action-space problems.']\n",
      "['continuous', 'state', 'continuous', 'action', 'batch', 'reinforcement', 'learning', 'goal', 'learn', 'good', 'policy', 'suﬃciently', 'rich', 'tra', 'jectory', 'generated', 'policy', 'study', 'variant', 'tted', 'Qiteration', 'greedy', 'action', 'selection', 'replaced', 'searching', 'policy', 'restricted', 'set', 'candidate', 'policies', 'maximizing', 'average', 'action', 'values', 'provide', 'rigorous', 'theoretical', 'analysis', 'algorithm', 'proving', 'rst', 'nite-time', 'bounds', 'value-function', 'based', 'algorithms', 'continuous', 'stateand', 'action-space', 'problems']\n",
      "[('continuous', 3), ('action', 3), ('policy', 3), ('state', 1), ('batch', 1), ('reinforcement', 1), ('learning', 1), ('goal', 1), ('learn', 1), ('good', 1)]\n"
     ]
    }
   ],
   "source": [
    "abstracts= []\n",
    "abstracts.append(a[\"abstract\"])\n",
    "print(str(abstracts))\n",
    "tokenizer = RegexpTokenizer(r\"[A-Za-z]\\w+(?:[-'?]\\w+)?\") \n",
    "unigram_tokens = tokenizer.tokenize(str(abstracts))\n",
    "uni_voc = list(set(unigram_tokens))\n",
    "mwe_tokenizer = MWETokenizer(uni_voc)\n",
    "mwe_tokens = mwe_tokenizer.tokenize(unigram_tokens)\n",
    "stopwords_list = []\n",
    "with open('./stopwords_en.txt') as f:\n",
    "    stopwords_list = f.read().splitlines()\n",
    "stopped_tokens = [w for w in  mwe_tokens if w.lower() not in stopwords_list]\n",
    "print(stopped_tokens)\n",
    "\n",
    "fdist1 = FreqDist(stopped_tokens)\n",
    "print(fdist1.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "print(abstracts10)\n",
    "print(title10)\n",
    "print(author10) \n",
    "df = pd.DataFrame({'top10_terms_in_abstracts':abstracts10,'top10_terms_in_titles':title10,'top10_authors':author10})\n",
    "df.to_csv('Group067_stats.csv', index = None, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Summary\n",
    "Give a short summary of your work done above, such as your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PP3206.pdf',\n",
       " 'PP3233.pdf',\n",
       " 'PP3235.pdf',\n",
       " 'PP3289.pdf',\n",
       " 'PP3316.pdf',\n",
       " 'PP3367.pdf',\n",
       " 'PP3453.pdf',\n",
       " 'PP3475.pdf',\n",
       " 'PP3527.pdf',\n",
       " 'PP3529.pdf',\n",
       " 'PP3594.pdf',\n",
       " 'PP3596.pdf',\n",
       " 'PP3635.pdf',\n",
       " 'PP3664.pdf',\n",
       " 'PP3693.pdf',\n",
       " 'PP3698.pdf',\n",
       " 'PP3727.pdf',\n",
       " 'PP3729.pdf',\n",
       " 'PP3733.pdf',\n",
       " 'PP3794.pdf',\n",
       " 'PP3798.pdf',\n",
       " 'PP3807.pdf',\n",
       " 'PP3814.pdf',\n",
       " 'PP3822.pdf',\n",
       " 'PP3835.pdf',\n",
       " 'PP3851.pdf',\n",
       " 'PP3857.pdf',\n",
       " 'PP3872.pdf',\n",
       " 'PP3874.pdf',\n",
       " 'PP3887.pdf',\n",
       " 'PP3888.pdf',\n",
       " 'PP3920.pdf',\n",
       " 'PP3921.pdf',\n",
       " 'PP3928.pdf',\n",
       " 'PP3955.pdf',\n",
       " 'PP3965.pdf',\n",
       " 'PP3967.pdf',\n",
       " 'PP3988.pdf',\n",
       " 'PP3994.pdf',\n",
       " 'PP4034.pdf',\n",
       " 'PP4076.pdf',\n",
       " 'PP4083.pdf',\n",
       " 'PP4096.pdf',\n",
       " 'PP4098.pdf',\n",
       " 'PP4100.pdf',\n",
       " 'PP4177.pdf',\n",
       " 'PP4182.pdf',\n",
       " 'PP4213.pdf',\n",
       " 'PP4247.pdf',\n",
       " 'PP4257.pdf',\n",
       " 'PP4300.pdf',\n",
       " 'PP4315.pdf',\n",
       " 'PP4318.pdf',\n",
       " 'PP4327.pdf',\n",
       " 'PP4401.pdf',\n",
       " 'PP4423.pdf',\n",
       " 'PP4427.pdf',\n",
       " 'PP4437.pdf',\n",
       " 'PP4448.pdf',\n",
       " 'PP4501.pdf',\n",
       " 'PP4528.pdf',\n",
       " 'PP4543.pdf',\n",
       " 'PP4544.pdf',\n",
       " 'PP4573.pdf',\n",
       " 'PP4580.pdf',\n",
       " 'PP4620.pdf',\n",
       " 'PP4659.pdf',\n",
       " 'PP4679.pdf',\n",
       " 'PP4687.pdf',\n",
       " 'PP4690.pdf',\n",
       " 'PP4707.pdf',\n",
       " 'PP4733.pdf',\n",
       " 'PP4734.pdf',\n",
       " 'PP4745.pdf',\n",
       " 'PP4756.pdf',\n",
       " 'PP4786.pdf',\n",
       " 'PP4798.pdf',\n",
       " 'PP4823.pdf',\n",
       " 'PP4877.pdf',\n",
       " 'PP4904.pdf',\n",
       " 'PP4921.pdf',\n",
       " 'PP4931.pdf',\n",
       " 'PP4946.pdf',\n",
       " 'PP4958.pdf',\n",
       " 'PP4968.pdf',\n",
       " 'PP5006.pdf',\n",
       " 'PP5016.pdf',\n",
       " 'PP5025.pdf',\n",
       " 'PP5043.pdf',\n",
       " 'PP5073.pdf',\n",
       " 'PP5074.pdf',\n",
       " 'PP5084.pdf',\n",
       " 'PP5087.pdf',\n",
       " 'PP5099.pdf',\n",
       " 'PP5107.pdf',\n",
       " 'PP5118.pdf',\n",
       " 'PP5122.pdf',\n",
       " 'PP5172.pdf',\n",
       " 'PP5223.pdf',\n",
       " 'PP5235.pdf',\n",
       " 'PP5268.pdf',\n",
       " 'PP5283.pdf',\n",
       " 'PP5299.pdf',\n",
       " 'PP5319.pdf',\n",
       " 'PP5343.pdf',\n",
       " 'PP5357.pdf',\n",
       " 'PP5415.pdf',\n",
       " 'PP5416.pdf',\n",
       " 'PP5425.pdf',\n",
       " 'PP5444.pdf',\n",
       " 'PP5446.pdf',\n",
       " 'PP5447.pdf',\n",
       " 'PP5489.pdf',\n",
       " 'PP5492.pdf',\n",
       " 'PP5496.pdf',\n",
       " 'PP5504.pdf',\n",
       " 'PP5522.pdf',\n",
       " 'PP5563.pdf',\n",
       " 'PP5572.pdf',\n",
       " 'PP5582.pdf',\n",
       " 'PP5601.pdf',\n",
       " 'PP5614.pdf',\n",
       " 'PP5629.pdf',\n",
       " 'PP5630.pdf',\n",
       " 'PP5706.pdf',\n",
       " 'PP5721.pdf',\n",
       " 'PP5722.pdf',\n",
       " 'PP5772.pdf',\n",
       " 'PP5779.pdf',\n",
       " 'PP5807.pdf',\n",
       " 'PP5847.pdf',\n",
       " 'PP5855.pdf',\n",
       " 'PP5876.pdf',\n",
       " 'PP5882.pdf',\n",
       " 'PP5899.pdf',\n",
       " 'PP5927.pdf',\n",
       " 'PP5931.pdf',\n",
       " 'PP5932.pdf',\n",
       " 'PP5934.pdf',\n",
       " 'PP5941.pdf',\n",
       " 'PP5945.pdf',\n",
       " 'PP5981.pdf',\n",
       " 'PP6001.pdf',\n",
       " 'PP6017.pdf',\n",
       " 'PP6021.pdf',\n",
       " 'PP6024.pdf',\n",
       " 'PP6034.pdf',\n",
       " 'PP6035.pdf',\n",
       " 'PP6045.pdf',\n",
       " 'PP6080.pdf',\n",
       " 'PP6110.pdf',\n",
       " 'PP6132.pdf',\n",
       " 'PP6171.pdf',\n",
       " 'PP6221.pdf',\n",
       " 'PP6233.pdf',\n",
       " 'PP6295.pdf',\n",
       " 'PP6334.pdf',\n",
       " 'PP6395.pdf',\n",
       " 'PP6426.pdf',\n",
       " 'PP6429.pdf',\n",
       " 'PP6435.pdf',\n",
       " 'PP6439.pdf',\n",
       " 'PP6462.pdf',\n",
       " 'PP6492.pdf',\n",
       " 'PP6495.pdf',\n",
       " 'PP6497.pdf',\n",
       " 'PP6518.pdf',\n",
       " 'PP6519.pdf',\n",
       " 'PP6539.pdf',\n",
       " 'PP6569.pdf',\n",
       " 'PP6618.pdf',\n",
       " 'PP6624.pdf',\n",
       " 'PP6641.pdf',\n",
       " 'PP6670.pdf',\n",
       " 'PP6686.pdf',\n",
       " 'PP6693.pdf',\n",
       " 'PP6715.pdf',\n",
       " 'PP6787.pdf',\n",
       " 'PP6791.pdf',\n",
       " 'PP6799.pdf',\n",
       " 'PP6825.pdf',\n",
       " 'PP6846.pdf',\n",
       " 'PP6899.pdf',\n",
       " 'PP6943.pdf',\n",
       " 'PP6945.pdf',\n",
       " 'PP6963.pdf',\n",
       " 'PP6964.pdf',\n",
       " 'PP6983.pdf',\n",
       " 'PP7054.pdf',\n",
       " 'PP7062.pdf',\n",
       " 'PP7078.pdf',\n",
       " 'PP7114.pdf',\n",
       " 'PP7122.pdf',\n",
       " 'PP7181.pdf',\n",
       " 'PP7189.pdf',\n",
       " 'PP7221.pdf',\n",
       " 'PP7245.pdf',\n",
       " 'PP7262.pdf',\n",
       " 'PP7274.pdf',\n",
       " 'PP7275.pdf']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
